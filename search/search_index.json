{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>NVIDIA BioNeMo Framework is a collection of programming tools, libraries, and models for computational drug discovery. It accelerates the most time-consuming and costly stages of building and adapting biomolecular AI models by providing domain-specific, optimized models and tooling that are easily integrated into GPU-based computational resources for the fastest performance on the market. You can access BioNeMo Framework as a free community resource or learn more about getting an enterprise license for improved expert-level support at the BioNeMo homepage.</p> <ul> <li> <p> User Guide</p> <p>Install BioNeMo and set up your environment to start accelerating your bioinformatics workflows.</p> <p>Get Started</p> </li> <li> <p> API Reference</p> <p>Access comprehensive documentation on BioNeMo's sub-packages, functions, and classes.</p> <p>API Reference</p> </li> <li> <p> Models</p> <p>Explore detailed instructions and best practices for using BioNeMo models in your research.</p> <p>Explore Models</p> </li> <li> <p> Datasets</p> <p>Explore biomolecular datasets that come pre-packaged with the BioNeMo Framework.</p> <p>Explore Datasets</p> </li> </ul>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>User Guide</li> <li>Models</li> <li>Datasets</li> <li>API</li> </ul>"},{"location":"API_reference/","title":"API reference","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"API_reference/bionemo/core/api/","title":"Api","text":""},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/api/#bionemo.core.api.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/api/","title":"Api","text":""},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/","title":"Multi epoch dataset","text":""},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex","title":"<code>EpochIndex</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A tuple that contains both the current epoch and index for multi-epoch training.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class EpochIndex(NamedTuple):\n    \"\"\"A tuple that contains both the current epoch and index for multi-epoch training.\"\"\"\n\n    epoch: int\n    \"\"\"An integer representing the current epoch.\"\"\"\n\n    idx: int\n    \"\"\"An integer representing the index within the current epoch.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.epoch","title":"<code>epoch: int</code>  <code>instance-attribute</code>","text":"<p>An integer representing the current epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.EpochIndex.idx","title":"<code>idx: int</code>  <code>instance-attribute</code>","text":"<p>An integer representing the index within the current epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper","title":"<code>IdentityMultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MultiEpochDatasetWrapper[T, T]</code></p> <p>An implementation of the <code>MultiEpochDatasetWrapper</code> that does not apply any transformations.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class IdentityMultiEpochDatasetWrapper(MultiEpochDatasetWrapper[T, T]):\n    \"\"\"An implementation of the `MultiEpochDatasetWrapper` that does not apply any transformations.\"\"\"\n\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n        \"\"\"Return the sample as is.\"\"\"\n        del index  # Unused.\n        return sample\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.IdentityMultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>","text":"<p>Return the sample as is.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def apply_transform(self, sample: T, index: EpochIndex) -&gt; T:\n    \"\"\"Return the sample as is.\"\"\"\n    del index  # Unused.\n    return sample\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDataset","title":"<code>MultiEpochDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for datasets for multi-epoch training in Megatron-LM.</p> <p>Dataset determinism in Megatron-LM</p> <p>In megatron training, the sampler and dataset objects are used to ensure consistent data loading across model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for every call to <code>__getitem__</code> with the same index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class MultiEpochDataset(Protocol[T_co]):\n    \"\"\"A protocol for datasets for multi-epoch training in Megatron-LM.\n\n    !!! important \"Dataset determinism in Megatron-LM\"\n        In megatron training, the sampler and dataset objects are used to ensure consistent data loading across\n        model-parallel ranks. For datasets to work with megatron training, they must return exactly the same data for\n        every call to `__getitem__` with the same index.\n    \"\"\"\n\n    def __getitem__(self, index: EpochIndex) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler","title":"<code>MultiEpochDatasetResampler</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.</p> <p>Either <code>num_epochs</code> or <code>num_samples</code> should be provided. If neither are provided, the dataset will use a single epoch. If <code>num_epochs</code> is given, the resampled dataset will have <code>len(dataset) * num_epochs</code> samples. If <code>num_samples</code> the resampled dataset will have <code>num_samples</code> samples. For <code>num_samples</code>, the dataset will be repeated for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetResampler(Dataset[T_co]):\n    \"\"\"A dataset wrapper class that converts the sequential sampling from Megatron-LM to epoch-based sampling.\n\n    Either `num_epochs` or `num_samples` should be provided. If neither are provided, the dataset will use a single\n    epoch. If `num_epochs` is given, the resampled dataset will have `len(dataset) * num_epochs` samples. If\n    `num_samples` the resampled dataset will have `num_samples` samples. For `num_samples`, the dataset will be repeated\n    for multiple epochs until the desired number of samples is reached (with the final epoch being truncated).\n    \"\"\"\n\n    dataset: MultiEpochDataset[T_co]\n    \"\"\"The dataset to resample. Must support indexing with an `EpochIndex`.\"\"\"\n\n    num_epochs: int | None = None\n    \"\"\"The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.\"\"\"\n\n    num_samples: int | None = None\n    \"\"\"The total number of samples to draw.\n\n    The number of epochs will be determined by the number of samples and the length of the dataset.\n    \"\"\"\n\n    shuffle: bool = True\n    \"\"\"Whether to shuffle the samples in the dataset each epoch.\"\"\"\n\n    seed: int = 42  # type: ignore\n    \"\"\"A random seed for reproducibility.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n        if self.num_epochs is None and self.num_samples is None:\n            self.num_epochs = 1\n        elif self.num_epochs is not None and self.num_samples is not None:\n            raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n        if self.num_epochs is None and self.num_samples is not None:\n            self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n        elif self.num_samples is None and self.num_epochs is not None:\n            self.num_samples = len(self.dataset) * self.num_epochs\n\n        # Type guard statements, the above if/elif block should ensure these are not None.\n        assert self.num_epochs is not None\n        assert self.num_samples is not None\n\n        if self.num_epochs &lt; 1:\n            raise ValueError(\"num_epochs must be at least 1.\")\n\n        rng = np.random.default_rng(self.seed)\n\n        # Initialize a vector of random seeds so that each epoch is shuffled differently.\n        self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Get the sample at the given index.\"\"\"\n        if index not in range(len(self)):\n            raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n        return self.dataset[self._global_index_to_permuted_local_index(index)]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the resampled dataset.\"\"\"\n        return self.num_samples  # type: ignore\n\n    def _global_index_to_permuted_local_index(self, index: int) -&gt; EpochIndex:\n        \"\"\"Convert a global index to an epoch index.\"\"\"\n        epoch = index // len(self.dataset)\n        idx = index % len(self.dataset)\n        if self.shuffle:\n            idx = permute(idx, len(self.dataset), self.epoch_seeds[epoch])\n        return EpochIndex(epoch, idx)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.dataset","title":"<code>dataset: MultiEpochDataset[T_co]</code>  <code>instance-attribute</code>","text":"<p>The dataset to resample. Must support indexing with an <code>EpochIndex</code>.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_epochs","title":"<code>num_epochs: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of epochs. The length of the resampled dataset will be len(dataset) * num_epochs.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.num_samples","title":"<code>num_samples: int | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The total number of samples to draw.</p> <p>The number of epochs will be determined by the number of samples and the length of the dataset.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.seed","title":"<code>seed: int = 42</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A random seed for reproducibility.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.shuffle","title":"<code>shuffle: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to shuffle the samples in the dataset each epoch.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Get the sample at the given index.\"\"\"\n    if index not in range(len(self)):\n        raise IndexError(f\"Index {index} out of bounds for dataset of length {len(self)}.\")\n    return self.dataset[self._global_index_to_permuted_local_index(index)]\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the resampled dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the resampled dataset.\"\"\"\n    return self.num_samples  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Pre-shuffle each epoch's samples.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Pre-shuffle each epoch's samples.\"\"\"\n    if self.num_epochs is None and self.num_samples is None:\n        self.num_epochs = 1\n    elif self.num_epochs is not None and self.num_samples is not None:\n        raise ValueError(\"Only one of num_epochs and num_samples should be provided.\")\n\n    if self.num_epochs is None and self.num_samples is not None:\n        self.num_epochs = math.ceil(self.num_samples / len(self.dataset))\n\n    elif self.num_samples is None and self.num_epochs is not None:\n        self.num_samples = len(self.dataset) * self.num_epochs\n\n    # Type guard statements, the above if/elif block should ensure these are not None.\n    assert self.num_epochs is not None\n    assert self.num_samples is not None\n\n    if self.num_epochs &lt; 1:\n        raise ValueError(\"num_epochs must be at least 1.\")\n\n    rng = np.random.default_rng(self.seed)\n\n    # Initialize a vector of random seeds so that each epoch is shuffled differently.\n    self.epoch_seeds = rng.integers(0, np.iinfo(np.int32).max, size=self.num_epochs)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper","title":"<code>MultiEpochDatasetWrapper</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Dataset[U_co]</code>, <code>Generic[T, U_co]</code>, <code>ABC</code></p> <p>A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.</p> <p>The underlying dataset's getitem method must be deterministic, i.e. it must return the same data for the same index every time it is called. If there are any non-deterministic operations, they should be moved to the <code>apply_transform</code> method. This method must also be deterministic for every (epoch, index) pair, but it can use the epoch to implement data augmentation each epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@dataclass\nclass MultiEpochDatasetWrapper(Dataset[U_co], Generic[T, U_co], ABC):\n    \"\"\"A wrapper to convert a standard pytorch dataset into one that supports multi-epoch megatron training.\n\n    The underlying dataset's __getitem__ method must be deterministic, i.e. it must return the same data for the same\n    index every time it is called. If there are any non-deterministic operations, they should be moved to the\n    `apply_transform` method. This method must also be deterministic for every (epoch, index) pair, but it can use\n    the epoch to implement data augmentation each epoch.\n    \"\"\"\n\n    dataset: SizedDataset[T]\n    \"\"\"A deterministic dataset that supports indexing with an integer index.\"\"\"\n\n    @abstractmethod\n    def apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n        \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, index: EpochIndex) -&gt; U_co:\n        \"\"\"Get the sample at the given epoch and index.\"\"\"\n        return self.apply_transform(self.dataset[index.idx], index)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.dataset)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.dataset","title":"<code>dataset: SizedDataset[T]</code>  <code>instance-attribute</code>","text":"<p>A deterministic dataset that supports indexing with an integer index.</p>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get the sample at the given epoch and index.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; U_co:\n    \"\"\"Get the sample at the given epoch and index.\"\"\"\n    return self.apply_transform(self.dataset[index.idx], index)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of the dataset.\"\"\"\n    return len(self.dataset)\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetWrapper.apply_transform","title":"<code>apply_transform(sample, index)</code>  <code>abstractmethod</code>","text":"<p>Apply any transformations to the sample for the given epoch.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>@abstractmethod\ndef apply_transform(self, sample: T, index: EpochIndex) -&gt; U_co:\n    \"\"\"Apply any transformations to the sample for the given epoch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/core/data/multi_epoch_dataset/#bionemo.core.data.multi_epoch_dataset.SizedDataset","title":"<code>SizedDataset</code>","text":"<p>               Bases: <code>Protocol[T_co]</code></p> <p>A protocol for integer-indexed datasets that have a fixed length.</p> Source code in <code>bionemo/core/data/multi_epoch_dataset.py</code> <pre><code>class SizedDataset(Protocol[T_co]):\n    \"\"\"A protocol for integer-indexed datasets that have a fixed length.\"\"\"\n\n    def __getitem__(self, index: int) -&gt; T_co:  # noqa: D105\n        ...\n\n    def __len__(self) -&gt; int:  # noqa: D105\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/data/permute/","title":"Permute","text":""},{"location":"API_reference/bionemo/core/data/permute/#bionemo.core.data.permute.permute","title":"<code>permute(index, length, seed)</code>","text":"<p>Index into a permuted array with constant space and time complexity.</p> <p>This function permutes an index <code>i</code> into a range <code>[0, l)</code> using a hash function. See https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to permute.</p> required <code>length</code> <code>int</code> <p>The range of the permuted index.</p> required <code>seed</code> <code>int</code> <p>The permutation seed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The permuted index in range(0, length).</p> Source code in <code>bionemo/core/data/permute.py</code> <pre><code>def permute(index: int, length: int, seed: int) -&gt; int:\n    \"\"\"Index into a permuted array with constant space and time complexity.\n\n    This function permutes an index `i` into a range `[0, l)` using a hash function. See\n    https://afnan.io/posts/2019-04-05-explaining-the-hashed-permutation/ for more details and\n    \"Correlated Multi-Jittered Sampling\" by Andrew Kensler for the original algorithm.\n\n    Args:\n        index: The index to permute.\n        length: The range of the permuted index.\n        seed: The permutation seed.\n\n    Returns:\n        The permuted index in range(0, length).\n    \"\"\"\n    if length &lt;= 1:\n        raise ValueError(\"The length of the permuted range must be greater than 1.\")\n\n    if index not in range(length):\n        raise ValueError(\"The index to permute must be in the range [0, l).\")\n\n    if seed &lt; 0:\n        raise ValueError(\"The permutation seed must be greater than or equal to 0.\")\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        w = length - 1\n        w |= w &gt;&gt; 1\n        w |= w &gt;&gt; 2\n        w |= w &gt;&gt; 4\n        w |= w &gt;&gt; 8\n        w |= w &gt;&gt; 16\n\n        while True:\n            index ^= seed\n            index *= 0xE170893D\n            index ^= seed &gt;&gt; 16\n            index ^= (index &amp; w) &gt;&gt; 4\n            index ^= seed &gt;&gt; 8\n            index *= 0x0929EB3F\n            index ^= seed &gt;&gt; 23\n            index ^= (index &amp; w) &gt;&gt; 1\n            index *= 1 | seed &gt;&gt; 27\n            index *= 0x6935FA69\n            index ^= (index &amp; w) &gt;&gt; 11\n            index *= 0x74DCB303\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0x9E501CC3\n            index ^= (index &amp; w) &gt;&gt; 2\n            index *= 0xC860A3DF\n            index &amp;= w\n            if index &lt; length:\n                break\n\n    return (index + seed) % length\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/","title":"Resamplers","text":""},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset","title":"<code>PRNGResampleDataset</code>","text":"<p>               Bases: <code>Dataset[T_co]</code></p> <p>A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.</p> <p>PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It works by generating random indices assuming that the requesting function asks for them sequentially. Although random lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing from 0 if the last requested index was greater than or equal to this requested index. This should work well with the megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not generating those numbers.</p> <p>Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</p> <p>This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based sampling provided by <code>bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler</code> instead, which ensures that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large for the shuffled list of indices to fit in memory and exhaustive sampling is not required.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>class PRNGResampleDataset(Dataset[T_co]):\n    \"\"\"A thread-safe dataset shuffler that uses a pseudo-random number generator (PRNG) to shuffle the dataset.\n\n    PRNGResampleDataset shuffles a given dataset using a pseudo-random number generator (PRNG). This allows for\n    reproducible shuffling by controlling the random seed, while not ever storing the list of indices in memory. It\n    works by generating random indices assuming that the requesting function asks for them sequentially. Although random\n    lookups are supported, random lookups will involve recomputing state which is slow, and involves linearly advancing\n    from 0 if the last requested index was greater than or equal to this requested index. This should work well with the\n    megatron sampler which is sequential. It handles skipped lookups as will happen with multiple workers by not\n    generating those numbers.\n\n    !!! warning \"Prefer bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler\"\n\n        This class performs sampling with replacement of an underlying dataset. It is recommended to use the epoch-based\n        sampling provided by `bionemo.core.data.multi_epoch_dataset.MultiEpochDatasetResampler` instead, which ensures\n        that each sample is seen exactly once per epoch. This dataset is useful for cases where the dataset is too large\n        for the shuffled list of indices to fit in memory and exhaustive sampling is not required.\n    \"\"\"\n\n    def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n        \"\"\"Initializes the PRNGResampleDataset.\n\n        Args:\n            dataset: The dataset to be shuffled.\n            seed: The seed value for the PRNG. Default is 42.\n            num_samples: The number of samples to draw from the dataset.\n                If None, the length of the dataset is used. Default is None.\n        \"\"\"\n        self.initial_seed = seed\n        self.rng = random.Random(seed)\n        self.dataset_len = len(dataset)  # type: ignore\n        self.num_samples = num_samples if num_samples is not None else len(dataset)\n        self.dataset = dataset\n        # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n        #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n        #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n        #  will proceed normally.\n        self.last_index: Union[int, math.inf] = math.inf\n        self.last_rand_index: Optional[int] = None\n\n    def rand_idx(self) -&gt; int:\n        \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n        return self.rng.randint(0, self.dataset_len - 1)\n\n    def advance_state(self, num_to_advance: int):\n        \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n        Args:\n            num_to_advance: The number of random state steps to advance.\n        \"\"\"\n        for _ in range(num_to_advance):\n            self.rand_idx()\n\n    def __getitem__(self, index: int) -&gt; T_co:\n        \"\"\"Returns the item from the dataset at the specified index.\n\n        Args:\n            index: The index of the item to retrieve.\n\n        Returns:\n            The item from the dataset at the specified index.\n\n        Note:\n            If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n            and advanced to the correct state. This is less efficient than advancing forward.\n        \"\"\"\n        idx_diff = index - self.last_index\n        if idx_diff &lt; 0:\n            # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n            #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n            self.rng = random.Random(self.initial_seed)\n            self.advance_state(index)\n        elif idx_diff == 0:\n            # If the index is the same as the last index, we can just return the last random index that was generated.\n            #  no state needs to be updated in this case so just return.\n            return self.dataset[self.last_rand_index]\n        else:\n            # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n            #  by `idx_diff - 1` to accomodate for skipped indices.\n            self.advance_state(idx_diff - 1)\n        self.last_index = index\n        self.last_rand_index = (\n            self.rand_idx()\n        )  # store the last index called incase the user wants to requrest this index again.\n        return self.dataset[self.last_rand_index]  # Advances state by 1\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n        return self.num_samples\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Returns the item from the dataset at the specified index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the item to retrieve.</p> required <p>Returns:</p> Type Description <code>T_co</code> <p>The item from the dataset at the specified index.</p> Note <p>If the requested index is before the last accessed index, the PRNG state is reset to the initial seed and advanced to the correct state. This is less efficient than advancing forward.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __getitem__(self, index: int) -&gt; T_co:\n    \"\"\"Returns the item from the dataset at the specified index.\n\n    Args:\n        index: The index of the item to retrieve.\n\n    Returns:\n        The item from the dataset at the specified index.\n\n    Note:\n        If the requested index is before the last accessed index, the PRNG state is reset to the initial seed\n        and advanced to the correct state. This is less efficient than advancing forward.\n    \"\"\"\n    idx_diff = index - self.last_index\n    if idx_diff &lt; 0:\n        # We need to go backwards (or it is the first call), which involves resetting to the initial seed and\n        #   then advancing to just before the correct index, which is accomplished with `range(index)`.\n        self.rng = random.Random(self.initial_seed)\n        self.advance_state(index)\n    elif idx_diff == 0:\n        # If the index is the same as the last index, we can just return the last random index that was generated.\n        #  no state needs to be updated in this case so just return.\n        return self.dataset[self.last_rand_index]\n    else:\n        # We need to advance however many steps were skipped since the last call. Since i+1 - i = 1, we need to advance\n        #  by `idx_diff - 1` to accomodate for skipped indices.\n        self.advance_state(idx_diff - 1)\n    self.last_index = index\n    self.last_rand_index = (\n        self.rand_idx()\n    )  # store the last index called incase the user wants to requrest this index again.\n    return self.dataset[self.last_rand_index]  # Advances state by 1\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__init__","title":"<code>__init__(dataset, seed=42, num_samples=None)</code>","text":"<p>Initializes the PRNGResampleDataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[T_co]</code> <p>The dataset to be shuffled.</p> required <code>seed</code> <code>int</code> <p>The seed value for the PRNG. Default is 42.</p> <code>42</code> <code>num_samples</code> <code>Optional[int]</code> <p>The number of samples to draw from the dataset. If None, the length of the dataset is used. Default is None.</p> <code>None</code> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __init__(self, dataset: Dataset[T_co], seed: int = 42, num_samples: Optional[int] = None):\n    \"\"\"Initializes the PRNGResampleDataset.\n\n    Args:\n        dataset: The dataset to be shuffled.\n        seed: The seed value for the PRNG. Default is 42.\n        num_samples: The number of samples to draw from the dataset.\n            If None, the length of the dataset is used. Default is None.\n    \"\"\"\n    self.initial_seed = seed\n    self.rng = random.Random(seed)\n    self.dataset_len = len(dataset)  # type: ignore\n    self.num_samples = num_samples if num_samples is not None else len(dataset)\n    self.dataset = dataset\n    # Store the last accessed index. On this first pass this is initialized to infinity, which will trigger a reset since\n    #  index - inf &lt; 0 for all values of index. This will lead to `self.advance_state(index)` being called which will advance\n    #  the state to the correct starting index. The last_index will be then be replaced by `index` in that case and the algorithm\n    #  will proceed normally.\n    self.last_index: Union[int, math.inf] = math.inf\n    self.last_rand_index: Optional[int] = None\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples in the dataset.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples in the dataset.\"\"\"\n    return self.num_samples\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.advance_state","title":"<code>advance_state(num_to_advance)</code>","text":"<p>Advances the PRNG state by generating n_to_advance random indices.</p> <p>Parameters:</p> Name Type Description Default <code>num_to_advance</code> <code>int</code> <p>The number of random state steps to advance.</p> required Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def advance_state(self, num_to_advance: int):\n    \"\"\"Advances the PRNG state by generating n_to_advance random indices.\n\n    Args:\n        num_to_advance: The number of random state steps to advance.\n    \"\"\"\n    for _ in range(num_to_advance):\n        self.rand_idx()\n</code></pre>"},{"location":"API_reference/bionemo/core/data/resamplers/#bionemo.core.data.resamplers.PRNGResampleDataset.rand_idx","title":"<code>rand_idx()</code>","text":"<p>Generates a random index within the range of the dataset size.</p> Source code in <code>bionemo/core/data/resamplers.py</code> <pre><code>def rand_idx(self) -&gt; int:\n    \"\"\"Generates a random index within the range of the dataset size.\"\"\"\n    return self.rng.randint(0, self.dataset_len - 1)\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/","title":"Config","text":""},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.LossType","title":"<code>LossType = TypeVar('LossType')</code>  <code>module-attribute</code>","text":"<p>Stand-in for a loss function; no constraints.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelOutput","title":"<code>ModelOutput = TypeVar('ModelOutput', Tensor, list[Tensor], tuple[Tensor], dict[str, Tensor], covariant=True)</code>  <code>module-attribute</code>","text":"<p>A Model's forward pass may produce a tensor, multiple tensors, or named tensors.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.ModelType","title":"<code>ModelType = TypeVar('ModelType', bound=Model)</code>  <code>module-attribute</code>","text":"<p>Generic type for things that have a forward pass.</p>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig","title":"<code>BionemoModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType]</code>, <code>ABC</code></p> <p>An abstract class for model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoModelConfig(Generic[ModelType], ABC):\n    \"\"\"An abstract class for model configuration.\"\"\"\n\n    @abstractmethod\n    def configure_model(self, *args, **kwargs) -&gt; Model:\n        \"\"\"Configures the model.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoModelConfig.configure_model","title":"<code>configure_model(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Configures the model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef configure_model(self, *args, **kwargs) -&gt; Model:\n    \"\"\"Configures the model.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig","title":"<code>BionemoTrainableModelConfig</code>","text":"<p>               Bases: <code>Generic[ModelType, LossType]</code>, <code>BionemoModelConfig[ModelType]</code></p> <p>An abstract class for trainable model configuration.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class BionemoTrainableModelConfig(Generic[ModelType, LossType], BionemoModelConfig[ModelType]):\n    \"\"\"An abstract class for trainable model configuration.\"\"\"\n\n    @abstractmethod\n    def get_loss_reduction_class(self) -&gt; Type[LossType]:\n        \"\"\"Returns the loss reduction class.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.BionemoTrainableModelConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>  <code>abstractmethod</code>","text":"<p>Returns the loss reduction class.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>@abstractmethod\ndef get_loss_reduction_class(self) -&gt; Type[LossType]:\n    \"\"\"Returns the loss reduction class.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>Protocol[ModelOutput]</code></p> <p>Lightweight interface for a model: must have a forward method.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>class Model(Protocol[ModelOutput]):\n    \"\"\"Lightweight interface for a model: must have a forward method.\"\"\"\n\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        \"\"\"Prediction / forward-step for a model.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/core/model/config/#bionemo.core.model.config.Model.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Prediction / forward-step for a model.</p> Source code in <code>bionemo/core/model/config.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; ModelOutput:\n    \"\"\"Prediction / forward-step for a model.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/batching_utils/","title":"Batching utils","text":""},{"location":"API_reference/bionemo/core/utils/batching_utils/#bionemo.core.utils.batching_utils.pad_token_ids","title":"<code>pad_token_ids(token_ids, padding_value=0, padding_len=None, pad_size_divisible_by=1, **convert_to_kwargs)</code>","text":"<p>Pads token ids with padding value, and return the padded tokens and the corresponding mask.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>Union[List[int], List[Tensor]]</code> <p>List of token ids or tensors</p> required <code>padding_value</code> <code>int</code> <p>Value to pad with. Defaults to 0.</p> <code>0</code> <code>padding_len</code> <code>Optional[int]</code> <p>Max length of the padded token ids. Defaults to None.</p> <code>None</code> <code>pad_size_divisible_by</code> <code>int</code> <p>Pad the length of the token ids to be divisible by this number. Defaults to 1.</p> <code>1</code> <code>**convert_to_kwargs</code> <p>Passed directly to tensor.to(**kwargs) if provided</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[List[int], List[int]]: Padded token ids and mask</p> Source code in <code>bionemo/core/utils/batching_utils.py</code> <pre><code>def pad_token_ids(\n    token_ids: Union[List[int], List[torch.Tensor]],\n    padding_value: int = 0,\n    padding_len: Optional[int] = None,\n    pad_size_divisible_by: int = 1,\n    **convert_to_kwargs,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Pads token ids with padding value, and return the padded tokens and the corresponding mask.\n\n    Args:\n        token_ids: List of token ids or tensors\n        padding_value: Value to pad with. Defaults to 0.\n        padding_len: Max length of the padded token ids. Defaults to None.\n        pad_size_divisible_by: Pad the length of the token ids to be divisible by this number. Defaults to 1.\n        **convert_to_kwargs: Passed directly to tensor.to(**kwargs) if provided\n\n    Returns:\n        Tuple[List[int], List[int]]: Padded token ids and mask\n    \"\"\"\n    lengths = torch.tensor([len(s) for s in token_ids])\n    if padding_len is None:\n        padding_len = lengths.max()\n\n    # make padding divisible by pad_size_divisible_by\n    if pad_size_divisible_by &gt; 1:\n        padding_len = int(math.ceil(padding_len / pad_size_divisible_by) * pad_size_divisible_by)\n\n    # build mask\n    mask = torch.arange(padding_len)[None, :] &lt; lengths[:, None]\n\n    # make sure all sequences are pytorch tensors\n    token_ids = [torch.tensor(s) if not torch.is_tensor(s) else s for s in token_ids]\n    # pad sequences\n    masked_token_ids = torch.nn.utils.rnn.pad_sequence(token_ids, batch_first=True, padding_value=padding_value)\n\n    # convert to desired device\n    if len(convert_to_kwargs):\n        mask = mask.to(**convert_to_kwargs)\n        masked_token_ids = masked_token_ids.to(**convert_to_kwargs)\n\n    # Further pad the sequences to the fixed maximum length, if necessary\n    if masked_token_ids.size(1) &lt; padding_len:\n        padding_size = padding_len - masked_token_ids.size(1)\n        masked_token_ids = torch.nn.functional.pad(masked_token_ids, [0, padding_size], value=padding_value)\n\n    return masked_token_ids, mask\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/dtypes/","title":"Dtypes","text":""},{"location":"API_reference/bionemo/core/utils/dtypes/#bionemo.core.utils.dtypes.get_autocast_dtype","title":"<code>get_autocast_dtype(precision)</code>","text":"<p>Returns the torch dtype corresponding to the given precision.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>PrecisionTypes</code> <p>The precision type.</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The torch dtype corresponding to the given precision.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the precision is not supported.</p> Source code in <code>bionemo/core/utils/dtypes.py</code> <pre><code>def get_autocast_dtype(precision: PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Returns the torch dtype corresponding to the given precision.\n\n    Args:\n        precision: The precision type.\n\n    Returns:\n        torch.dtype: The torch dtype corresponding to the given precision.\n\n    Raises:\n        ValueError: If the precision is not supported.\n    \"\"\"\n    # TODO move this to a utilities folder, or find/import the function that does this in NeMo\n    if precision == \"fp16\":\n        return torch.float16\n    elif precision == \"bf16\":\n        return torch.bfloat16\n    elif precision == \"fp32\":\n        return torch.float32\n    elif precision == \"16-mixed\":\n        return torch.float16\n    elif precision == \"fp16-mixed\":\n        return torch.float16\n    elif precision == \"bf16-mixed\":\n        return torch.bfloat16\n    elif precision == \"fp32-mixed\":\n        return torch.float32\n    elif precision == 16:\n        return torch.float16\n    elif precision == 32:\n        return torch.float32\n    else:\n        raise ValueError(f\"Unsupported precision: {precision}\")\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/random_utils/","title":"Random utils","text":""},{"location":"API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.get_seed_from_rng","title":"<code>get_seed_from_rng(rng, dtype=np.int64)</code>","text":"<p>Generates a deterministic random seed from an existing random generator.</p> <p>This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often do in initializing a numpy random generator with epoch, index, and global seeds.</p> <p>Used to seed a torch random generator from a numpy random generator.</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>def get_seed_from_rng(rng: np.random.Generator, dtype: Type[np.signedinteger] = np.int64) -&gt; int:\n    \"\"\"Generates a deterministic random seed from an existing random generator.\n\n    This is useful in particular because setting the torch seed doesn't want to accept a tuple of numbers, we we often\n    do in initializing a numpy random generator with epoch, index, and global seeds.\n\n    Used to seed a torch random generator from a numpy random generator.\n    \"\"\"\n    return int(rng.integers(np.iinfo(dtype).max))\n</code></pre>"},{"location":"API_reference/bionemo/core/utils/random_utils/#bionemo.core.utils.random_utils.random_numpy_context","title":"<code>random_numpy_context(seed=42)</code>","text":"<p>Context manager for setting numpy random state.</p> <p>The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state in a <code>with</code> context using this function, and get back to whatever state was there before. This is useful for testing where you don't want the random state from one test to impact other tests.</p> Example <p>import numpy as np from bionemo.core.utils.random_utils import random_numpy_context ori_state = np.random.get_state() with random_numpy_context(45):     np.random.randint(5) # this will change the state new_state = np.random.get_state() assert ori_state == new_state</p> Source code in <code>bionemo/core/utils/random_utils.py</code> <pre><code>@contextmanager\ndef random_numpy_context(seed: int = 42) -&gt; Iterator[None]:\n    \"\"\"Context manager for setting numpy random state.\n\n    The state is saved on entry and restored on exit to what it was. This way you can run code that needs random state\n    in a `with` context using this function, and get back to whatever state was there before. This is useful for testing\n    where you don't want the random state from one test to impact other tests.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from bionemo.core.utils.random_utils import random_numpy_context\n        &gt;&gt;&gt; ori_state = np.random.get_state()\n        &gt;&gt;&gt; with random_numpy_context(45):\n            np.random.randint(5) # this will change the state\n        &gt;&gt;&gt; new_state = np.random.get_state()\n        &gt;&gt;&gt; assert ori_state == new_state\n    \"\"\"\n    state = np.random.get_state()  # just fail if this fails\n    try:\n        np.random.seed(seed)\n        yield\n    finally:\n        np.random.set_state(state)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/","title":"Api","text":""},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Config","title":"<code>ESM2Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig</code>, <code>IOMixinWithGettersSetters</code></p> <p>Configuration class for ESM2 model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2Config(ESM2GenericConfig, iom.IOMixinWithGettersSetters):\n    \"\"\"Configuration class for ESM2 model.\"\"\"\n\n    model_cls: Type[ESM2Model] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2GenericConfig","title":"<code>ESM2GenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[ESM2ModelT, MegatronLossType]</code></p> <p>Configuration class for ESM2 model.</p> <p>Attributes:</p> Name Type Description <code>num_layers</code> <code>int</code> <p>Number of layers in the model.</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the model.</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads in the model.</p> <code>ffn_hidden_size</code> <code>int</code> <p>Hidden size of the feed-forward network.</p> <code>hidden_dropout</code> <code>float</code> <p>Dropout rate for hidden layers.</p> <code>attention_dropout</code> <code>float</code> <p>Dropout rate for attention layers.</p> <code>apply_residual_connection_post_layernorm</code> <code>bool</code> <p>Whether to apply residual connection after layer normalization.</p> <code>layernorm_epsilon</code> <code>float</code> <p>Epsilon value for layer normalization.</p> <code>layernorm_zero_centered_gamma</code> <code>float</code> <p>Whether to zero-center the gamma parameter in layer normalization.</p> <code>activation_func</code> <code>Callable</code> <p>Activation function used in the model.</p> <code>init_method_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>apply_query_key_layer_scaling</code> <code>float</code> <p>Whether to apply scaling to query and key layers.</p> <code>masked_softmax_fusion</code> <code>float</code> <p>Whether to use a kernel that fuses attention softmax with its mask.</p> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>Whether to share embeddings and output weights.</p> <code>enable_autocast</code> <code>bool</code> <p>Whether to enable autocast for mixed precision.</p> <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>BiobertSpecOption for the model.</p> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Type of position embedding used in the model.</p> <code>seq_length</code> <code>int</code> <p>Length of the input sequence.</p> <code>make_vocab_size_divisible_by</code> <code>int</code> <p>Make the vocabulary size divisible by this value.</p> <code>token_dropout</code> <code>bool</code> <p>Whether to apply token dropout.</p> <code>use_attention_mask</code> <code>bool</code> <p>Whether to use attention mask.</p> <code>use_esm_attention</code> <code>bool</code> <p>Whether to use ESM attention.</p> <code>attention_softmax_in_fp32</code> <code>bool</code> <p>Whether to use fp32 for attention softmax.</p> <code>optimizer_fn</code> <code>Optional[Callable[[MegatronBioBertModel], Optimizer]]</code> <p>Optional optimizer function for the model.</p> <code>parallel_output</code> <code>bool</code> <p>Whether to use parallel output.</p> <code>rotary_base</code> <code>int</code> <p>Base value for rotary positional encoding.</p> <code>rotary_percent</code> <code>float</code> <p>Percentage of rotary positional encoding.</p> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length.</p> <code>get_attention_mask_from_fusion</code> <code>Optional[float]</code> <p>Whether to get attention mask from fusion.</p> <code>nemo1_ckpt_path</code> <code>str | None</code> <p>Path to NEMO1 checkpoint.</p> <code>return_only_hidden_states</code> <code>bool</code> <p>Whether to return only hidden states.</p> <code>loss_reduction_class</code> <code>bool</code> <p>Loss reduction class for the model. Default to BERTMLMLossWithReduction.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2GenericConfig(BioBertConfig[ESM2ModelT, MegatronLossType]):\n    \"\"\"Configuration class for ESM2 model.\n\n    Attributes:\n        num_layers: Number of layers in the model.\n        hidden_size: Hidden size of the model.\n        num_attention_heads: Number of attention heads in the model.\n        ffn_hidden_size: Hidden size of the feed-forward network.\n        hidden_dropout: Dropout rate for hidden layers.\n        attention_dropout: Dropout rate for attention layers.\n        apply_residual_connection_post_layernorm: Whether to apply residual connection after layer normalization.\n        layernorm_epsilon: Epsilon value for layer normalization.\n        layernorm_zero_centered_gamma: Whether to zero-center the gamma parameter in layer normalization.\n        activation_func: Activation function used in the model.\n        init_method_std: Standard deviation for weight initialization.\n        apply_query_key_layer_scaling: Whether to apply scaling to query and key layers.\n        masked_softmax_fusion: Whether to use a kernel that fuses attention softmax with its mask.\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        share_embeddings_and_output_weights: Whether to share embeddings and output weights.\n        enable_autocast: Whether to enable autocast for mixed precision.\n        biobert_spec_option: BiobertSpecOption for the model.\n        position_embedding_type: Type of position embedding used in the model.\n        seq_length: Length of the input sequence.\n        make_vocab_size_divisible_by: Make the vocabulary size divisible by this value.\n        token_dropout: Whether to apply token dropout.\n        use_attention_mask: Whether to use attention mask.\n        use_esm_attention: Whether to use ESM attention.\n        attention_softmax_in_fp32: Whether to use fp32 for attention softmax.\n        optimizer_fn: Optional optimizer function for the model.\n        parallel_output: Whether to use parallel output.\n        rotary_base: Base value for rotary positional encoding.\n        rotary_percent: Percentage of rotary positional encoding.\n        seq_len_interpolation_factor: Interpolation factor for sequence length.\n        get_attention_mask_from_fusion: Whether to get attention mask from fusion.\n        nemo1_ckpt_path: Path to NEMO1 checkpoint.\n        return_only_hidden_states: Whether to return only hidden states.\n        loss_reduction_class: Loss reduction class for the model. Default to BERTMLMLossWithReduction.\n    \"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[ESM2ModelT] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n    num_attention_heads: int = 20\n    ffn_hidden_size: int = 4 * 1280  # Transformer FFN hidden size. Usually 4 * hidden_size.\n    hidden_dropout: float = 0  # ESM2 removes dropout from hidden layers and attention\n    attention_dropout: float = 0.0  # ESM2 does not use attention dropout\n    apply_residual_connection_post_layernorm: bool = False  # TODO: farhadr False is new default, True was BERT pub.\n    layernorm_epsilon: float = 1.0e-5\n    bias_activation_fusion: bool = True  # True degrades accuracy slightly, but is faster.\n    activation_func: Callable = F.gelu  # esm_gelu_func  # ESM2 MLP\n    init_method_std: float = 0.02\n\n    # embedding\n    token_dropout: bool = True\n    use_attention_mask: bool = True\n\n    # core attention\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    attention_softmax_in_fp32: bool = False\n    normalize_attention_scores: bool = False\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    fp16_lm_cross_entropy: bool = False  # Move the cross entropy unreduced loss calculation for lm head to fp16\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"rope\"  # ESM2 uses relative positional encoding 'ROPE' to extrapolate to longer sequences unseen during training\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[MegatronBioBertModel], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    nemo1_ckpt_path: str | None = None\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    #  self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # TODO (@jstjohn) come up with a cleaner way in the biobert module to return user requested\n    #  things as part of the workflow for inference and fine-tuning.\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    skip_logits: bool = False\n    return_only_hidden_states: bool = False  # return logits\n\n    def __post_init__(self):\n        # TODO, as a validator?\n        \"\"\"Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.\"\"\"\n        super().__post_init__()\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n            self.core_attention_override = ESM2TEDotProductAttention\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n            self.core_attention_override = ESM2DotProductAttention\n        else:\n            raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2GenericConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __post_init__(self):\n    # TODO, as a validator?\n    \"\"\"Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.\"\"\"\n    super().__post_init__()\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n        self.core_attention_override = ESM2TEDotProductAttention\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n        self.core_attention_override = ESM2DotProductAttention\n    else:\n        raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model","title":"<code>ESM2Model</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>ESM2 Transformer language model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>class ESM2Model(MegatronBioBertModel):\n    \"\"\"ESM2 Transformer language model.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: spec_utils.ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[BioNeMoESMTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = True,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        skip_logits: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 model.\n\n        Args:\n            config (TransformerConfig): transformer config\n            num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n            transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n            vocab_size (int): vocabulary size\n            max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n            tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n            pre_process (bool): Include embedding layer (used with pipeline parallelism)\n            post_process (bool): Include an output layer (used with pipeline parallelism)\n            fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n            parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n            share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n            position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n                Defaults is 'learned_absolute'.\n            rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n                Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n            seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n            add_binary_head (bool): Whether to add a binary head. Defaults to True.\n            return_embeddings (bool): Whether to return embeddings. Defaults to False.\n            include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n            use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n            include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n            skip_logits (bool): Skip writing the token logits in output dict\n        \"\"\"\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n\n        # Embeddings.\n        if self.pre_process:\n            # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n            # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n            # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n            self.embedding = ESM2Embedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n                # ESM2 NEW ARGS\n                token_dropout=self.config.token_dropout,\n                use_attention_mask=self.config.use_attention_mask,\n                mask_token_id=tokenizer.mask_token_id,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                seq_len_interpolation_factor=seq_len_interpolation_factor,\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def embedding_forward(\n        self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n    ):\n        \"\"\"Forward pass of the embedding layer.\n\n        Args:\n            input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n            position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n            tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n            attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n        \"\"\"\n        # ESM2 Customization: ESM2Embedding forward takes attention_mask\n        # in addition to the args required by LanguageModelEmbedding\n        return self.embedding(\n            input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model.__init__","title":"<code>__init__(config, num_tokentypes, transformer_layer_spec, vocab_size, max_sequence_length, tokenizer=None, pre_process=True, post_process=True, fp16_lm_cross_entropy=False, parallel_output=True, share_embeddings_and_output_weights=False, position_embedding_type='learned_absolute', rotary_percent=1.0, seq_len_interpolation_factor=None, add_binary_head=True, return_embeddings=False, include_embeddings=False, use_full_attention_mask=False, include_hiddens=False, skip_logits=False)</code>","text":"<p>Initialize the ESM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>optional tokenizer object (currently only used in the constructor of ESM2Model)</p> <code>None</code> <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>False</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>string</code> <p>Position embedding type. Options ['learned_absolute', 'rope']. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length. Defaults to None.</p> <code>None</code> <code>add_binary_head</code> <code>bool</code> <p>Whether to add a binary head. Defaults to True.</p> <code>True</code> <code>return_embeddings</code> <code>bool</code> <p>Whether to return embeddings. Defaults to False.</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the output dictionary. Defaults to False.</p> <code>False</code> <code>use_full_attention_mask</code> <code>bool</code> <p>Whether to use full attention mask. Defaults to False.</p> <code>False</code> <code>include_hiddens</code> <code>bool</code> <p>Whether to include hidden states in the output dictionary. Defaults to False.</p> <code>False</code> <code>skip_logits</code> <code>bool</code> <p>Skip writing the token logits in output dict</p> <code>False</code> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    num_tokentypes: int,\n    transformer_layer_spec: spec_utils.ModuleSpec,\n    vocab_size: int,\n    max_sequence_length: int,\n    tokenizer: Optional[BioNeMoESMTokenizer] = None,\n    pre_process: bool = True,\n    post_process: bool = True,\n    fp16_lm_cross_entropy: bool = False,\n    parallel_output: bool = True,\n    share_embeddings_and_output_weights: bool = False,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n    rotary_percent: float = 1.0,\n    seq_len_interpolation_factor: Optional[float] = None,\n    add_binary_head: bool = True,\n    return_embeddings: bool = False,\n    include_embeddings: bool = False,\n    use_full_attention_mask: bool = False,\n    include_hiddens: bool = False,\n    skip_logits: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 model.\n\n    Args:\n        config (TransformerConfig): transformer config\n        num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n        vocab_size (int): vocabulary size\n        max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n        tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n        pre_process (bool): Include embedding layer (used with pipeline parallelism)\n        post_process (bool): Include an output layer (used with pipeline parallelism)\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n        position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n            Defaults is 'learned_absolute'.\n        rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n        seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n        add_binary_head (bool): Whether to add a binary head. Defaults to True.\n        return_embeddings (bool): Whether to return embeddings. Defaults to False.\n        include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n        use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n        include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n        skip_logits (bool): Skip writing the token logits in output dict\n    \"\"\"\n    super(MegatronBioBertModel, self).__init__(config=config)\n    self.post_process = post_process\n    self.add_binary_head = add_binary_head\n    if return_embeddings:\n        assert self.post_process, \"only return embeddings on the last pipeline stage\"\n    # `b` = batch, `s` = sequence.\n    # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n    #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n    self.use_full_attention_mask = use_full_attention_mask\n    self.config: TransformerConfig = config\n    self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n    self.vocab_size = vocab_size\n    self.max_sequence_length = max_sequence_length\n    self.pre_process = pre_process\n    self.post_process = post_process\n    self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n    self.parallel_output = parallel_output\n    self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n    self.position_embedding_type = position_embedding_type\n    self.add_binary_head = add_binary_head\n    self.return_embeddings = return_embeddings\n    self.include_embeddings = include_embeddings\n    self.include_hiddens = include_hiddens\n    self.skip_logits = skip_logits\n\n    # megatron core pipelining currently depends on model type\n    self.model_type = ModelType.encoder_or_decoder\n\n    # Embeddings.\n    if self.pre_process:\n        # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n        # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n        # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n        self.embedding = ESM2Embedding(\n            config=self.config,\n            vocab_size=self.vocab_size,\n            max_sequence_length=self.max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n            # ESM2 NEW ARGS\n            token_dropout=self.config.token_dropout,\n            use_attention_mask=self.config.use_attention_mask,\n            mask_token_id=tokenizer.mask_token_id,\n        )\n\n    if self.position_embedding_type == \"rope\":\n        self.rotary_pos_emb = RotaryEmbedding(\n            kv_channels=self.config.kv_channels,\n            rotary_percent=rotary_percent,\n            rotary_interleaved=self.config.rotary_interleaved,\n            seq_len_interpolation_factor=seq_len_interpolation_factor,\n        )\n\n    # Transformer.\n    self.encoder = TransformerBlock(\n        config=self.config,\n        spec=self.transformer_layer_spec,\n        pre_process=self.pre_process,\n        post_process=self.post_process,\n    )\n\n    # Output\n    if post_process:\n        # TODO: Make sure you are passing in the mpu_vocab_size properly\n        self.lm_head = BertLMHead(\n            config.hidden_size,\n            config,\n        )\n\n        self.output_layer = tensor_parallel.ColumnParallelLinear(\n            config.hidden_size,\n            self.vocab_size,\n            config=config,\n            init_method=config.init_method,\n            bias=True,\n            skip_bias_add=False,\n            gather_output=not self.parallel_output,\n            skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n        )\n\n        self.binary_head = None\n        if self.add_binary_head:\n            # TODO: Shoudl switch this to TE ?\n            self.binary_head = get_linear_layer(\n                config.hidden_size, 2, config.init_method, config.perform_initialization\n            )\n\n            self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n    if self.pre_process or self.post_process:\n        self.setup_embeddings_and_output_layer()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/api/#bionemo.esm2.api.ESM2Model.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, sequence_length) containing the input IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the position IDs.</p> required <code>tokentype_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def embedding_forward(\n    self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n):\n    \"\"\"Forward pass of the embedding layer.\n\n    Args:\n        input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n        position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n        tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n        attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n    \"\"\"\n    # ESM2 Customization: ESM2Embedding forward takes attention_mask\n    # in addition to the args required by LanguageModelEmbedding\n    return self.embedding(\n        input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule","title":"<code>ESMDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>LightningDataModule wrapper of <code>ESMDataset</code>.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>class ESMDataModule(MegatronDataModule):\n    \"\"\"LightningDataModule wrapper of `ESMDataset`.\"\"\"\n\n    def __init__(\n        self,\n        train_cluster_path: str | os.PathLike,\n        train_database_path: str | os.PathLike,\n        valid_cluster_path: str | os.PathLike,\n        valid_database_path: str | os.PathLike,\n        seed: int | None = 42,\n        min_seq_length: int | None = None,\n        max_seq_length: int = 1024,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        num_workers: int = 10,  # TODO(@jomitchell) can this be automatically set?\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n        rampup_batch_size: list[int] | None = None,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        mask_random_prob: float = 0.1,\n        random_mask_strategy: dataset.RandomMaskStrategy = dataset.RandomMaskStrategy.ALL_TOKENS,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        dataloader_type: Literal[\"single\", \"cyclic\"] = \"single\",\n    ) -&gt; None:\n        \"\"\"Initialize the ESMDataModule.\n\n        Args:\n            train_cluster_path: A path to the parquet files containing UniRef90 training clusters.\n            train_database_path: A path to the sqlite file mapping UniRef90 cluster IDs to sequences.\n            valid_cluster_path: A path to the parquet files containing UniRef50 validation clusters.\n            valid_database_path: A path to the sqlite file mapping UniRef50 cluster IDs to sequences.\n            seed: Input random seed. If None, initializes randomly. Defaults to 42.\n            min_seq_length: Whether to pad sequences to a minimum length. If None, no extra padding is added. Defaults\n                to None.\n            max_seq_length: The maximum context length for the ESM transformer. Defaults to 1024.\n            micro_batch_size: Passed to MegatronDataSampler. Defaults to 4.\n            global_batch_size: Passed to MegatronDataSampler.. Defaults to 8.\n            num_workers: The number of workers for the pytorch Dataloaders. Defaults to 10.\n            persistent_workers: Whether to keep the workers alive between epochs. Defaults to True.\n            pin_memory: Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.\n            rampup_batch_size: Passed to MegatronDataSampler. Defaults to None.\n            mask_prob: The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.\n            mask_token_prob: Percentage of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n            mask_random_prob: Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.\n            random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n            tokenizer: The ESM2 tokenizer. Defaults to the one returned by `tokenizer.get_tokenizer()`.\n            dataloader_type: The type of dataloader to use. Defaults to \"single\".\n        \"\"\"\n        super().__init__()\n        self._train_cluster_path = train_cluster_path\n        self._train_database_path = train_database_path\n        self._valid_cluster_path = valid_cluster_path\n        self._valid_database_path = valid_database_path\n        self._seed = seed\n        self._min_seq_length = min_seq_length\n        self._max_seq_length = max_seq_length\n        self._mask_prob = mask_prob\n        self._mask_token_prob = mask_token_prob\n        self._mask_random_prob = mask_random_prob\n        self._random_mask_strategy = random_mask_strategy\n        self._tokenizer = tokenizer\n\n        self._micro_batch_size = micro_batch_size\n        self._num_workers = num_workers\n        self._persistent_workers = persistent_workers\n        self._pin_memory = pin_memory\n\n        self.data_sampler = MegatronDataSampler(\n            seq_len=max_seq_length,\n            micro_batch_size=micro_batch_size,\n            global_batch_size=global_batch_size,\n            dataloader_type=dataloader_type,  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n            rampup_batch_size=rampup_batch_size,\n        )\n\n    @property\n    def tokenizer(self) -&gt; tokenizer.BioNeMoESMTokenizer:\n        \"\"\"Returns the tokenizer.\"\"\"\n        return self._tokenizer\n\n    def setup(self, stage: str = \"\") -&gt; None:\n        \"\"\"Setup the ESMDataModule.\n\n        Args:\n            stage: Unused.\n\n        Raises:\n            RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n        \"\"\"\n        del stage  # Unused.\n\n        if not hasattr(self, \"trainer\") or self.trainer is None:\n            raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n        if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n            logging.warning(\n                \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n                \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n            )\n\n        max_train_steps = self.trainer.max_steps\n        if max_train_steps &lt;= 0:\n            raise RuntimeError(\"Please specify trainer.max_steps\")\n\n        # Create training dataset\n        num_train_samples = int(\n            max_train_steps * self.data_sampler.global_batch_size\n        )  # training data requires upsampling (multiply by max_train_steps) on single MegatronPretrainingRandomSampler\n        self._train_ds = dataset.create_train_dataset(\n            cluster_file=self._train_cluster_path,\n            db_path=self._train_database_path,\n            total_samples=num_train_samples,\n            seed=self._seed,\n            max_seq_length=self._max_seq_length,\n            mask_prob=self._mask_prob,\n            mask_token_prob=self._mask_token_prob,\n            mask_random_prob=self._mask_random_prob,\n            random_mask_strategy=self._random_mask_strategy,\n            tokenizer=self._tokenizer,\n        )\n\n        # Create validation dataset\n        val_clusters = dataset.create_valid_clusters(self._valid_cluster_path)\n        num_val_samples = infer_num_samples(\n            limit_batches=self.trainer.limit_val_batches,\n            num_samples_in_dataset=len(val_clusters),\n            global_batch_size=self.data_sampler.global_batch_size,\n            stage=\"val\",\n        )\n        self._valid_ds = dataset.create_valid_dataset(\n            clusters=self._valid_cluster_path,\n            db_path=self._valid_database_path,\n            total_samples=num_val_samples,\n            seed=self._seed,\n            max_seq_length=self._max_seq_length,\n            mask_prob=self._mask_prob,\n            mask_token_prob=self._mask_token_prob,\n            mask_random_prob=self._mask_random_prob,\n            random_mask_strategy=self._random_mask_strategy,\n            tokenizer=self._tokenizer,\n        )\n\n        assert (\n            hasattr(self, \"trainer\") and self.trainer is not None\n        ), \"Setup should be completed when trainer and config are attached.\"\n\n    def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n        \"\"\"Create dataloader for train, validation, and test stages.\n\n        Args:\n            dataset: The dataset to create the dataloader for.\n            mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n            **kwargs: Additional arguments to pass to the dataloader.\n        \"\"\"\n        self.update_init_global_step()\n        assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            num_workers=self._num_workers,\n            pin_memory=self._pin_memory,\n            persistent_workers=self._persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self._tokenizer.pad_token_id,\n                min_length=self._min_seq_length,\n                max_length=self._max_seq_length,\n            ),\n            **kwargs,\n        )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n        \"\"\"Returns the dataloader for training data.\"\"\"\n        return self._create_dataloader(self._train_ds, mode=\"train\")\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for validation data.\"\"\"\n        return self._create_dataloader(self._valid_ds, mode=\"validation\")\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Raises a not implemented error.\"\"\"\n        raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.tokenizer","title":"<code>tokenizer: tokenizer.BioNeMoESMTokenizer</code>  <code>property</code>","text":"<p>Returns the tokenizer.</p>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.__init__","title":"<code>__init__(train_cluster_path, train_database_path, valid_cluster_path, valid_database_path, seed=42, min_seq_length=None, max_seq_length=1024, micro_batch_size=4, global_batch_size=8, num_workers=10, persistent_workers=True, pin_memory=True, rampup_batch_size=None, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=dataset.RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer(), dataloader_type='single')</code>","text":"<p>Initialize the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>train_cluster_path</code> <code>str | PathLike</code> <p>A path to the parquet files containing UniRef90 training clusters.</p> required <code>train_database_path</code> <code>str | PathLike</code> <p>A path to the sqlite file mapping UniRef90 cluster IDs to sequences.</p> required <code>valid_cluster_path</code> <code>str | PathLike</code> <p>A path to the parquet files containing UniRef50 validation clusters.</p> required <code>valid_database_path</code> <code>str | PathLike</code> <p>A path to the sqlite file mapping UniRef50 cluster IDs to sequences.</p> required <code>seed</code> <code>int | None</code> <p>Input random seed. If None, initializes randomly. Defaults to 42.</p> <code>42</code> <code>min_seq_length</code> <code>int | None</code> <p>Whether to pad sequences to a minimum length. If None, no extra padding is added. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>The maximum context length for the ESM transformer. Defaults to 1024.</p> <code>1024</code> <code>micro_batch_size</code> <code>int</code> <p>Passed to MegatronDataSampler. Defaults to 4.</p> <code>4</code> <code>global_batch_size</code> <code>int</code> <p>Passed to MegatronDataSampler.. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>The number of workers for the pytorch Dataloaders. Defaults to 10.</p> <code>10</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to keep the workers alive between epochs. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.</p> <code>True</code> <code>rampup_batch_size</code> <code>list[int] | None</code> <p>Passed to MegatronDataSampler. Defaults to None.</p> <code>None</code> <code>mask_prob</code> <code>float</code> <p>The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Percentage of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The ESM2 tokenizer. Defaults to the one returned by <code>tokenizer.get_tokenizer()</code>.</p> <code>get_tokenizer()</code> <code>dataloader_type</code> <code>Literal['single', 'cyclic']</code> <p>The type of dataloader to use. Defaults to \"single\".</p> <code>'single'</code> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def __init__(\n    self,\n    train_cluster_path: str | os.PathLike,\n    train_database_path: str | os.PathLike,\n    valid_cluster_path: str | os.PathLike,\n    valid_database_path: str | os.PathLike,\n    seed: int | None = 42,\n    min_seq_length: int | None = None,\n    max_seq_length: int = 1024,\n    micro_batch_size: int = 4,\n    global_batch_size: int = 8,\n    num_workers: int = 10,  # TODO(@jomitchell) can this be automatically set?\n    persistent_workers: bool = True,\n    pin_memory: bool = True,\n    rampup_batch_size: list[int] | None = None,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: dataset.RandomMaskStrategy = dataset.RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    dataloader_type: Literal[\"single\", \"cyclic\"] = \"single\",\n) -&gt; None:\n    \"\"\"Initialize the ESMDataModule.\n\n    Args:\n        train_cluster_path: A path to the parquet files containing UniRef90 training clusters.\n        train_database_path: A path to the sqlite file mapping UniRef90 cluster IDs to sequences.\n        valid_cluster_path: A path to the parquet files containing UniRef50 validation clusters.\n        valid_database_path: A path to the sqlite file mapping UniRef50 cluster IDs to sequences.\n        seed: Input random seed. If None, initializes randomly. Defaults to 42.\n        min_seq_length: Whether to pad sequences to a minimum length. If None, no extra padding is added. Defaults\n            to None.\n        max_seq_length: The maximum context length for the ESM transformer. Defaults to 1024.\n        micro_batch_size: Passed to MegatronDataSampler. Defaults to 4.\n        global_batch_size: Passed to MegatronDataSampler.. Defaults to 8.\n        num_workers: The number of workers for the pytorch Dataloaders. Defaults to 10.\n        persistent_workers: Whether to keep the workers alive between epochs. Defaults to True.\n        pin_memory: Whether to pin GPU memory in the pytorch Dataloaders. Defaults to True.\n        rampup_batch_size: Passed to MegatronDataSampler. Defaults to None.\n        mask_prob: The overall chance of masking a token and having it appear in the loss fn. Defaults to 0.15.\n        mask_token_prob: Percentage of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Percentage of masked tokens assigned to a random amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The ESM2 tokenizer. Defaults to the one returned by `tokenizer.get_tokenizer()`.\n        dataloader_type: The type of dataloader to use. Defaults to \"single\".\n    \"\"\"\n    super().__init__()\n    self._train_cluster_path = train_cluster_path\n    self._train_database_path = train_database_path\n    self._valid_cluster_path = valid_cluster_path\n    self._valid_database_path = valid_database_path\n    self._seed = seed\n    self._min_seq_length = min_seq_length\n    self._max_seq_length = max_seq_length\n    self._mask_prob = mask_prob\n    self._mask_token_prob = mask_token_prob\n    self._mask_random_prob = mask_random_prob\n    self._random_mask_strategy = random_mask_strategy\n    self._tokenizer = tokenizer\n\n    self._micro_batch_size = micro_batch_size\n    self._num_workers = num_workers\n    self._persistent_workers = persistent_workers\n    self._pin_memory = pin_memory\n\n    self.data_sampler = MegatronDataSampler(\n        seq_len=max_seq_length,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        dataloader_type=dataloader_type,  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n        rampup_batch_size=rampup_batch_size,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.setup","title":"<code>setup(stage='')</code>","text":"<p>Setup the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Unused.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the trainer is not attached, or if the trainer's max_steps is not set.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def setup(self, stage: str = \"\") -&gt; None:\n    \"\"\"Setup the ESMDataModule.\n\n    Args:\n        stage: Unused.\n\n    Raises:\n        RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n    \"\"\"\n    del stage  # Unused.\n\n    if not hasattr(self, \"trainer\") or self.trainer is None:\n        raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n    if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n        logging.warning(\n            \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n            \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n        )\n\n    max_train_steps = self.trainer.max_steps\n    if max_train_steps &lt;= 0:\n        raise RuntimeError(\"Please specify trainer.max_steps\")\n\n    # Create training dataset\n    num_train_samples = int(\n        max_train_steps * self.data_sampler.global_batch_size\n    )  # training data requires upsampling (multiply by max_train_steps) on single MegatronPretrainingRandomSampler\n    self._train_ds = dataset.create_train_dataset(\n        cluster_file=self._train_cluster_path,\n        db_path=self._train_database_path,\n        total_samples=num_train_samples,\n        seed=self._seed,\n        max_seq_length=self._max_seq_length,\n        mask_prob=self._mask_prob,\n        mask_token_prob=self._mask_token_prob,\n        mask_random_prob=self._mask_random_prob,\n        random_mask_strategy=self._random_mask_strategy,\n        tokenizer=self._tokenizer,\n    )\n\n    # Create validation dataset\n    val_clusters = dataset.create_valid_clusters(self._valid_cluster_path)\n    num_val_samples = infer_num_samples(\n        limit_batches=self.trainer.limit_val_batches,\n        num_samples_in_dataset=len(val_clusters),\n        global_batch_size=self.data_sampler.global_batch_size,\n        stage=\"val\",\n    )\n    self._valid_ds = dataset.create_valid_dataset(\n        clusters=self._valid_cluster_path,\n        db_path=self._valid_database_path,\n        total_samples=num_val_samples,\n        seed=self._seed,\n        max_seq_length=self._max_seq_length,\n        mask_prob=self._mask_prob,\n        mask_token_prob=self._mask_token_prob,\n        mask_random_prob=self._mask_random_prob,\n        random_mask_strategy=self._random_mask_strategy,\n        tokenizer=self._tokenizer,\n    )\n\n    assert (\n        hasattr(self, \"trainer\") and self.trainer is not None\n    ), \"Setup should be completed when trainer and config are attached.\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Raises a not implemented error.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Raises a not implemented error.\"\"\"\n    raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the dataloader for training data.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n    \"\"\"Returns the dataloader for training data.\"\"\"\n    return self._create_dataloader(self._train_ds, mode=\"train\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/datamodule/#bionemo.esm2.data.datamodule.ESMDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the dataloader for validation data.</p> Source code in <code>bionemo/esm2/data/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for validation data.\"\"\"\n    return self._create_dataloader(self._valid_ds, mode=\"validation\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/","title":"Dataset","text":""},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset","title":"<code>ESMMaskedResidueDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset class for ESM pretraining that implements cluster sampling of UniRef50 and UniRef90 sequences.</p> <p>Megatron-LM expects the input datasets to be indexable, and for the output of the dataset for a given index to be deterministic. In cluster sampling, this can be tricky, since we need to perform weighted sampling over UniRef50 clusters.</p> <p>Here, the getitem(i) returns a randomly sampled UniRef90 sequence from the i % len(dataset) UniRef50 cluster, with i controlling the random seed used for selecting the UniRef90 sequence and performing the masking.</p> <p>Multi-epoch training</p> <p>Currently, this class owns the logic for upsampling proteins for multi-epoch training by directly passing a total_samples that's larger than the number of clusters provided. This is done because megatron training assumes that <code>dataset[i]</code> will always return the exact same tensors in distributed training. Because the we want to vary mask patterns and cluster sampling each time a given cluster is sampled, we create our own pseudo-epochs inside the dataset itself. Eventually we'd like to move away from this paradigm and allow multi-epoch training to vary the dataset's random state through a callback, and allow megatron samplers to handle the epoch-to-epoch shuffling of sample order.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class ESMMaskedResidueDataset(Dataset):\n    \"\"\"Dataset class for ESM pretraining that implements cluster sampling of UniRef50 and UniRef90 sequences.\n\n    Megatron-LM expects the input datasets to be indexable, and for the output of the dataset for a given index to be\n    deterministic. In cluster sampling, this can be tricky, since we need to perform weighted sampling over UniRef50\n    clusters.\n\n    Here, the getitem(i) returns a randomly sampled UniRef90 sequence from the i % len(dataset) UniRef50 cluster, with i\n    controlling the random seed used for selecting the UniRef90 sequence and performing the masking.\n\n    !!! note \"Multi-epoch training\"\n\n        Currently, this class owns the logic for upsampling proteins for multi-epoch training by directly passing a\n        total_samples that's larger than the number of clusters provided. This is done because megatron training assumes\n        that `dataset[i]` will always return the exact same tensors in distributed training. Because the we want to vary\n        mask patterns and cluster sampling each time a given cluster is sampled, we create our own pseudo-epochs inside\n        the dataset itself. Eventually we'd like to move away from this paradigm and allow multi-epoch training to vary\n        the dataset's random state through a callback, and allow megatron samplers to handle the epoch-to-epoch\n        shuffling of sample order.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        protein_dataset: Dataset,\n        clusters: Sequence[Sequence[str]],\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n        max_seq_length: int = 1024,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        mask_random_prob: float = 0.1,\n        random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    ) -&gt; None:\n        \"\"\"Initializes the dataset.\n\n        Args:\n            protein_dataset: Dataset containing protein sequences, indexed by UniRef90 ids.\n            clusters: UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for\n                validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a\n                single UniRef50 id.\n            total_samples: Total number of samples to draw from the dataset.\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n            max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n            mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n            mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n            mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n            random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n            tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n        \"\"\"\n        self.protein_dataset = protein_dataset\n        self.clusters = clusters\n        self.seed = seed\n        self.max_seq_length = max_seq_length\n        self.random_mask_strategy = random_mask_strategy\n\n        if tokenizer.mask_token_id is None:\n            raise ValueError(\"Tokenizer does not have a mask token.\")\n\n        self.mask_config = masking.BertMaskConfig(\n            tokenizer=tokenizer,\n            random_tokens=range(len(tokenizer.all_tokens))\n            if self.random_mask_strategy == RandomMaskStrategy.ALL_TOKENS\n            else range(4, 24),\n            mask_prob=mask_prob,\n            mask_token_prob=mask_token_prob,\n            random_token_prob=mask_random_prob,\n        )\n\n        self.tokenizer = tokenizer\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of clusters, which constitutes a single epoch.\"\"\"\n        return len(self.clusters)\n\n    def __getitem__(self, index: EpochIndex) -&gt; BertSample:\n        \"\"\"Deterministically masks and returns a protein sequence from the dataset.\n\n        This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same\n        cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.\n\n        Args:\n            index: The current epoch and the index of the cluster to sample.\n\n        Returns:\n            A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.\n        \"\"\"\n        # Initialize a random number generator with a seed that is a combination of the dataset seed, epoch, and index.\n        rng = np.random.default_rng([self.seed, index.epoch, index.idx])\n        if not len(self.clusters[index.idx]):\n            raise ValueError(f\"Cluster {index.idx} is empty.\")\n\n        sequence_id = rng.choice(self.clusters[index.idx])\n        sequence = self.protein_dataset[sequence_id]\n\n        # We don't want special tokens before we pass the input to the masking function; we add these in the collate_fn.\n        tokenized_sequence = self._tokenize(sequence)\n        cropped_sequence = _random_crop(tokenized_sequence, self.max_seq_length, rng)\n\n        # Get a single integer seed for torch from our rng, since the index tuple is hard to pass directly to torch.\n        torch_seed = random_utils.get_seed_from_rng(rng)\n        masked_sequence, labels, loss_mask = masking.apply_bert_pretraining_mask(\n            tokenized_sequence=cropped_sequence,  # type: ignore\n            random_seed=torch_seed,\n            mask_config=self.mask_config,\n        )\n\n        return {\n            \"text\": masked_sequence,\n            \"types\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(masked_sequence, dtype=torch.int64),\n            \"labels\": labels,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n        }\n\n    def _tokenize(self, sequence: str) -&gt; torch.Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Deterministically masks and returns a protein sequence from the dataset.</p> <p>This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>EpochIndex</code> <p>The current epoch and the index of the cluster to sample.</p> required <p>Returns:</p> Type Description <code>BertSample</code> <p>A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; BertSample:\n    \"\"\"Deterministically masks and returns a protein sequence from the dataset.\n\n    This method samples from the i % len(dataset) cluster from the input clusters list. Random draws of the same\n    cluster can be achieved by calling this method with i + len(dataset), i.e., wrapping around the dataset length.\n\n    Args:\n        index: The current epoch and the index of the cluster to sample.\n\n    Returns:\n        A (possibly-truncated), masked protein sequence with CLS and EOS tokens and associated mask fields.\n    \"\"\"\n    # Initialize a random number generator with a seed that is a combination of the dataset seed, epoch, and index.\n    rng = np.random.default_rng([self.seed, index.epoch, index.idx])\n    if not len(self.clusters[index.idx]):\n        raise ValueError(f\"Cluster {index.idx} is empty.\")\n\n    sequence_id = rng.choice(self.clusters[index.idx])\n    sequence = self.protein_dataset[sequence_id]\n\n    # We don't want special tokens before we pass the input to the masking function; we add these in the collate_fn.\n    tokenized_sequence = self._tokenize(sequence)\n    cropped_sequence = _random_crop(tokenized_sequence, self.max_seq_length, rng)\n\n    # Get a single integer seed for torch from our rng, since the index tuple is hard to pass directly to torch.\n    torch_seed = random_utils.get_seed_from_rng(rng)\n    masked_sequence, labels, loss_mask = masking.apply_bert_pretraining_mask(\n        tokenized_sequence=cropped_sequence,  # type: ignore\n        random_seed=torch_seed,\n        mask_config=self.mask_config,\n    )\n\n    return {\n        \"text\": masked_sequence,\n        \"types\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(masked_sequence, dtype=torch.int64),\n        \"labels\": labels,\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(masked_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__init__","title":"<code>__init__(protein_dataset, clusters, seed=np.random.SeedSequence().entropy, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>protein_dataset</code> <code>Dataset</code> <p>Dataset containing protein sequences, indexed by UniRef90 ids.</p> required <code>clusters</code> <code>Sequence[Sequence[str]]</code> <p>UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a single UniRef50 id.</p> required <code>total_samples</code> <p>Total number of samples to draw from the dataset.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The input ESM tokenizer. Defaults to the standard ESM tokenizer.</p> <code>get_tokenizer()</code> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __init__(\n    self,\n    protein_dataset: Dataset,\n    clusters: Sequence[Sequence[str]],\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n) -&gt; None:\n    \"\"\"Initializes the dataset.\n\n    Args:\n        protein_dataset: Dataset containing protein sequences, indexed by UniRef90 ids.\n        clusters: UniRef90 ids for all training sequences, bucketed by UniRef50 cluster. Alternatively for\n            validation, this can also just a list of UniRef50 ids, with each entry being a length-1 list with a\n            single UniRef50 id.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n    \"\"\"\n    self.protein_dataset = protein_dataset\n    self.clusters = clusters\n    self.seed = seed\n    self.max_seq_length = max_seq_length\n    self.random_mask_strategy = random_mask_strategy\n\n    if tokenizer.mask_token_id is None:\n        raise ValueError(\"Tokenizer does not have a mask token.\")\n\n    self.mask_config = masking.BertMaskConfig(\n        tokenizer=tokenizer,\n        random_tokens=range(len(tokenizer.all_tokens))\n        if self.random_mask_strategy == RandomMaskStrategy.ALL_TOKENS\n        else range(4, 24),\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        random_token_prob=mask_random_prob,\n    )\n\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ESMMaskedResidueDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of clusters, which constitutes a single epoch.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of clusters, which constitutes a single epoch.\"\"\"\n    return len(self.clusters)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset","title":"<code>ProteinSQLiteDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for protein sequences stored in a SQLite database.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class ProteinSQLiteDataset(Dataset):\n    \"\"\"Dataset for protein sequences stored in a SQLite database.\"\"\"\n\n    def __init__(self, db_path: str | os.PathLike):\n        \"\"\"Initializes the dataset.\n\n        Args:\n            db_path: Path to the SQLite database.\n        \"\"\"\n        self.conn = sqlite3.connect(str(db_path))\n        self.cursor = self.conn.cursor()\n        self._len = None\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of proteins in the dataset.\n\n        Returns:\n            Number of proteins in the dataset.\n        \"\"\"\n        if self._len is None:\n            self.cursor.execute(\"SELECT COUNT(*) FROM protein\")\n            self._len = int(self.cursor.fetchone()[0])\n        return self._len\n\n    def __getitem__(self, idx: str) -&gt; str:\n        \"\"\"Returns the sequence of a protein at a given index.\n\n        TODO: This method may want to support batched indexing for improved performance.\n\n        Args:\n            idx: An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation\n                data, they are UniRef50 IDs.\n\n        Returns:\n            The protein sequence as a string.\n        \"\"\"\n        if not isinstance(idx, str):\n            raise TypeError(f\"Expected string, got {type(idx)}: {idx}.\")\n\n        self.cursor.execute(\"SELECT sequence FROM protein WHERE id = ?\", (idx,))\n        return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the sequence of a protein at a given index.</p> <p>TODO: This method may want to support batched indexing for improved performance.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>str</code> <p>An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation data, they are UniRef50 IDs.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The protein sequence as a string.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __getitem__(self, idx: str) -&gt; str:\n    \"\"\"Returns the sequence of a protein at a given index.\n\n    TODO: This method may want to support batched indexing for improved performance.\n\n    Args:\n        idx: An identifier for the protein sequence. For training data, these are UniRef90 IDs, while for validation\n            data, they are UniRef50 IDs.\n\n    Returns:\n        The protein sequence as a string.\n    \"\"\"\n    if not isinstance(idx, str):\n        raise TypeError(f\"Expected string, got {type(idx)}: {idx}.\")\n\n    self.cursor.execute(\"SELECT sequence FROM protein WHERE id = ?\", (idx,))\n    return self.cursor.fetchone()[0]\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__init__","title":"<code>__init__(db_path)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __init__(self, db_path: str | os.PathLike):\n    \"\"\"Initializes the dataset.\n\n    Args:\n        db_path: Path to the SQLite database.\n    \"\"\"\n    self.conn = sqlite3.connect(str(db_path))\n    self.cursor = self.conn.cursor()\n    self._len = None\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.ProteinSQLiteDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of proteins in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of proteins in the dataset.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of proteins in the dataset.\n\n    Returns:\n        Number of proteins in the dataset.\n    \"\"\"\n    if self._len is None:\n        self.cursor.execute(\"SELECT COUNT(*) FROM protein\")\n        self._len = int(self.cursor.fetchone()[0])\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy","title":"<code>RandomMaskStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for different random masking strategies.</p> <p>In ESM2 pretraining, 15% of all tokens are masked and among which 10% are replaced with a random token. This class controls the set of random tokens to choose from.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>class RandomMaskStrategy(str, Enum):\n    \"\"\"Enum for different random masking strategies.\n\n    In ESM2 pretraining, 15% of all tokens are masked and among which 10% are replaced with a random token. This class controls the set of random tokens to choose from.\n\n    \"\"\"\n\n    AMINO_ACIDS_ONLY = \"amino_acids_only\"\n    \"\"\"Mask only with amino acid tokens.\"\"\"\n\n    ALL_TOKENS = \"all_tokens\"\n    \"\"\"Mask with all tokens in the tokenizer, including special tokens, padding and non-canonical amino acid tokens.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy.ALL_TOKENS","title":"<code>ALL_TOKENS = 'all_tokens'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mask with all tokens in the tokenizer, including special tokens, padding and non-canonical amino acid tokens.</p>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.RandomMaskStrategy.AMINO_ACIDS_ONLY","title":"<code>AMINO_ACIDS_ONLY = 'amino_acids_only'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mask only with amino acid tokens.</p>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_train_dataset","title":"<code>create_train_dataset(cluster_file, db_path, total_samples, seed, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Creates a training dataset for ESM pretraining.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <code>str | PathLike</code> <p>Path to the cluster file. The file should contain a \"ur90_id\" column, where each row contains a list of UniRef90 ids for a single UniRef50 cluster.</p> required <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required <code>total_samples</code> <code>int</code> <p>Total number of samples to draw from the dataset.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> <code>ALL_TOKENS</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The input ESM tokenizer. Defaults to the standard ESM tokenizer.</p> <code>get_tokenizer()</code> <p>Returns:</p> Type Description <p>A dataset for ESM pretraining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cluster file does not exist, the database file does not exist, or the cluster file does not contain a \"ur90_id\" column.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_train_dataset(\n    cluster_file: str | os.PathLike,\n    db_path: str | os.PathLike,\n    total_samples: int,\n    seed: int,\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n):\n    \"\"\"Creates a training dataset for ESM pretraining.\n\n    Args:\n        cluster_file: Path to the cluster file. The file should contain a \"ur90_id\" column, where each row contains a\n            list of UniRef90 ids for a single UniRef50 cluster.\n        db_path: Path to the SQLite database.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_mask_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n        tokenizer: The input ESM tokenizer. Defaults to the standard ESM tokenizer.\n\n    Returns:\n        A dataset for ESM pretraining.\n\n    Raises:\n        ValueError: If the cluster file does not exist, the database file does not exist, or the cluster file does not\n            contain a \"ur90_id\" column.\n    \"\"\"\n    if not Path(cluster_file).exists():\n        raise ValueError(f\"Cluster file {cluster_file} not found.\")\n\n    if not Path(db_path).exists():\n        raise ValueError(f\"Database file {db_path} not found.\")\n\n    cluster_df = pd.read_parquet(cluster_file)\n    if \"ur90_id\" not in cluster_df.columns:\n        raise ValueError(f\"Training cluster file must contain a 'ur90_id' column. Found columns {cluster_df.columns}.\")\n\n    protein_dataset = ProteinSQLiteDataset(db_path)\n    masked_cluster_dataset = ESMMaskedResidueDataset(\n        protein_dataset=protein_dataset,\n        clusters=cluster_df[\"ur90_id\"],\n        seed=seed,\n        max_seq_length=max_seq_length,\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        mask_random_prob=mask_random_prob,\n        random_mask_strategy=random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n\n    return MultiEpochDatasetResampler(masked_cluster_dataset, num_samples=total_samples, shuffle=True, seed=seed)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_valid_clusters","title":"<code>create_valid_clusters(cluster_file)</code>","text":"<p>Create a pandas series of UniRef50 cluster IDs from a cluster parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <code>str | PathLike</code> <p>Path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A pandas series of UniRef50 cluster IDs.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_valid_clusters(cluster_file: str | os.PathLike) -&gt; pd.Series:\n    \"\"\"Create a pandas series of UniRef50 cluster IDs from a cluster parquet file.\n\n    Args:\n        cluster_file: Path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50\n        IDs, with one UniRef50 ID per row.\n\n    Returns:\n        A pandas series of UniRef50 cluster IDs.\n    \"\"\"\n    if not Path(cluster_file).exists():\n        raise ValueError(f\"Cluster file {cluster_file} not found.\")\n\n    cluster_df = pd.read_parquet(cluster_file)\n    if \"ur50_id\" not in cluster_df.columns:\n        raise ValueError(\n            f\"Validation cluster file must contain a 'ur50_id' column. Found columns {cluster_df.columns}.\"\n        )\n    clusters = cluster_df[\"ur50_id\"].apply(lambda x: [x])\n    return clusters\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/dataset/#bionemo.esm2.data.dataset.create_valid_dataset","title":"<code>create_valid_dataset(clusters, db_path, seed, total_samples=None, max_seq_length=1024, mask_prob=0.15, mask_token_prob=0.8, mask_random_prob=0.1, random_mask_strategy=RandomMaskStrategy.ALL_TOKENS, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Creates a validation dataset for ESM pretraining.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_file</code> <p>Clusters as pd.Series, or path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50 IDs, with one UniRef50 ID per row.</p> required <code>db_path</code> <code>str | PathLike</code> <p>Path to the SQLite database.</p> required <code>total_samples</code> <code>int | None</code> <p>Total number of samples to draw from the dataset.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>max_seq_length</code> <code>int</code> <p>Crop long sequences to a maximum of this length, including BOS and EOS tokens.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>The overall probability a token is included in the loss function. Defaults to 0.15.</p> <code>0.15</code> <code>mask_token_prob</code> <code>float</code> <p>Proportion of masked tokens that get assigned the  id. Defaults to 0.8. <code>0.8</code> <code>mask_random_prob</code> <code>float</code> <p>Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.</p> <code>0.1</code> <code>random_masking_strategy</code> <p>Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cluster file does not exist, the database file does not exist, or the cluster file does not contain a \"ur50_id\" column.</p> Source code in <code>bionemo/esm2/data/dataset.py</code> <pre><code>def create_valid_dataset(  # noqa: D417\n    clusters: pd.Series | str | os.PathLike,\n    db_path: str | os.PathLike,\n    seed: int,\n    total_samples: int | None = None,\n    max_seq_length: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    mask_random_prob: float = 0.1,\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n):\n    \"\"\"Creates a validation dataset for ESM pretraining.\n\n    Args:\n        cluster_file: Clusters as pd.Series, or path to the cluster file. The file should contain a single column named \"ur50_id\" with UniRef50\n            IDs, with one UniRef50 ID per row.\n        db_path: Path to the SQLite database.\n        total_samples: Total number of samples to draw from the dataset.\n        seed: Random seed for reproducibility.\n        max_seq_length: Crop long sequences to a maximum of this length, including BOS and EOS tokens.\n        mask_prob: The overall probability a token is included in the loss function. Defaults to 0.15.\n        mask_token_prob: Proportion of masked tokens that get assigned the &lt;MASK&gt; id. Defaults to 0.8.\n        mask_random_prob: Proportion of tokens that get assigned a random natural amino acid. Defaults to 0.1.\n        random_masking_strategy: Whether to replace random masked tokens with all tokens or amino acids only. Defaults to RandomMaskStrategy.ALL_TOKENS.\n\n    Raises:\n        ValueError: If the cluster file does not exist, the database file does not exist, or the cluster file does not\n            contain a \"ur50_id\" column.\n    \"\"\"\n    if isinstance(clusters, (str, os.PathLike)):\n        clusters = create_valid_clusters(clusters)\n\n    elif not isinstance(clusters, pd.Series):\n        raise ValueError(f\"Clusters must be a pandas Series. Got {type(clusters)}.\")\n\n    if not Path(db_path).exists():\n        raise ValueError(f\"Database file {db_path} not found.\")\n\n    protein_dataset = ProteinSQLiteDataset(db_path)\n    masked_dataset = ESMMaskedResidueDataset(\n        protein_dataset=protein_dataset,\n        clusters=clusters,\n        seed=seed,\n        max_seq_length=max_seq_length,\n        mask_prob=mask_prob,\n        mask_token_prob=mask_token_prob,\n        mask_random_prob=mask_random_prob,\n        random_mask_strategy=random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n\n    return MultiEpochDatasetResampler(masked_dataset, num_samples=total_samples, shuffle=True, seed=seed)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/data/tokenizer/","title":"Vendored tokenizer config for facebook/esm2_t33_650M_UR50D","text":"<p>This directory contains the output of</p> <pre><code>from transformers import AutoTokenizer\nAutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\").save_pretrained(\"...\")\n</code></pre> <p>for reproducible results and to reduce reliance on external API calls.</p>"},{"location":"API_reference/bionemo/esm2/model/attention/","title":"Attention","text":""},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2DotProductAttention","title":"<code>ESM2DotProductAttention</code>","text":"<p>               Bases: <code>DotProductAttention</code></p> <p>ESM2-Specific core attention.</p> <p>Region where selective activation recomputation is applied. This region is memory intensive but less compute intensive which makes activation checkpointing more efficient for LLMs (20B+). See Reducing Activation Recomputation in Large Transformer Models: https://arxiv.org/abs/2205.05198 for more details.</p> We use the following notation <p>h: hidden size n: number of attention heads p: number of tensor model parallel partitions b: batch size s: sequence length</p> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>class ESM2DotProductAttention(DotProductAttention):\n    \"\"\"ESM2-Specific core attention.\n\n    Region where selective activation recomputation is applied.\n    This region is memory intensive but less compute intensive which\n    makes activation checkpointing more efficient for LLMs (20B+).\n    See Reducing Activation Recomputation in Large Transformer Models:\n    https://arxiv.org/abs/2205.05198 for more details.\n\n    We use the following notation:\n     h: hidden size\n     n: number of attention heads\n     p: number of tensor model parallel partitions\n     b: batch size\n     s: sequence length\n    \"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        layer_number: int,\n        attn_mask_type: AttnMaskType,\n        attention_type: str,\n        attention_dropout: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Attention class.\n\n        Args:\n            config: The configuration object for the transformer.\n            layer_number: The layer number of the attention module.\n            attn_mask_type: The type of attention mask to be used.\n            attention_type: The type of attention mechanism.\n            attention_dropout: The dropout rate for attention weights. Defaults to None.\n        \"\"\"\n        super().__init__(\n            config=config,\n            layer_number=layer_number,\n            attn_mask_type=attn_mask_type,\n            attention_type=attention_type,\n            attention_dropout=attention_dropout,\n        )\n\n    def forward(\n        self,\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n        attention_mask: Tensor,\n        attn_mask_type: Optional[AttnMaskType] = None,\n        packed_seq_params: Optional[PackedSeqParams] = None,\n    ):\n        \"\"\"Forward pass of the ESM2DotProductAttention module.\n\n        Args:\n            query: The query tensor of shape [sq, b, np, hn].\n            key: The key tensor of shape [sk, b, ng, hn].\n            value: The value tensor of shape [sk, b, ng, hn].\n            attention_mask: The attention mask tensor of shape [b, np, sq, sk].\n            attn_mask_type: The attention mask type, currently unused. Defaults to None.\n            packed_seq_params: The packed sequence parameters. These are used for context parallelism so will be needed\n                to be implemented if we want to support this. Defaults to None.\n\n        Returns:\n            Tensor: The context tensor of shape [sq, b, hp].\n        \"\"\"\n        if packed_seq_params is not None:\n            raise ValueError(\n                \"Packed sequence is not supported by DotProductAttention. \" \"Please use TEDotProductAttention instead.\"\n            )\n\n        # ===================================\n        # Raw attention scores. [b, n/p, s, s]\n        # ===================================\n\n        # expand the key and value [sk, b, ng, hn] -&gt; [sk, b, np, hn]\n        # This is a noop for normal attention where ng == np. When using group query attention this\n        # creates a view that has the keys and values virtually repeated along their dimension to\n        # match the number of queries.\n\n        # attn_mask_type is not used.\n        if (np_ng := self.num_attention_heads_per_partition // self.num_query_groups_per_partition) &gt; 1:\n            key = key.repeat_interleave(np_ng, dim=2)\n            value = value.repeat_interleave(np_ng, dim=2)\n\n        # [b, np, sq, sk]\n        b, np, sq, sk = query.size(1), query.size(2), query.size(0), key.size(0)\n\n        # [sq, b, np, hn] -&gt; [sq, b * np, hn]\n        # This will be a simple view when doing normal attention, but in group query attention\n        # the key and value tensors are repeated to match the queries so you can't use simple strides\n        # to extract the queries.\n        query = query.reshape(sq, b * np, -1)\n        # [sk, b, np, hn] -&gt; [sk, b * np, hn]\n        key = key.view(sk, b * np, -1)\n\n        # preallocting input tensor: [b * np, sq, sk]\n        matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(\n            (b * np, sq, sk),\n            query.dtype,\n            \"mpu\",\n        )\n\n        # Raw attention scores. [b * np, sq, sk]\n        matmul_result = torch.baddbmm(\n            matmul_input_buffer,\n            query.transpose(0, 1),  # [b * np, sq, hn]\n            key.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n            beta=0.0,\n            alpha=(1.0 / self.norm_factor) if self.config.normalize_attention_scores else 1.0,\n        )\n\n        # change view to [b, np, sq, sk]\n        attention_scores = matmul_result.view(b, np, sq, sk)\n\n        # ===========================\n        # Attention probs and dropout\n        # ===========================\n\n        # attention scores and attention mask [b, np, sq, sk]\n        # ESM2 Customization\n        if self.config.use_esm_attention:\n            # NOTE: the slicing here is to make the attention_mask the same shape as the extended\n            # attention mask in ESM2. The multiplication by -3.4028e+38 (float32 min_val) is\n            # similarly motivated by ESM2's masking approach, which forces softmax of attention scores\n            # for masked entries to be close to 0. This number is replaced with min_val of the precision\n            # using min_val instead of -inf is stable in an special case where all sequence is masked\n            min_val = torch.finfo(attention_scores.dtype).min\n\n            attention_probs: Tensor = self.esm2_scale_mask_softmax(\n                attention_scores.masked_fill(attention_mask[:, :, 0:1, :].to(bool), min_val)\n            )\n        # END ESM2 Customization\n        else:\n            attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n\n        if not self.config.sequence_parallel:\n            with tensor_parallel.get_cuda_rng_tracker().fork():\n                attention_probs = self.attention_dropout(attention_probs)\n        else:\n            attention_probs = self.attention_dropout(attention_probs)\n\n        # =========================\n        # Context layer. [sq, b, hp]\n        # =========================\n\n        # value -&gt; context layer.\n        # [sk, b, np, hn] --&gt; [b, np, sq, hn]\n\n        # context layer shape: [b, np, sq, hn]\n        b, np, sq, hn = value.size(1), value.size(2), query.size(0), value.size(3)\n\n        # change view [sk, b * np, hn]\n        value = value.view(value.size(0), b * np, -1)\n\n        # change view [b * np, sq, sk]\n        attention_probs = attention_probs.view(b * np, sq, -1)\n\n        # matmul: [b * np, sq, hn]\n        context = torch.bmm(attention_probs, value.transpose(0, 1))\n\n        # change view [b, np, sq, hn]\n        context = context.view(b, np, sq, hn)\n\n        # [b, np, sq, hn] --&gt; [sq, b, np, hn]\n        context = context.permute(2, 0, 1, 3).contiguous()\n\n        # [sq, b, np, hn] --&gt; [sq, b, hp]\n        context = context.view(sq, b, self.hidden_size_per_partition)\n\n        return context\n\n    def esm2_scale_mask_softmax(\n        self,\n        input: Tensor,\n        mask: Optional[Tensor] = None,\n        scale: Optional[Union[float, int]] = None,\n        mask_func: Optional[Callable] = None,\n    ) -&gt; Tensor:\n        \"\"\"Scale Mask Softmax function.\n\n        Args:\n            input: Tensor of shape (Batch, NP, SK, SQ). The input may or may not have already\n                had a mask applied to it.\n            mask: If a mask is to be applied, it will go here.\n            scale: A scale factor that will be applied before the softmax.\n            mask_func: An optional function to apply to the mask. If None, it is assumed that\n                the input already had the mask applied to it.\n\n        Returns:\n            probs: Tensor of normalized probabilities after the softmax has been applied,\n                of shape (Batch, NP, SK, SQ).\n        \"\"\"\n        if self.attn_mask_type.name != \"padding\":\n            raise ValueError(\n                f\"self.attn_mask_type: {self.attn_mask_type} is not 'padding'. \"\n                \"Only 'padding' type is supported currently.\"\n            )\n\n        original_dtype = input.dtype  # Store original dtype\n        if (\n            original_dtype == torch.float16 or original_dtype == torch.bfloat16\n        ) and self.config.attention_softmax_in_fp32:\n            input = input.float()  # Convert to float32 for softmax\n\n        if scale is not None:\n            input = input * scale  # Apply scaling\n\n        if mask is not None and mask_func is not None:\n            input = mask_func(input, mask)  # Apply mask function if provided\n\n        probs = torch.nn.functional.softmax(input, dim=-1)  # Apply softmax\n\n        if self.config.attention_softmax_in_fp32 and original_dtype in (torch.float16, torch.bfloat16):\n            probs = probs.to(original_dtype)  # Convert back to original dtype if necessary\n\n        return probs\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2DotProductAttention.__init__","title":"<code>__init__(config, layer_number, attn_mask_type, attention_type, attention_dropout=None)</code>","text":"<p>Initializes the Attention class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The configuration object for the transformer.</p> required <code>layer_number</code> <code>int</code> <p>The layer number of the attention module.</p> required <code>attn_mask_type</code> <code>AttnMaskType</code> <p>The type of attention mask to be used.</p> required <code>attention_type</code> <code>str</code> <p>The type of attention mechanism.</p> required <code>attention_dropout</code> <code>Optional[float]</code> <p>The dropout rate for attention weights. Defaults to None.</p> <code>None</code> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    layer_number: int,\n    attn_mask_type: AttnMaskType,\n    attention_type: str,\n    attention_dropout: Optional[float] = None,\n) -&gt; None:\n    \"\"\"Initializes the Attention class.\n\n    Args:\n        config: The configuration object for the transformer.\n        layer_number: The layer number of the attention module.\n        attn_mask_type: The type of attention mask to be used.\n        attention_type: The type of attention mechanism.\n        attention_dropout: The dropout rate for attention weights. Defaults to None.\n    \"\"\"\n    super().__init__(\n        config=config,\n        layer_number=layer_number,\n        attn_mask_type=attn_mask_type,\n        attention_type=attention_type,\n        attention_dropout=attention_dropout,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2DotProductAttention.esm2_scale_mask_softmax","title":"<code>esm2_scale_mask_softmax(input, mask=None, scale=None, mask_func=None)</code>","text":"<p>Scale Mask Softmax function.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Tensor of shape (Batch, NP, SK, SQ). The input may or may not have already had a mask applied to it.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If a mask is to be applied, it will go here.</p> <code>None</code> <code>scale</code> <code>Optional[Union[float, int]]</code> <p>A scale factor that will be applied before the softmax.</p> <code>None</code> <code>mask_func</code> <code>Optional[Callable]</code> <p>An optional function to apply to the mask. If None, it is assumed that the input already had the mask applied to it.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>probs</code> <code>Tensor</code> <p>Tensor of normalized probabilities after the softmax has been applied, of shape (Batch, NP, SK, SQ).</p> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>def esm2_scale_mask_softmax(\n    self,\n    input: Tensor,\n    mask: Optional[Tensor] = None,\n    scale: Optional[Union[float, int]] = None,\n    mask_func: Optional[Callable] = None,\n) -&gt; Tensor:\n    \"\"\"Scale Mask Softmax function.\n\n    Args:\n        input: Tensor of shape (Batch, NP, SK, SQ). The input may or may not have already\n            had a mask applied to it.\n        mask: If a mask is to be applied, it will go here.\n        scale: A scale factor that will be applied before the softmax.\n        mask_func: An optional function to apply to the mask. If None, it is assumed that\n            the input already had the mask applied to it.\n\n    Returns:\n        probs: Tensor of normalized probabilities after the softmax has been applied,\n            of shape (Batch, NP, SK, SQ).\n    \"\"\"\n    if self.attn_mask_type.name != \"padding\":\n        raise ValueError(\n            f\"self.attn_mask_type: {self.attn_mask_type} is not 'padding'. \"\n            \"Only 'padding' type is supported currently.\"\n        )\n\n    original_dtype = input.dtype  # Store original dtype\n    if (\n        original_dtype == torch.float16 or original_dtype == torch.bfloat16\n    ) and self.config.attention_softmax_in_fp32:\n        input = input.float()  # Convert to float32 for softmax\n\n    if scale is not None:\n        input = input * scale  # Apply scaling\n\n    if mask is not None and mask_func is not None:\n        input = mask_func(input, mask)  # Apply mask function if provided\n\n    probs = torch.nn.functional.softmax(input, dim=-1)  # Apply softmax\n\n    if self.config.attention_softmax_in_fp32 and original_dtype in (torch.float16, torch.bfloat16):\n        probs = probs.to(original_dtype)  # Convert back to original dtype if necessary\n\n    return probs\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2DotProductAttention.forward","title":"<code>forward(query, key, value, attention_mask, attn_mask_type=None, packed_seq_params=None)</code>","text":"<p>Forward pass of the ESM2DotProductAttention module.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Tensor</code> <p>The query tensor of shape [sq, b, np, hn].</p> required <code>key</code> <code>Tensor</code> <p>The key tensor of shape [sk, b, ng, hn].</p> required <code>value</code> <code>Tensor</code> <p>The value tensor of shape [sk, b, ng, hn].</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor of shape [b, np, sq, sk].</p> required <code>attn_mask_type</code> <code>Optional[AttnMaskType]</code> <p>The attention mask type, currently unused. Defaults to None.</p> <code>None</code> <code>packed_seq_params</code> <code>Optional[PackedSeqParams]</code> <p>The packed sequence parameters. These are used for context parallelism so will be needed to be implemented if we want to support this. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The context tensor of shape [sq, b, hp].</p> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>def forward(\n    self,\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    attention_mask: Tensor,\n    attn_mask_type: Optional[AttnMaskType] = None,\n    packed_seq_params: Optional[PackedSeqParams] = None,\n):\n    \"\"\"Forward pass of the ESM2DotProductAttention module.\n\n    Args:\n        query: The query tensor of shape [sq, b, np, hn].\n        key: The key tensor of shape [sk, b, ng, hn].\n        value: The value tensor of shape [sk, b, ng, hn].\n        attention_mask: The attention mask tensor of shape [b, np, sq, sk].\n        attn_mask_type: The attention mask type, currently unused. Defaults to None.\n        packed_seq_params: The packed sequence parameters. These are used for context parallelism so will be needed\n            to be implemented if we want to support this. Defaults to None.\n\n    Returns:\n        Tensor: The context tensor of shape [sq, b, hp].\n    \"\"\"\n    if packed_seq_params is not None:\n        raise ValueError(\n            \"Packed sequence is not supported by DotProductAttention. \" \"Please use TEDotProductAttention instead.\"\n        )\n\n    # ===================================\n    # Raw attention scores. [b, n/p, s, s]\n    # ===================================\n\n    # expand the key and value [sk, b, ng, hn] -&gt; [sk, b, np, hn]\n    # This is a noop for normal attention where ng == np. When using group query attention this\n    # creates a view that has the keys and values virtually repeated along their dimension to\n    # match the number of queries.\n\n    # attn_mask_type is not used.\n    if (np_ng := self.num_attention_heads_per_partition // self.num_query_groups_per_partition) &gt; 1:\n        key = key.repeat_interleave(np_ng, dim=2)\n        value = value.repeat_interleave(np_ng, dim=2)\n\n    # [b, np, sq, sk]\n    b, np, sq, sk = query.size(1), query.size(2), query.size(0), key.size(0)\n\n    # [sq, b, np, hn] -&gt; [sq, b * np, hn]\n    # This will be a simple view when doing normal attention, but in group query attention\n    # the key and value tensors are repeated to match the queries so you can't use simple strides\n    # to extract the queries.\n    query = query.reshape(sq, b * np, -1)\n    # [sk, b, np, hn] -&gt; [sk, b * np, hn]\n    key = key.view(sk, b * np, -1)\n\n    # preallocting input tensor: [b * np, sq, sk]\n    matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(\n        (b * np, sq, sk),\n        query.dtype,\n        \"mpu\",\n    )\n\n    # Raw attention scores. [b * np, sq, sk]\n    matmul_result = torch.baddbmm(\n        matmul_input_buffer,\n        query.transpose(0, 1),  # [b * np, sq, hn]\n        key.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n        beta=0.0,\n        alpha=(1.0 / self.norm_factor) if self.config.normalize_attention_scores else 1.0,\n    )\n\n    # change view to [b, np, sq, sk]\n    attention_scores = matmul_result.view(b, np, sq, sk)\n\n    # ===========================\n    # Attention probs and dropout\n    # ===========================\n\n    # attention scores and attention mask [b, np, sq, sk]\n    # ESM2 Customization\n    if self.config.use_esm_attention:\n        # NOTE: the slicing here is to make the attention_mask the same shape as the extended\n        # attention mask in ESM2. The multiplication by -3.4028e+38 (float32 min_val) is\n        # similarly motivated by ESM2's masking approach, which forces softmax of attention scores\n        # for masked entries to be close to 0. This number is replaced with min_val of the precision\n        # using min_val instead of -inf is stable in an special case where all sequence is masked\n        min_val = torch.finfo(attention_scores.dtype).min\n\n        attention_probs: Tensor = self.esm2_scale_mask_softmax(\n            attention_scores.masked_fill(attention_mask[:, :, 0:1, :].to(bool), min_val)\n        )\n    # END ESM2 Customization\n    else:\n        attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)\n\n    # This is actually dropping out entire tokens to attend to, which might\n    # seem a bit unusual, but is taken from the original Transformer paper.\n\n    if not self.config.sequence_parallel:\n        with tensor_parallel.get_cuda_rng_tracker().fork():\n            attention_probs = self.attention_dropout(attention_probs)\n    else:\n        attention_probs = self.attention_dropout(attention_probs)\n\n    # =========================\n    # Context layer. [sq, b, hp]\n    # =========================\n\n    # value -&gt; context layer.\n    # [sk, b, np, hn] --&gt; [b, np, sq, hn]\n\n    # context layer shape: [b, np, sq, hn]\n    b, np, sq, hn = value.size(1), value.size(2), query.size(0), value.size(3)\n\n    # change view [sk, b * np, hn]\n    value = value.view(value.size(0), b * np, -1)\n\n    # change view [b * np, sq, sk]\n    attention_probs = attention_probs.view(b * np, sq, -1)\n\n    # matmul: [b * np, sq, hn]\n    context = torch.bmm(attention_probs, value.transpose(0, 1))\n\n    # change view [b, np, sq, hn]\n    context = context.view(b, np, sq, hn)\n\n    # [b, np, sq, hn] --&gt; [sq, b, np, hn]\n    context = context.permute(2, 0, 1, 3).contiguous()\n\n    # [sq, b, np, hn] --&gt; [sq, b, hp]\n    context = context.view(sq, b, self.hidden_size_per_partition)\n\n    return context\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2TEDotProductAttention","title":"<code>ESM2TEDotProductAttention</code>","text":"<p>               Bases: <code>TEDotProductAttention</code></p> <p>ESM2-Specific transformer engine core attention.</p> <p>Override the softmax_scale to 1.0 to match the ESM2 implementation while keeping the rest from the original TEDotProductAttention.</p> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>class ESM2TEDotProductAttention(TEDotProductAttention):\n    \"\"\"ESM2-Specific transformer engine core attention.\n\n    Override the softmax_scale to 1.0 to match the ESM2 implementation while keeping the rest from the original TEDotProductAttention.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        layer_number: int,\n        attn_mask_type: AttnMaskType,\n        attention_type: str,\n        attention_dropout: float | None = None,\n    ):\n        \"\"\"Initialize ESM2TEDotProductAttention.\"\"\"\n        self.config = config\n        self.te_forward_mask_type = False\n        self.qkv_format: str = \"sbhd\"\n\n        if self.config.apply_query_key_layer_scaling != bool(int(os.getenv(\"NVTE_APPLY_QK_LAYER_SCALING\", \"0\"))):\n            raise ValueError(\n                f\"apply_query_key_layer_scaling is {self.config.apply_query_key_layer_scaling} \"\n                f\"but environment variable NVTE_APPLY_QK_LAYER_SCALING is \"\n                f\"{os.getenv('NVTE_APPLY_QK_LAYER_SCALING')}. Transformer Engine does not support \"\n                f\"setting query key layer scaling via argument, so these two must match.\"\n            )\n\n        extra_kwargs = {}\n        if _te_version &gt;= packaging.version.Version(\"0.11.0\"):\n            extra_kwargs[\"num_gqa_groups\"] = self.config.num_query_groups\n        elif self.config.num_query_groups != self.config.num_attention_heads:\n            raise ValueError(\n                f\"Transformer Engine v{_te_version} does not support Grouped Query Attention, \"\n                f\"use a newer version of Transformer Engine. \"\n                f\"(num_query_groups ({self.config.num_query_groups}) != \"\n                f\"num_attention_heads ({self.config.num_attention_heads}))\"\n            )\n\n        if _te_version &gt;= packaging.version.Version(\"0.10.0\"):\n            extra_kwargs[\"attention_type\"] = attention_type\n            # older version don't need attention_type\n\n        if _te_version &gt; packaging.version.Version(\"0.12.0\"):\n            self.te_forward_mask_type = True\n\n        # Only Transformer-Engine version &gt;= 1.0.0 supports context parallelism\n        if _te_version &gt;= packaging.version.Version(\"1.0.0\"):\n            if getattr(TEDotProductAttention, \"cp_stream\") is None:\n                TEDotProductAttention.cp_stream = torch.cuda.Stream()\n            extra_kwargs[\"cp_group\"] = get_context_parallel_group(check_initialized=False)\n            extra_kwargs[\"cp_global_ranks\"] = get_context_parallel_global_ranks(check_initialized=False)\n            extra_kwargs[\"cp_stream\"] = TEDotProductAttention.cp_stream\n        else:\n            assert (\n                self.config.context_parallel_size == 1\n            ), \"Only Transformer-Engine version &gt;= 1.0.0 supports context parallelism!\"\n\n        if self.config.deterministic_mode:\n            if int(os.getenv(\"NVTE_ALLOW_NONDETERMINISTIC_ALGO\", \"1\")) != 0:\n                raise RuntimeError(\n                    \"deterministic_mode is on and we are using DotProductAttention from \"\n                    \"Transformer Engine, but NVTE_ALLOW_NONDETERMINISTIC_ALGO is not 0. \"\n                    f\"Currently set to: {os.getenv('NVTE_ALLOW_NONDETERMINISTIC_ALGO', 'not set')}.\"\n                )\n\n        if config.window_size is not None:\n            # Check version\n            assert _te_version &gt;= packaging.version.Version(\"1.2.0\"), (\n                f\"Transformer-Engine version ({str(_te_version)}) must be &gt;= 1.2.0 to support\"\n                \"sliding window attention.\"\n            )\n            extra_kwargs[\"window_size\"] = config.window_size\n\n        super(TEDotProductAttention, self).__init__(\n            num_attention_heads=self.config.num_attention_heads,\n            kv_channels=self.config.kv_channels,\n            attention_dropout=(self.config.attention_dropout if attention_dropout is None else attention_dropout),\n            attn_mask_type=attn_mask_type.name,\n            sequence_parallel=self.config.sequence_parallel,\n            tp_size=self.config.tensor_model_parallel_size,\n            get_rng_state_tracker=(get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None),\n            tp_group=get_tensor_model_parallel_group(check_initialized=False),\n            layer_number=layer_number,\n            softmax_scale=1.0,  # TODO subclassing only changes softmax_scale from None to 1.0. Upstream to make this exposed without subclassing\n            **extra_kwargs,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/attention/#bionemo.esm2.model.attention.ESM2TEDotProductAttention.__init__","title":"<code>__init__(config, layer_number, attn_mask_type, attention_type, attention_dropout=None)</code>","text":"<p>Initialize ESM2TEDotProductAttention.</p> Source code in <code>bionemo/esm2/model/attention.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    layer_number: int,\n    attn_mask_type: AttnMaskType,\n    attention_type: str,\n    attention_dropout: float | None = None,\n):\n    \"\"\"Initialize ESM2TEDotProductAttention.\"\"\"\n    self.config = config\n    self.te_forward_mask_type = False\n    self.qkv_format: str = \"sbhd\"\n\n    if self.config.apply_query_key_layer_scaling != bool(int(os.getenv(\"NVTE_APPLY_QK_LAYER_SCALING\", \"0\"))):\n        raise ValueError(\n            f\"apply_query_key_layer_scaling is {self.config.apply_query_key_layer_scaling} \"\n            f\"but environment variable NVTE_APPLY_QK_LAYER_SCALING is \"\n            f\"{os.getenv('NVTE_APPLY_QK_LAYER_SCALING')}. Transformer Engine does not support \"\n            f\"setting query key layer scaling via argument, so these two must match.\"\n        )\n\n    extra_kwargs = {}\n    if _te_version &gt;= packaging.version.Version(\"0.11.0\"):\n        extra_kwargs[\"num_gqa_groups\"] = self.config.num_query_groups\n    elif self.config.num_query_groups != self.config.num_attention_heads:\n        raise ValueError(\n            f\"Transformer Engine v{_te_version} does not support Grouped Query Attention, \"\n            f\"use a newer version of Transformer Engine. \"\n            f\"(num_query_groups ({self.config.num_query_groups}) != \"\n            f\"num_attention_heads ({self.config.num_attention_heads}))\"\n        )\n\n    if _te_version &gt;= packaging.version.Version(\"0.10.0\"):\n        extra_kwargs[\"attention_type\"] = attention_type\n        # older version don't need attention_type\n\n    if _te_version &gt; packaging.version.Version(\"0.12.0\"):\n        self.te_forward_mask_type = True\n\n    # Only Transformer-Engine version &gt;= 1.0.0 supports context parallelism\n    if _te_version &gt;= packaging.version.Version(\"1.0.0\"):\n        if getattr(TEDotProductAttention, \"cp_stream\") is None:\n            TEDotProductAttention.cp_stream = torch.cuda.Stream()\n        extra_kwargs[\"cp_group\"] = get_context_parallel_group(check_initialized=False)\n        extra_kwargs[\"cp_global_ranks\"] = get_context_parallel_global_ranks(check_initialized=False)\n        extra_kwargs[\"cp_stream\"] = TEDotProductAttention.cp_stream\n    else:\n        assert (\n            self.config.context_parallel_size == 1\n        ), \"Only Transformer-Engine version &gt;= 1.0.0 supports context parallelism!\"\n\n    if self.config.deterministic_mode:\n        if int(os.getenv(\"NVTE_ALLOW_NONDETERMINISTIC_ALGO\", \"1\")) != 0:\n            raise RuntimeError(\n                \"deterministic_mode is on and we are using DotProductAttention from \"\n                \"Transformer Engine, but NVTE_ALLOW_NONDETERMINISTIC_ALGO is not 0. \"\n                f\"Currently set to: {os.getenv('NVTE_ALLOW_NONDETERMINISTIC_ALGO', 'not set')}.\"\n            )\n\n    if config.window_size is not None:\n        # Check version\n        assert _te_version &gt;= packaging.version.Version(\"1.2.0\"), (\n            f\"Transformer-Engine version ({str(_te_version)}) must be &gt;= 1.2.0 to support\"\n            \"sliding window attention.\"\n        )\n        extra_kwargs[\"window_size\"] = config.window_size\n\n    super(TEDotProductAttention, self).__init__(\n        num_attention_heads=self.config.num_attention_heads,\n        kv_channels=self.config.kv_channels,\n        attention_dropout=(self.config.attention_dropout if attention_dropout is None else attention_dropout),\n        attn_mask_type=attn_mask_type.name,\n        sequence_parallel=self.config.sequence_parallel,\n        tp_size=self.config.tensor_model_parallel_size,\n        get_rng_state_tracker=(get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None),\n        tp_group=get_tensor_model_parallel_group(check_initialized=False),\n        layer_number=layer_number,\n        softmax_scale=1.0,  # TODO subclassing only changes softmax_scale from None to 1.0. Upstream to make this exposed without subclassing\n        **extra_kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/","title":"Embedding","text":""},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding","title":"<code>ESM2Embedding</code>","text":"<p>               Bases: <code>LanguageModelEmbedding</code></p> <p>ESM2 Embedding with custom logic for attention masking and token dropout.</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>class ESM2Embedding(LanguageModelEmbedding):\n    \"\"\"ESM2 Embedding with custom logic for attention masking and token dropout.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        vocab_size: int,\n        max_sequence_length: int,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"rope\",\n        num_tokentypes: int = 0,\n        # ESM2 NEW ARGS\n        token_dropout: bool = True,\n        use_attention_mask: bool = True,\n        mask_token_id: Optional[int] = torch.nan,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 Embedding module.\"\"\"\n        super().__init__(\n            config=config,\n            vocab_size=vocab_size,\n            max_sequence_length=max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n        )\n        self.token_dropout = token_dropout\n        self.use_attention_mask = use_attention_mask\n        self.mask_token_id = mask_token_id\n\n    @property\n    def dtype(self) -&gt; torch.dtype:\n        \"\"\"The dtype of the embedding weights.\"\"\"\n        return self.word_embeddings.weight.dtype\n\n    def _apply_esm2_customization(\n        self, word_embeddings: Tensor, input_ids: Tensor, attention_mask: Tensor\n    ) -&gt; Tuple[Tensor, Tensor]:\n        \"\"\"ESM2 customization for attention masking and token dropout.\n\n        Args:\n            word_embeddings (Tensor[float]): The input tokens. Shape: [b, s, h]\n            input_ids (Tensor[int]): The input tokens. Shape: [b, s]\n            attention_mask (Tensor[bool]): attention mask. Shape: [b, s]\n\n        Returns:\n            Tuple[Tensor, Tensor]: (Updated embeddings, embedding mask) Shape: ([b, s, h], [b, s])\n        \"\"\"\n        embeddings_mask = None\n        if attention_mask is not None and (self.token_dropout or self.use_attention_mask):\n            embeddings_mask = attention_mask\n\n        if embeddings_mask is not None and self.token_dropout:\n            word_embeddings = word_embeddings.masked_fill((input_ids == self.mask_token_id).unsqueeze(-1), 0.0)\n            src_lengths = embeddings_mask.sum(-1)\n            mask_ratio_observed = (input_ids == self.mask_token_id).sum(-1).to(self.dtype) / src_lengths\n\n            scale_factor = (1 - ESM2_MASK_RATIO_TRAIN) / (1 - mask_ratio_observed)[:, None, None]\n            word_embeddings = (word_embeddings * scale_factor).to(word_embeddings.dtype)\n        if embeddings_mask is not None and self.use_attention_mask:\n            word_embeddings = (word_embeddings * embeddings_mask.unsqueeze(-1)).to(word_embeddings.dtype)\n        return word_embeddings, embeddings_mask\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        tokentype_ids: Optional[int] = None,\n        attention_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Forward pass of the embedding module.\n\n        Args:\n            input_ids (Tensor): The input tokens. Shape: [b, s]\n            position_ids (Tensor): The position id's used to calculate position embeddings. Shape: [b, s]\n            tokentype_ids (int, optional): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None\n            attention_mask (Tensor): attention mask. Shape: [b, s]\n\n        Returns:\n            Tensor: The output embeddings\n        \"\"\"\n        word_embeddings = self.word_embeddings(input_ids)  # [b, s, h]\n\n        # ESM2 Customization\n        word_embeddings, embeddings_mask = self._apply_esm2_customization(word_embeddings, input_ids, attention_mask)\n\n        if self.add_position_embedding:\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings = word_embeddings + position_embeddings\n        else:\n            embeddings = word_embeddings\n\n        # ESM2 Customization: include attention masking from ESM2\n        if embeddings_mask is not None and self.use_attention_mask:\n            embeddings = (embeddings * embeddings_mask.unsqueeze(-1)).to(embeddings.dtype)\n\n        # Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].\n        embeddings = embeddings.transpose(0, 1).contiguous()\n\n        if tokentype_ids is not None:\n            if self.tokentype_embeddings is None:\n                raise ValueError(\"tokentype_embedding is needed to process tokentype_ids\")\n            # [b s h] -&gt; [s b h] (So that it can be added with embeddings)\n            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)\n            embeddings = embeddings + tokentype_embedding\n        else:\n            assert self.tokentype_embeddings is None\n\n        # If the input flag for fp32 residual connection is set, convert for float.\n        if self.config.fp32_residual_connection:\n            embeddings = embeddings.float()\n\n        # Dropout.\n        if self.config.sequence_parallel:\n            embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)\n            # `scatter_to_sequence_parallel_region` returns a view, which prevents\n            # the original tensor from being garbage collected. Clone to facilitate GC.\n            # Has a small runtime cost (~0.5%).\n            if self.config.clone_scatter_output_in_embedding:\n                embeddings = embeddings.clone()\n            with tensor_parallel.get_cuda_rng_tracker().fork():\n                embeddings = self.embedding_dropout(embeddings)\n        else:\n            embeddings = self.embedding_dropout(embeddings)\n\n        return embeddings\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.dtype","title":"<code>dtype: torch.dtype</code>  <code>property</code>","text":"<p>The dtype of the embedding weights.</p>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.__init__","title":"<code>__init__(config, vocab_size, max_sequence_length, position_embedding_type='rope', num_tokentypes=0, token_dropout=True, use_attention_mask=True, mask_token_id=torch.nan)</code>","text":"<p>Initialize the ESM2 Embedding module.</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    vocab_size: int,\n    max_sequence_length: int,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"rope\",\n    num_tokentypes: int = 0,\n    # ESM2 NEW ARGS\n    token_dropout: bool = True,\n    use_attention_mask: bool = True,\n    mask_token_id: Optional[int] = torch.nan,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 Embedding module.\"\"\"\n    super().__init__(\n        config=config,\n        vocab_size=vocab_size,\n        max_sequence_length=max_sequence_length,\n        position_embedding_type=position_embedding_type,\n        num_tokentypes=num_tokentypes,\n    )\n    self.token_dropout = token_dropout\n    self.use_attention_mask = use_attention_mask\n    self.mask_token_id = mask_token_id\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/embedding/#bionemo.esm2.model.embedding.ESM2Embedding.forward","title":"<code>forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding module.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tokens. Shape: [b, s]</p> required <code>position_ids</code> <code>Tensor</code> <p>The position id's used to calculate position embeddings. Shape: [b, s]</p> required <code>tokentype_ids</code> <code>int</code> <p>The token type ids. Used when args.bert_binary_head is set to True. Defaults to None</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>attention mask. Shape: [b, s]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The output embeddings</p> Source code in <code>bionemo/esm2/model/embedding.py</code> <pre><code>def forward(\n    self,\n    input_ids: Tensor,\n    position_ids: Tensor,\n    tokentype_ids: Optional[int] = None,\n    attention_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the embedding module.\n\n    Args:\n        input_ids (Tensor): The input tokens. Shape: [b, s]\n        position_ids (Tensor): The position id's used to calculate position embeddings. Shape: [b, s]\n        tokentype_ids (int, optional): The token type ids. Used when args.bert_binary_head is set to True. Defaults to None\n        attention_mask (Tensor): attention mask. Shape: [b, s]\n\n    Returns:\n        Tensor: The output embeddings\n    \"\"\"\n    word_embeddings = self.word_embeddings(input_ids)  # [b, s, h]\n\n    # ESM2 Customization\n    word_embeddings, embeddings_mask = self._apply_esm2_customization(word_embeddings, input_ids, attention_mask)\n\n    if self.add_position_embedding:\n        position_embeddings = self.position_embeddings(position_ids)\n        embeddings = word_embeddings + position_embeddings\n    else:\n        embeddings = word_embeddings\n\n    # ESM2 Customization: include attention masking from ESM2\n    if embeddings_mask is not None and self.use_attention_mask:\n        embeddings = (embeddings * embeddings_mask.unsqueeze(-1)).to(embeddings.dtype)\n\n    # Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].\n    embeddings = embeddings.transpose(0, 1).contiguous()\n\n    if tokentype_ids is not None:\n        if self.tokentype_embeddings is None:\n            raise ValueError(\"tokentype_embedding is needed to process tokentype_ids\")\n        # [b s h] -&gt; [s b h] (So that it can be added with embeddings)\n        tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(1, 0, 2)\n        embeddings = embeddings + tokentype_embedding\n    else:\n        assert self.tokentype_embeddings is None\n\n    # If the input flag for fp32 residual connection is set, convert for float.\n    if self.config.fp32_residual_connection:\n        embeddings = embeddings.float()\n\n    # Dropout.\n    if self.config.sequence_parallel:\n        embeddings = tensor_parallel.scatter_to_sequence_parallel_region(embeddings)\n        # `scatter_to_sequence_parallel_region` returns a view, which prevents\n        # the original tensor from being garbage collected. Clone to facilitate GC.\n        # Has a small runtime cost (~0.5%).\n        if self.config.clone_scatter_output_in_embedding:\n            embeddings = embeddings.clone()\n        with tensor_parallel.get_cuda_rng_tracker().fork():\n            embeddings = self.embedding_dropout(embeddings)\n    else:\n        embeddings = self.embedding_dropout(embeddings)\n\n    return embeddings\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/","title":"Model","text":""},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Config","title":"<code>ESM2Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig</code>, <code>IOMixinWithGettersSetters</code></p> <p>Configuration class for ESM2 model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2Config(ESM2GenericConfig, iom.IOMixinWithGettersSetters):\n    \"\"\"Configuration class for ESM2 model.\"\"\"\n\n    model_cls: Type[ESM2Model] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2GenericConfig","title":"<code>ESM2GenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[ESM2ModelT, MegatronLossType]</code></p> <p>Configuration class for ESM2 model.</p> <p>Attributes:</p> Name Type Description <code>num_layers</code> <code>int</code> <p>Number of layers in the model.</p> <code>hidden_size</code> <code>int</code> <p>Hidden size of the model.</p> <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads in the model.</p> <code>ffn_hidden_size</code> <code>int</code> <p>Hidden size of the feed-forward network.</p> <code>hidden_dropout</code> <code>float</code> <p>Dropout rate for hidden layers.</p> <code>attention_dropout</code> <code>float</code> <p>Dropout rate for attention layers.</p> <code>apply_residual_connection_post_layernorm</code> <code>bool</code> <p>Whether to apply residual connection after layer normalization.</p> <code>layernorm_epsilon</code> <code>float</code> <p>Epsilon value for layer normalization.</p> <code>layernorm_zero_centered_gamma</code> <code>float</code> <p>Whether to zero-center the gamma parameter in layer normalization.</p> <code>activation_func</code> <code>Callable</code> <p>Activation function used in the model.</p> <code>init_method_std</code> <code>float</code> <p>Standard deviation for weight initialization.</p> <code>apply_query_key_layer_scaling</code> <code>float</code> <p>Whether to apply scaling to query and key layers.</p> <code>masked_softmax_fusion</code> <code>float</code> <p>Whether to use a kernel that fuses attention softmax with its mask.</p> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>Whether to share embeddings and output weights.</p> <code>enable_autocast</code> <code>bool</code> <p>Whether to enable autocast for mixed precision.</p> <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>BiobertSpecOption for the model.</p> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Type of position embedding used in the model.</p> <code>seq_length</code> <code>int</code> <p>Length of the input sequence.</p> <code>make_vocab_size_divisible_by</code> <code>int</code> <p>Make the vocabulary size divisible by this value.</p> <code>token_dropout</code> <code>bool</code> <p>Whether to apply token dropout.</p> <code>use_attention_mask</code> <code>bool</code> <p>Whether to use attention mask.</p> <code>use_esm_attention</code> <code>bool</code> <p>Whether to use ESM attention.</p> <code>attention_softmax_in_fp32</code> <code>bool</code> <p>Whether to use fp32 for attention softmax.</p> <code>optimizer_fn</code> <code>Optional[Callable[[MegatronBioBertModel], Optimizer]]</code> <p>Optional optimizer function for the model.</p> <code>parallel_output</code> <code>bool</code> <p>Whether to use parallel output.</p> <code>rotary_base</code> <code>int</code> <p>Base value for rotary positional encoding.</p> <code>rotary_percent</code> <code>float</code> <p>Percentage of rotary positional encoding.</p> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length.</p> <code>get_attention_mask_from_fusion</code> <code>Optional[float]</code> <p>Whether to get attention mask from fusion.</p> <code>nemo1_ckpt_path</code> <code>str | None</code> <p>Path to NEMO1 checkpoint.</p> <code>return_only_hidden_states</code> <code>bool</code> <p>Whether to return only hidden states.</p> <code>loss_reduction_class</code> <code>bool</code> <p>Loss reduction class for the model. Default to BERTMLMLossWithReduction.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@dataclass\nclass ESM2GenericConfig(BioBertConfig[ESM2ModelT, MegatronLossType]):\n    \"\"\"Configuration class for ESM2 model.\n\n    Attributes:\n        num_layers: Number of layers in the model.\n        hidden_size: Hidden size of the model.\n        num_attention_heads: Number of attention heads in the model.\n        ffn_hidden_size: Hidden size of the feed-forward network.\n        hidden_dropout: Dropout rate for hidden layers.\n        attention_dropout: Dropout rate for attention layers.\n        apply_residual_connection_post_layernorm: Whether to apply residual connection after layer normalization.\n        layernorm_epsilon: Epsilon value for layer normalization.\n        layernorm_zero_centered_gamma: Whether to zero-center the gamma parameter in layer normalization.\n        activation_func: Activation function used in the model.\n        init_method_std: Standard deviation for weight initialization.\n        apply_query_key_layer_scaling: Whether to apply scaling to query and key layers.\n        masked_softmax_fusion: Whether to use a kernel that fuses attention softmax with its mask.\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        share_embeddings_and_output_weights: Whether to share embeddings and output weights.\n        enable_autocast: Whether to enable autocast for mixed precision.\n        biobert_spec_option: BiobertSpecOption for the model.\n        position_embedding_type: Type of position embedding used in the model.\n        seq_length: Length of the input sequence.\n        make_vocab_size_divisible_by: Make the vocabulary size divisible by this value.\n        token_dropout: Whether to apply token dropout.\n        use_attention_mask: Whether to use attention mask.\n        use_esm_attention: Whether to use ESM attention.\n        attention_softmax_in_fp32: Whether to use fp32 for attention softmax.\n        optimizer_fn: Optional optimizer function for the model.\n        parallel_output: Whether to use parallel output.\n        rotary_base: Base value for rotary positional encoding.\n        rotary_percent: Percentage of rotary positional encoding.\n        seq_len_interpolation_factor: Interpolation factor for sequence length.\n        get_attention_mask_from_fusion: Whether to get attention mask from fusion.\n        nemo1_ckpt_path: Path to NEMO1 checkpoint.\n        return_only_hidden_states: Whether to return only hidden states.\n        loss_reduction_class: Loss reduction class for the model. Default to BERTMLMLossWithReduction.\n    \"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[ESM2ModelT] = ESM2Model\n    num_layers: int = 33  # 650M\n    hidden_size: int = 1280  # 650M\n    num_attention_heads: int = 20\n    ffn_hidden_size: int = 4 * 1280  # Transformer FFN hidden size. Usually 4 * hidden_size.\n    hidden_dropout: float = 0  # ESM2 removes dropout from hidden layers and attention\n    attention_dropout: float = 0.0  # ESM2 does not use attention dropout\n    apply_residual_connection_post_layernorm: bool = False  # TODO: farhadr False is new default, True was BERT pub.\n    layernorm_epsilon: float = 1.0e-5\n    bias_activation_fusion: bool = True  # True degrades accuracy slightly, but is faster.\n    activation_func: Callable = F.gelu  # esm_gelu_func  # ESM2 MLP\n    init_method_std: float = 0.02\n\n    # embedding\n    token_dropout: bool = True\n    use_attention_mask: bool = True\n\n    # core attention\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    attention_softmax_in_fp32: bool = False\n    normalize_attention_scores: bool = False\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    fp16_lm_cross_entropy: bool = False  # Move the cross entropy unreduced loss calculation for lm head to fp16\n    parallel_output: bool = True\n    share_embeddings_and_output_weights: bool = True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"rope\"  # ESM2 uses relative positional encoding 'ROPE' to extrapolate to longer sequences unseen during training\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[MegatronBioBertModel], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    nemo1_ckpt_path: str | None = None\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    #  self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # TODO (@jstjohn) come up with a cleaner way in the biobert module to return user requested\n    #  things as part of the workflow for inference and fine-tuning.\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    skip_logits: bool = False\n    return_only_hidden_states: bool = False  # return logits\n\n    def __post_init__(self):\n        # TODO, as a validator?\n        \"\"\"Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.\"\"\"\n        super().__post_init__()\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n            self.core_attention_override = ESM2TEDotProductAttention\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n            self.core_attention_override = ESM2DotProductAttention\n        else:\n            raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2GenericConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __post_init__(self):\n    # TODO, as a validator?\n    \"\"\"Check compatibility between biobert_spec_option and apply_query_key_layer_scaling post initialization.\"\"\"\n    super().__post_init__()\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n        self.core_attention_override = ESM2TEDotProductAttention\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is depreciated. Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n        self.core_attention_override = ESM2DotProductAttention\n    else:\n        raise ValueError(f\"Unknown biobert_spec_option: {self.biobert_spec_option}\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model","title":"<code>ESM2Model</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>ESM2 Transformer language model.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>class ESM2Model(MegatronBioBertModel):\n    \"\"\"ESM2 Transformer language model.\"\"\"\n\n    def __init__(\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: spec_utils.ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[BioNeMoESMTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = True,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        skip_logits: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2 model.\n\n        Args:\n            config (TransformerConfig): transformer config\n            num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n            transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n            vocab_size (int): vocabulary size\n            max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n            tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n            pre_process (bool): Include embedding layer (used with pipeline parallelism)\n            post_process (bool): Include an output layer (used with pipeline parallelism)\n            fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n            parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n            share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n            position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n                Defaults is 'learned_absolute'.\n            rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n                Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n            seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n            add_binary_head (bool): Whether to add a binary head. Defaults to True.\n            return_embeddings (bool): Whether to return embeddings. Defaults to False.\n            include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n            use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n            include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n            skip_logits (bool): Skip writing the token logits in output dict\n        \"\"\"\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n\n        # Embeddings.\n        if self.pre_process:\n            # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n            # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n            # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n            self.embedding = ESM2Embedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n                # ESM2 NEW ARGS\n                token_dropout=self.config.token_dropout,\n                use_attention_mask=self.config.use_attention_mask,\n                mask_token_id=tokenizer.mask_token_id,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                seq_len_interpolation_factor=seq_len_interpolation_factor,\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def embedding_forward(\n        self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n    ):\n        \"\"\"Forward pass of the embedding layer.\n\n        Args:\n            input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n            position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n            tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n            attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n        Returns:\n            Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n        \"\"\"\n        # ESM2 Customization: ESM2Embedding forward takes attention_mask\n        # in addition to the args required by LanguageModelEmbedding\n        return self.embedding(\n            input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n        )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model.__init__","title":"<code>__init__(config, num_tokentypes, transformer_layer_spec, vocab_size, max_sequence_length, tokenizer=None, pre_process=True, post_process=True, fp16_lm_cross_entropy=False, parallel_output=True, share_embeddings_and_output_weights=False, position_embedding_type='learned_absolute', rotary_percent=1.0, seq_len_interpolation_factor=None, add_binary_head=True, return_embeddings=False, include_embeddings=False, use_full_attention_mask=False, include_hiddens=False, skip_logits=False)</code>","text":"<p>Initialize the ESM2 model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>tokenizer</code> <code>AutoTokenizer</code> <p>optional tokenizer object (currently only used in the constructor of ESM2Model)</p> <code>None</code> <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>fp16_lm_cross_entropy</code> <code>bool</code> <p>Whether to move the cross entropy unreduced loss calculation for lm head to fp16.</p> <code>False</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>string</code> <p>Position embedding type. Options ['learned_absolute', 'rope']. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> <code>seq_len_interpolation_factor</code> <code>Optional[float]</code> <p>Interpolation factor for sequence length. Defaults to None.</p> <code>None</code> <code>add_binary_head</code> <code>bool</code> <p>Whether to add a binary head. Defaults to True.</p> <code>True</code> <code>return_embeddings</code> <code>bool</code> <p>Whether to return embeddings. Defaults to False.</p> <code>False</code> <code>include_embeddings</code> <code>bool</code> <p>Whether to include embeddings in the output dictionary. Defaults to False.</p> <code>False</code> <code>use_full_attention_mask</code> <code>bool</code> <p>Whether to use full attention mask. Defaults to False.</p> <code>False</code> <code>include_hiddens</code> <code>bool</code> <p>Whether to include hidden states in the output dictionary. Defaults to False.</p> <code>False</code> <code>skip_logits</code> <code>bool</code> <p>Skip writing the token logits in output dict</p> <code>False</code> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def __init__(\n    self,\n    config: TransformerConfig,\n    num_tokentypes: int,\n    transformer_layer_spec: spec_utils.ModuleSpec,\n    vocab_size: int,\n    max_sequence_length: int,\n    tokenizer: Optional[BioNeMoESMTokenizer] = None,\n    pre_process: bool = True,\n    post_process: bool = True,\n    fp16_lm_cross_entropy: bool = False,\n    parallel_output: bool = True,\n    share_embeddings_and_output_weights: bool = False,\n    position_embedding_type: Literal[\"learned_absolute\", \"rope\"] = \"learned_absolute\",\n    rotary_percent: float = 1.0,\n    seq_len_interpolation_factor: Optional[float] = None,\n    add_binary_head: bool = True,\n    return_embeddings: bool = False,\n    include_embeddings: bool = False,\n    use_full_attention_mask: bool = False,\n    include_hiddens: bool = False,\n    skip_logits: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the ESM2 model.\n\n    Args:\n        config (TransformerConfig): transformer config\n        num_tokentypes (int): Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec (ModuleSpec): Specifies module to use for transformer layers\n        vocab_size (int): vocabulary size\n        max_sequence_length (int): maximum size of sequence. This is used for positional embedding\n        tokenizer (AutoTokenizer): optional tokenizer object (currently only used in the constructor of ESM2Model)\n        pre_process (bool): Include embedding layer (used with pipeline parallelism)\n        post_process (bool): Include an output layer (used with pipeline parallelism)\n        fp16_lm_cross_entropy: Whether to move the cross entropy unreduced loss calculation for lm head to fp16.\n        parallel_output (bool): Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights (bool): When True, input embeddings and output logit weights are shared. Defaults to False.\n        position_embedding_type (string): Position embedding type. Options ['learned_absolute', 'rope'].\n            Defaults is 'learned_absolute'.\n        rotary_percent (float): Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n        seq_len_interpolation_factor (Optional[float]): Interpolation factor for sequence length. Defaults to None.\n        add_binary_head (bool): Whether to add a binary head. Defaults to True.\n        return_embeddings (bool): Whether to return embeddings. Defaults to False.\n        include_embeddings (bool): Whether to include embeddings in the output dictionary. Defaults to False.\n        use_full_attention_mask (bool): Whether to use full attention mask. Defaults to False.\n        include_hiddens (bool): Whether to include hidden states in the output dictionary. Defaults to False.\n        skip_logits (bool): Skip writing the token logits in output dict\n    \"\"\"\n    super(MegatronBioBertModel, self).__init__(config=config)\n    self.post_process = post_process\n    self.add_binary_head = add_binary_head\n    if return_embeddings:\n        assert self.post_process, \"only return embeddings on the last pipeline stage\"\n    # `b` = batch, `s` = sequence.\n    # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n    #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n    self.use_full_attention_mask = use_full_attention_mask\n    self.config: TransformerConfig = config\n    self.transformer_layer_spec: spec_utils.ModuleSpec = transformer_layer_spec\n    self.vocab_size = vocab_size\n    self.max_sequence_length = max_sequence_length\n    self.pre_process = pre_process\n    self.post_process = post_process\n    self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n    self.parallel_output = parallel_output\n    self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n    self.position_embedding_type = position_embedding_type\n    self.add_binary_head = add_binary_head\n    self.return_embeddings = return_embeddings\n    self.include_embeddings = include_embeddings\n    self.include_hiddens = include_hiddens\n    self.skip_logits = skip_logits\n\n    # megatron core pipelining currently depends on model type\n    self.model_type = ModelType.encoder_or_decoder\n\n    # Embeddings.\n    if self.pre_process:\n        # ESM2 Customization: ESM2Embedding instead of LanguageModelEmbedding\n        # TODO: call super, overwrite the self.embedding, and setup_embeddings_and_output_layer in constructor.\n        # Note: need to avoid calling setup twice: skip with super (super(skip_setup=True))\n        self.embedding = ESM2Embedding(\n            config=self.config,\n            vocab_size=self.vocab_size,\n            max_sequence_length=self.max_sequence_length,\n            position_embedding_type=position_embedding_type,\n            num_tokentypes=num_tokentypes,\n            # ESM2 NEW ARGS\n            token_dropout=self.config.token_dropout,\n            use_attention_mask=self.config.use_attention_mask,\n            mask_token_id=tokenizer.mask_token_id,\n        )\n\n    if self.position_embedding_type == \"rope\":\n        self.rotary_pos_emb = RotaryEmbedding(\n            kv_channels=self.config.kv_channels,\n            rotary_percent=rotary_percent,\n            rotary_interleaved=self.config.rotary_interleaved,\n            seq_len_interpolation_factor=seq_len_interpolation_factor,\n        )\n\n    # Transformer.\n    self.encoder = TransformerBlock(\n        config=self.config,\n        spec=self.transformer_layer_spec,\n        pre_process=self.pre_process,\n        post_process=self.post_process,\n    )\n\n    # Output\n    if post_process:\n        # TODO: Make sure you are passing in the mpu_vocab_size properly\n        self.lm_head = BertLMHead(\n            config.hidden_size,\n            config,\n        )\n\n        self.output_layer = tensor_parallel.ColumnParallelLinear(\n            config.hidden_size,\n            self.vocab_size,\n            config=config,\n            init_method=config.init_method,\n            bias=True,\n            skip_bias_add=False,\n            gather_output=not self.parallel_output,\n            skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n        )\n\n        self.binary_head = None\n        if self.add_binary_head:\n            # TODO: Shoudl switch this to TE ?\n            self.binary_head = get_linear_layer(\n                config.hidden_size, 2, config.init_method, config.perform_initialization\n            )\n\n            self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n    if self.pre_process or self.post_process:\n        self.setup_embeddings_and_output_layer()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.ESM2Model.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Forward pass of the embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor of shape (batch_size, sequence_length) containing the input IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the position IDs.</p> required <code>tokentype_ids</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.</p> Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>def embedding_forward(\n    self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: Tensor = None, attention_mask: Tensor = None\n):\n    \"\"\"Forward pass of the embedding layer.\n\n    Args:\n        input_ids: The input tensor of shape (batch_size, sequence_length) containing the input IDs.\n        position_ids: The tensor of shape (batch_size, sequence_length) containing the position IDs.\n        tokentype_ids: The tensor of shape (batch_size, sequence_length) containing the token type IDs. Defaults to None.\n        attention_mask: The tensor of shape (batch_size, sequence_length) containing the attention mask. Defaults to None.\n\n    Returns:\n        Tensor: The output tensor of shape (batch_size, sequence_length, hidden_size) containing the embedded representations.\n    \"\"\"\n    # ESM2 Customization: ESM2Embedding forward takes attention_mask\n    # in addition to the args required by LanguageModelEmbedding\n    return self.embedding(\n        input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids, attention_mask=attention_mask\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/model/#bionemo.esm2.model.model.esm_gelu_func","title":"<code>esm_gelu_func(x)</code>","text":"<p>ESM2-specific gelu implementation from the original ESM repo.</p> <p>Warning</p> <p>Using F.gelu yields subtly wrong results, but only when used in combination with bias_activation_fusion=True This variant will not allow you to use bias_activation_fusion=True, which may be the only accuracy benefit over a native F.gelu.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of any given dimension</p> required Source code in <code>bionemo/esm2/model/model.py</code> <pre><code>@torch.compile\ndef esm_gelu_func(x: Tensor) -&gt; Tensor:\n    \"\"\"ESM2-specific gelu implementation from the original ESM repo.\n\n    !!! warning\n\n        Using F.gelu yields subtly wrong results, but only when used in combination with bias_activation_fusion=True\n        This variant will not allow you to use bias_activation_fusion=True, which may be the only accuracy benefit over\n        a native F.gelu.\n\n    Args:\n        x: input tensor of any given dimension\n    \"\"\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule","title":"<code>ESM2FineTuneDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>A PyTorch Lightning DataModule for fine-tuning ESM2 models.</p> <p>This DataModule is designed to handle the data preparation and loading for fine-tuning ESM2 models. It provides a flexible way to create and manage datasets, data loaders, and sampling strategies.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>class ESM2FineTuneDataModule(MegatronDataModule):\n    \"\"\"A PyTorch Lightning DataModule for fine-tuning ESM2 models.\n\n    This DataModule is designed to handle the data preparation and loading for fine-tuning ESM2 models.\n    It provides a flexible way to create and manage datasets, data loaders, and sampling strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataset: DATASET_TYPES = None,\n        valid_dataset: DATASET_TYPES = None,\n        predict_dataset: DATASET_TYPES = None,\n        seed: int = 42,\n        min_seq_length: int | None = None,\n        max_seq_length: int = 1024,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        num_workers: int = 10,\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n        rampup_batch_size: list[int] | None = None,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    ) -&gt; None:\n        \"\"\"Initialize the ESM2FineTuneDataModule.\n\n        Args:\n            train_dataset: The training dataset.\n            valid_dataset: The validation dataset.\n            predict_dataset: The prediction dataset. Should not be set together with train/valid datasets\n            seed: The random seed to use for shuffling the datasets. Defaults to 42.\n            min_seq_length: The minimum sequence length for the datasets. Defaults to None.\n            max_seq_length: The maximum sequence length for the datasets. Defaults to 1024.\n            micro_batch_size: The micro-batch size for the data loader. Defaults to 4.\n            global_batch_size: The global batch size for the data loader. Defaults to 8.\n            num_workers: The number of worker processes for the data loader. Defaults to 10.\n            persistent_workers: Whether to persist the worker processes. Defaults to True.\n            pin_memory: Whether to pin the data in memory. Defaults to True.\n            rampup_batch_size: The batch size ramp-up schedule. Defaults to None.\n            tokenizer: The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.valid_dataset = valid_dataset\n        self.predict_dataset = predict_dataset\n        if predict_dataset is not None:\n            assert train_dataset is None, \"Datamodule expects either trin/valid dataset or predict dataset\"\n        self._seed = seed\n        self._min_seq_length = min_seq_length\n        self._max_seq_length = max_seq_length\n        self._tokenizer = tokenizer\n\n        self._micro_batch_size = micro_batch_size\n        self._num_workers = num_workers\n        self._persistent_workers = persistent_workers\n        self._pin_memory = pin_memory\n\n        self.data_sampler = MegatronDataSampler(\n            seq_len=max_seq_length,\n            micro_batch_size=micro_batch_size,\n            global_batch_size=global_batch_size,\n            dataloader_type=\"single\",  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n            rampup_batch_size=rampup_batch_size,\n            output_log=predict_dataset is None,  # logging does not work with predict step\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Setup the ESMDataModule.\n\n        Args:\n            stage: Unused.\n\n        Raises:\n            RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n        \"\"\"\n        del stage  # Unused.\n\n        if not hasattr(self, \"trainer\") or self.trainer is None:\n            raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n        if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n            logging.warning(\n                \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n                \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n            )\n\n        # Create training dataset\n        if self.train_dataset is not None:\n            max_train_steps = self.trainer.max_steps\n            if max_train_steps &lt;= 0:\n                raise RuntimeError(\"Please specify trainer.max_steps\")\n\n            num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n            self._train_ds = self._create_epoch_based_dataset(self.train_dataset, num_train_samples)\n\n        # Create validation dataset\n        if self.valid_dataset is not None:\n            num_val_samples = infer_num_samples(\n                limit_batches=self.trainer.limit_val_batches,\n                num_samples_in_dataset=len(self.valid_dataset),\n                global_batch_size=self.data_sampler.global_batch_size,\n                stage=\"val\",\n            )\n            self._valid_ds = self._create_epoch_based_dataset(self.valid_dataset, num_val_samples)\n\n        assert (\n            hasattr(self, \"trainer\") and self.trainer is not None\n        ), \"Setup should be completed when trainer and config are attached.\"\n\n    def _create_epoch_based_dataset(\n        self,\n        dataset: InMemoryPerTokenValueDataset | InMemorySingleValueDataset,\n        total_samples: int,\n    ):\n        return MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(dataset),\n            num_samples=total_samples,\n            shuffle=self.predict_dataset is None,\n            seed=self._seed,\n        )\n\n    def _create_dataloader(self, dataset, **kwargs) -&gt; torch.utils.data.DataLoader:\n        assert self._tokenizer.pad_token_id is not None, \"Tokenizer must have a pad token id.\"\n\n        return torch.utils.data.DataLoader(\n            dataset,\n            num_workers=self._num_workers,\n            pin_memory=self._pin_memory,\n            persistent_workers=self._persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self._tokenizer.pad_token_id,\n                min_length=self._min_seq_length,\n                max_length=self._max_seq_length,\n            ),\n            **kwargs,\n        )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n        \"\"\"Returns the dataloader for training data.\"\"\"\n        assert self._train_ds is not None, \"train_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self._train_ds)\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for validation data.\"\"\"\n        assert self._valid_ds is not None, \"valid_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self._valid_ds)\n\n    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Returns the dataloader for prediction data.\"\"\"\n        assert self.predict_dataset is not None, \"predict_dataset is not provided to ESM2FineTuneDataModule\"\n        return self._create_dataloader(self.predict_dataset)\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n        \"\"\"Raises a not implemented error.\"\"\"\n        raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.__init__","title":"<code>__init__(train_dataset=None, valid_dataset=None, predict_dataset=None, seed=42, min_seq_length=None, max_seq_length=1024, micro_batch_size=4, global_batch_size=8, num_workers=10, persistent_workers=True, pin_memory=True, rampup_batch_size=None, tokenizer=tokenizer.get_tokenizer())</code>","text":"<p>Initialize the ESM2FineTuneDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>DATASET_TYPES</code> <p>The training dataset.</p> <code>None</code> <code>valid_dataset</code> <code>DATASET_TYPES</code> <p>The validation dataset.</p> <code>None</code> <code>predict_dataset</code> <code>DATASET_TYPES</code> <p>The prediction dataset. Should not be set together with train/valid datasets</p> <code>None</code> <code>seed</code> <code>int</code> <p>The random seed to use for shuffling the datasets. Defaults to 42.</p> <code>42</code> <code>min_seq_length</code> <code>int | None</code> <p>The minimum sequence length for the datasets. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for the datasets. Defaults to 1024.</p> <code>1024</code> <code>micro_batch_size</code> <code>int</code> <p>The micro-batch size for the data loader. Defaults to 4.</p> <code>4</code> <code>global_batch_size</code> <code>int</code> <p>The global batch size for the data loader. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>The number of worker processes for the data loader. Defaults to 10.</p> <code>10</code> <code>persistent_workers</code> <code>bool</code> <p>Whether to persist the worker processes. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin the data in memory. Defaults to True.</p> <code>True</code> <code>rampup_batch_size</code> <code>list[int] | None</code> <p>The batch size ramp-up schedule. Defaults to None.</p> <code>None</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.</p> <code>get_tokenizer()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def __init__(\n    self,\n    train_dataset: DATASET_TYPES = None,\n    valid_dataset: DATASET_TYPES = None,\n    predict_dataset: DATASET_TYPES = None,\n    seed: int = 42,\n    min_seq_length: int | None = None,\n    max_seq_length: int = 1024,\n    micro_batch_size: int = 4,\n    global_batch_size: int = 8,\n    num_workers: int = 10,\n    persistent_workers: bool = True,\n    pin_memory: bool = True,\n    rampup_batch_size: list[int] | None = None,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n) -&gt; None:\n    \"\"\"Initialize the ESM2FineTuneDataModule.\n\n    Args:\n        train_dataset: The training dataset.\n        valid_dataset: The validation dataset.\n        predict_dataset: The prediction dataset. Should not be set together with train/valid datasets\n        seed: The random seed to use for shuffling the datasets. Defaults to 42.\n        min_seq_length: The minimum sequence length for the datasets. Defaults to None.\n        max_seq_length: The maximum sequence length for the datasets. Defaults to 1024.\n        micro_batch_size: The micro-batch size for the data loader. Defaults to 4.\n        global_batch_size: The global batch size for the data loader. Defaults to 8.\n        num_workers: The number of worker processes for the data loader. Defaults to 10.\n        persistent_workers: Whether to persist the worker processes. Defaults to True.\n        pin_memory: Whether to pin the data in memory. Defaults to True.\n        rampup_batch_size: The batch size ramp-up schedule. Defaults to None.\n        tokenizer: The tokenizer to use for tokenization. Defaults to the BioNeMoESMTokenizer.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__()\n    self.train_dataset = train_dataset\n    self.valid_dataset = valid_dataset\n    self.predict_dataset = predict_dataset\n    if predict_dataset is not None:\n        assert train_dataset is None, \"Datamodule expects either trin/valid dataset or predict dataset\"\n    self._seed = seed\n    self._min_seq_length = min_seq_length\n    self._max_seq_length = max_seq_length\n    self._tokenizer = tokenizer\n\n    self._micro_batch_size = micro_batch_size\n    self._num_workers = num_workers\n    self._persistent_workers = persistent_workers\n    self._pin_memory = pin_memory\n\n    self.data_sampler = MegatronDataSampler(\n        seq_len=max_seq_length,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        dataloader_type=\"single\",  # `MegatronPretrainingRandomSampler` from \"cyclic\" is failing.\n        rampup_batch_size=rampup_batch_size,\n        output_log=predict_dataset is None,  # logging does not work with predict step\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the dataloader for prediction data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for prediction data.\"\"\"\n    assert self.predict_dataset is not None, \"predict_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self.predict_dataset)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Setup the ESMDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Unused.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the trainer is not attached, or if the trainer's max_steps is not set.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Setup the ESMDataModule.\n\n    Args:\n        stage: Unused.\n\n    Raises:\n        RuntimeError: If the trainer is not attached, or if the trainer's max_steps is not set.\n    \"\"\"\n    del stage  # Unused.\n\n    if not hasattr(self, \"trainer\") or self.trainer is None:\n        raise RuntimeError(\"Setup should be completed when trainer and config are attached.\")\n\n    if self.trainer.max_epochs is not None and self.trainer.max_epochs &gt; 1:\n        logging.warning(\n            \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used \"\n            \"in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n        )\n\n    # Create training dataset\n    if self.train_dataset is not None:\n        max_train_steps = self.trainer.max_steps\n        if max_train_steps &lt;= 0:\n            raise RuntimeError(\"Please specify trainer.max_steps\")\n\n        num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n        self._train_ds = self._create_epoch_based_dataset(self.train_dataset, num_train_samples)\n\n    # Create validation dataset\n    if self.valid_dataset is not None:\n        num_val_samples = infer_num_samples(\n            limit_batches=self.trainer.limit_val_batches,\n            num_samples_in_dataset=len(self.valid_dataset),\n            global_batch_size=self.data_sampler.global_batch_size,\n            stage=\"val\",\n        )\n        self._valid_ds = self._create_epoch_based_dataset(self.valid_dataset, num_val_samples)\n\n    assert (\n        hasattr(self, \"trainer\") and self.trainer is not None\n    ), \"Setup should be completed when trainer and config are attached.\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Raises a not implemented error.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Raises a not implemented error.\"\"\"\n    raise NotImplementedError(\"No test dataset provided for ESM2\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the dataloader for training data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; TRAIN_DATALOADERS:\n    \"\"\"Returns the dataloader for training data.\"\"\"\n    assert self._train_ds is not None, \"train_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self._train_ds)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the dataloader for validation data.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; EVAL_DATALOADERS:\n    \"\"\"Returns the dataloader for validation data.\"\"\"\n    assert self._valid_ds is not None, \"valid_dataset is not provided to ESM2FineTuneDataModule\"\n    return self._create_dataloader(self._valid_ds)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.InMemoryCSVDataset","title":"<code>InMemoryCSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>An in-memory dataset that tokenize strings into BertSample instances.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>class InMemoryCSVDataset(Dataset):\n    \"\"\"An in-memory dataset that tokenize strings into BertSample instances.\"\"\"\n\n    def __init__(\n        self,\n        data_path: str | os.PathLike,\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset for single-value regression fine-tuning.\n\n        This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n        dataset sequences provided.\n\n        Args:\n            data_path (str | os.PathLike): A path to the CSV file containing sequences.\n            labels (Optional[Sequence[float | str]]): An optional sequence of labels with 1:1 mapping to sequences.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n        \"\"\"\n        self.sequences, self.labels = self.load_data(data_path)\n\n        self.seed = seed\n        self._len = len(self.sequences)\n        self.tokenizer = tokenizer\n\n    def __len__(self) -&gt; int:\n        \"\"\"The size of the dataset.\"\"\"\n        return self._len\n\n    def __getitem__(self, index: int) -&gt; BertSample:\n        \"\"\"Obtains the BertSample at the given index.\"\"\"\n        sequence = self.sequences[index]\n        tokenized_sequence = self._tokenize(sequence)\n\n        label = tokenized_sequence if len(self.labels) == 0 else self.labels[index]\n        # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n        loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n        return {\n            \"text\": tokenized_sequence,\n            \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n            \"labels\": label,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        }\n\n    def load_data(self, csv_path: str | os.PathLike) -&gt; Tuple[Sequence, Sequence]:\n        \"\"\"Loads data from a CSV file, returning sequences and optionally labels.\n\n        This method should be implemented by subclasses to process labels for their specific dataset.\n\n        Args:\n            csv_path (str | os.PathLike): The path to the CSV file containing the data.\n            The file is expected to have at least one column named 'sequence'. A 'label' column is optional.\n\n        Returns:\n            Tuple[Sequence, Sequence]: A tuple where the first element is a list of sequences and the second element is\n            a list of labels. If the 'label' column is not present, an empty list is returned for labels.\n        \"\"\"\n        df = pd.read_csv(csv_path)\n        sequences = df[\"sequences\"].tolist()\n\n        if \"label\" in df.columns:\n            labels = df[\"labels\"].tolist()\n        else:\n            labels = []\n        return sequences, labels\n\n    def _tokenize(self, sequence: str) -&gt; Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.InMemoryCSVDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Obtains the BertSample at the given index.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def __getitem__(self, index: int) -&gt; BertSample:\n    \"\"\"Obtains the BertSample at the given index.\"\"\"\n    sequence = self.sequences[index]\n    tokenized_sequence = self._tokenize(sequence)\n\n    label = tokenized_sequence if len(self.labels) == 0 else self.labels[index]\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n    return {\n        \"text\": tokenized_sequence,\n        \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n        \"labels\": label,\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.InMemoryCSVDataset.__init__","title":"<code>__init__(data_path, tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset for single-value regression fine-tuning.</p> <p>This is an in-memory dataset that does not apply masking to the sequence. But keeps track of  in the dataset sequences provided. <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str | PathLike</code> <p>A path to the CSV file containing sequences.</p> required <code>labels</code> <code>Optional[Sequence[float | str]]</code> <p>An optional sequence of labels with 1:1 mapping to sequences.</p> required <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def __init__(\n    self,\n    data_path: str | os.PathLike,\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset for single-value regression fine-tuning.\n\n    This is an in-memory dataset that does not apply masking to the sequence. But keeps track of &lt;mask&gt; in the\n    dataset sequences provided.\n\n    Args:\n        data_path (str | os.PathLike): A path to the CSV file containing sequences.\n        labels (Optional[Sequence[float | str]]): An optional sequence of labels with 1:1 mapping to sequences.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n    \"\"\"\n    self.sequences, self.labels = self.load_data(data_path)\n\n    self.seed = seed\n    self._len = len(self.sequences)\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.InMemoryCSVDataset.__len__","title":"<code>__len__()</code>","text":"<p>The size of the dataset.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The size of the dataset.\"\"\"\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/datamodule/#bionemo.esm2.model.finetune.datamodule.InMemoryCSVDataset.load_data","title":"<code>load_data(csv_path)</code>","text":"<p>Loads data from a CSV file, returning sequences and optionally labels.</p> <p>This method should be implemented by subclasses to process labels for their specific dataset.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>str | PathLike</code> <p>The path to the CSV file containing the data.</p> required <p>Returns:</p> Type Description <code>Sequence</code> <p>Tuple[Sequence, Sequence]: A tuple where the first element is a list of sequences and the second element is</p> <code>Sequence</code> <p>a list of labels. If the 'label' column is not present, an empty list is returned for labels.</p> Source code in <code>bionemo/esm2/model/finetune/datamodule.py</code> <pre><code>def load_data(self, csv_path: str | os.PathLike) -&gt; Tuple[Sequence, Sequence]:\n    \"\"\"Loads data from a CSV file, returning sequences and optionally labels.\n\n    This method should be implemented by subclasses to process labels for their specific dataset.\n\n    Args:\n        csv_path (str | os.PathLike): The path to the CSV file containing the data.\n        The file is expected to have at least one column named 'sequence'. A 'label' column is optional.\n\n    Returns:\n        Tuple[Sequence, Sequence]: A tuple where the first element is a list of sequences and the second element is\n        a list of labels. If the 'label' column is not present, an empty list is returned for labels.\n    \"\"\"\n    df = pd.read_csv(csv_path)\n    sequences = df[\"sequences\"].tolist()\n\n    if \"label\" in df.columns:\n        labels = df[\"labels\"].tolist()\n    else:\n        labels = []\n    return sequences, labels\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/","title":"Finetune regressor","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.ESM2FineTuneSeqConfig","title":"<code>ESM2FineTuneSeqConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>@dataclass\nclass ESM2FineTuneSeqConfig(\n    ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    encoder_frozen: bool = True  # freeze encoder parameters\n    ft_dropout: float = 0.25  # MLP layer dropout\n\n    def get_loss_reduction_class(self) -&gt; Type[RegressorLossReduction]:\n        \"\"\"Returns RegressorLossReduction class.\"\"\"\n        return RegressorLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.ESM2FineTuneSeqConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Returns RegressorLossReduction class.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[RegressorLossReduction]:\n    \"\"\"Returns RegressorLossReduction class.\"\"\"\n    return RegressorLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.ESM2FineTuneSeqModel","title":"<code>ESM2FineTuneSeqModel</code>","text":"<p>               Bases: <code>ESM2Model</code></p> <p>ESM2 model that is suitable for fine-tuning on downstream tasks.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>class ESM2FineTuneSeqModel(ESM2Model):\n    \"\"\"ESM2 model that is suitable for fine-tuning on downstream tasks.\"\"\"\n\n    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n        \"\"\"Constructs an instance of the ESM2 model suitable for fine-tuning.\"\"\"\n        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        self.include_embeddings_finetuning = (\n            include_embeddings  # this include_embeddings is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.regression_head = MegatronMLPHead(config)\n\n    def forward(self, *args, **kwargs) -&gt; BioBertOutput | Tensor:\n        \"\"\"Inference.\"\"\"\n        output = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or \"embeddings\" not in output:\n            raise ValueError(\n                f\"Expected to find 'embeddings' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_embeddings=True in the call to super().__init__\"\n            )\n        # Get the embeddings from the parent output, and pull out the [CLS] token for this task\n        embeddings: Tensor = output[\"embeddings\"]\n        # Predict our 1d regression target\n        regression_output = self.regression_head(embeddings)\n        if not self.include_embeddings_finetuning:\n            del output[\"embeddings\"]\n        output[\"regression_output\"] = regression_output\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.ESM2FineTuneSeqModel.__init__","title":"<code>__init__(config, *args, post_process=True, include_embeddings=False, **kwargs)</code>","text":"<p>Constructs an instance of the ESM2 model suitable for fine-tuning.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n    \"\"\"Constructs an instance of the ESM2 model suitable for fine-tuning.\"\"\"\n    super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n\n    # freeze encoder parameters\n    if config.encoder_frozen:\n        for _, param in self.named_parameters():\n            param.requires_grad = False\n\n    self.include_embeddings_finetuning = (\n        include_embeddings  # this include_embeddings is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.regression_head = MegatronMLPHead(config)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.ESM2FineTuneSeqModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; BioBertOutput | Tensor:\n    \"\"\"Inference.\"\"\"\n    output = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or \"embeddings\" not in output:\n        raise ValueError(\n            f\"Expected to find 'embeddings' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_embeddings=True in the call to super().__init__\"\n        )\n    # Get the embeddings from the parent output, and pull out the [CLS] token for this task\n    embeddings: Tensor = output[\"embeddings\"]\n    # Predict our 1d regression target\n    regression_output = self.regression_head(embeddings)\n    if not self.include_embeddings_finetuning:\n        del output[\"embeddings\"]\n    output[\"regression_output\"] = regression_output\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.InMemorySingleValueDataset","title":"<code>InMemorySingleValueDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>An in-memory dataset that tokenizes strings into BertSample instances.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>class InMemorySingleValueDataset(Dataset):\n    \"\"\"An in-memory dataset that tokenizes strings into BertSample instances.\"\"\"\n\n    def __init__(\n        self,\n        data: Sequence[Tuple[str, float]],\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset for single-value regression fine-tuning.\n\n        This is an in-memory dataset that does not apply masking to the sequence.\n\n        Args:\n            data (Sequence[Tuple[str, float]]): A sequence of tuples containing the sequence and target data.\n            tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n                that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n                generated.\n        \"\"\"\n        self.data = data\n        self.seed = seed\n        self._len = len(self.data)\n        self.tokenizer = tokenizer\n\n    def __len__(self) -&gt; int:\n        \"\"\"The size of the dataset.\"\"\"\n        return self._len\n\n    def __getitem__(self, index: int) -&gt; BertSample:\n        \"\"\"Obtains the BertSample at the given index.\"\"\"\n        sequence, target = self.data[index]\n        tokenized_sequence = self._tokenize(sequence)\n        # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n        loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n        return {\n            \"text\": tokenized_sequence,\n            \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n            \"labels\": torch.tensor([target], dtype=torch.float),\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        }\n\n    def _tokenize(self, sequence: str) -&gt; Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.InMemorySingleValueDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Obtains the BertSample at the given index.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def __getitem__(self, index: int) -&gt; BertSample:\n    \"\"\"Obtains the BertSample at the given index.\"\"\"\n    sequence, target = self.data[index]\n    tokenized_sequence = self._tokenize(sequence)\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    loss_mask = ~torch.isin(tokenized_sequence, Tensor(self.tokenizer.all_special_ids))\n\n    return {\n        \"text\": tokenized_sequence,\n        \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n        \"labels\": torch.tensor([target], dtype=torch.float),\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.InMemorySingleValueDataset.__init__","title":"<code>__init__(data, tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset for single-value regression fine-tuning.</p> <p>This is an in-memory dataset that does not apply masking to the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Tuple[str, float]]</code> <p>A sequence of tuples containing the sequence and target data.</p> required <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def __init__(\n    self,\n    data: Sequence[Tuple[str, float]],\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset for single-value regression fine-tuning.\n\n    This is an in-memory dataset that does not apply masking to the sequence.\n\n    Args:\n        data (Sequence[Tuple[str, float]]): A sequence of tuples containing the sequence and target data.\n        tokenizer (tokenizer.BioNeMoESMTokenizer, optional): The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure\n            that __getitem__ is deterministic, but can be random across different runs. If None, a random seed is\n            generated.\n    \"\"\"\n    self.data = data\n    self.seed = seed\n    self._len = len(self.data)\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.InMemorySingleValueDataset.__len__","title":"<code>__len__()</code>","text":"<p>The size of the dataset.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The size of the dataset.\"\"\"\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.MegatronMLPHead","title":"<code>MegatronMLPHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>An MLP class for sequence-level regression.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>class MegatronMLPHead(MegatronModule):\n    \"\"\"An MLP class for sequence-level regression.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n\n        layer_sizes = [config.hidden_size, 256, 1]\n        self.linear_layers = torch.nn.ModuleList(\n            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]  # noqa: RUF007\n        )\n        self.act = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n\n    def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n        \"\"\"Inference.\"\"\"\n        # [b, s, h]\n        for layer in self.linear_layers[:-1]:\n            hidden_states = self.dropout(self.act(layer(hidden_states)))\n\n        output = self.linear_layers[-1](hidden_states)\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.MegatronMLPHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n\n    layer_sizes = [config.hidden_size, 256, 1]\n    self.linear_layers = torch.nn.ModuleList(\n        [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]  # noqa: RUF007\n    )\n    self.act = torch.nn.ReLU()\n    self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.MegatronMLPHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n    \"\"\"Inference.\"\"\"\n    # [b, s, h]\n    for layer in self.linear_layers[:-1]:\n        hidden_states = self.dropout(self.act(layer(hidden_states)))\n\n    output = self.linear_layers[-1](hidden_states)\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.RegressorLossReduction","title":"<code>RegressorLossReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>A class for calculating the MSE loss of regression output.</p> <p>This class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>class RegressorLossReduction(BERTMLMLossWithReduction):\n    \"\"\"A class for calculating the MSE loss of regression output.\n\n    This class used for calculating the loss, and for logging the reduced loss across micro batches.\n    \"\"\"\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside classification head.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        regression_output = forward_out[\"regression_output\"]\n        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            loss = torch.nn.functional.mse_loss(regression_output, targets)\n        else:  # TODO: support CP with masked_token_loss_context_parallel\n            raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.RegressorLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside classification head.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside classification head.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    regression_output = forward_out[\"regression_output\"]\n    targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        loss = torch.nn.functional.mse_loss(regression_output, targets)\n    else:  # TODO: support CP with masked_token_loss_context_parallel\n        raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_regressor/#bionemo.esm2.model.finetune.finetune_regressor.RegressorLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/esm2/model/finetune/finetune_regressor.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/","title":"Finetune token classifier","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ClassifierInput","title":"<code>ClassifierInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Used as input in the ClassifierLossReduction's forward method.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class ClassifierInput(TypedDict):\n    \"\"\"Used as input in the ClassifierLossReduction's forward method.\"\"\"\n\n    labels: Tensor\n    loss_mask: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ClassifierLossReduction","title":"<code>ClassifierLossReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>A class for calculating the cross entropy loss of classification output.</p> <p>This class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class ClassifierLossReduction(BERTMLMLossWithReduction):\n    \"\"\"A class for calculating the cross entropy loss of classification output.\n\n    This class used for calculating the loss, and for logging the reduced loss across micro batches.\n    \"\"\"\n\n    def forward(\n        self, batch: ClassifierInput, forward_out: Esm2FineTuneTokenOutput\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside classification head.\n\n        Returns:\n            A tuple where the loss tensor will be used for backpropagation and the dict will be passed to\n            the reduce method, which currently only works for logging.\n        \"\"\"\n        targets = batch[\"labels\"]  # [b, s]\n        # [b, s, num_class] -&gt; [b, num_class, s] to satisfy input dims for cross_entropy loss\n        classification_output = forward_out[\"classification_output\"].permute(0, 2, 1)\n        loss_mask = batch[\"loss_mask\"]  # [b, s]\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            losses = torch.nn.functional.cross_entropy(classification_output, targets, reduction=\"none\")\n            # losses may contain NaNs at masked locations. We use masked_select to filter out these NaNs\n            masked_loss = torch.masked_select(losses, loss_mask)\n            loss = masked_loss.sum() / loss_mask.sum()\n        else:  # TODO: support CP with masked_token_loss_context_parallel\n            raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ClassifierInput</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Esm2FineTuneTokenOutput</code> <p>the output of the forward method inside classification head.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tuple where the loss tensor will be used for backpropagation and the dict will be passed to</p> <code>PerTokenLossDict | SameSizeLossDict</code> <p>the reduce method, which currently only works for logging.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def forward(\n    self, batch: ClassifierInput, forward_out: Esm2FineTuneTokenOutput\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside classification head.\n\n    Returns:\n        A tuple where the loss tensor will be used for backpropagation and the dict will be passed to\n        the reduce method, which currently only works for logging.\n    \"\"\"\n    targets = batch[\"labels\"]  # [b, s]\n    # [b, s, num_class] -&gt; [b, num_class, s] to satisfy input dims for cross_entropy loss\n    classification_output = forward_out[\"classification_output\"].permute(0, 2, 1)\n    loss_mask = batch[\"loss_mask\"]  # [b, s]\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        losses = torch.nn.functional.cross_entropy(classification_output, targets, reduction=\"none\")\n        # losses may contain NaNs at masked locations. We use masked_select to filter out these NaNs\n        masked_loss = torch.masked_select(losses, loss_mask)\n        loss = masked_loss.sum() / loss_mask.sum()\n    else:  # TODO: support CP with masked_token_loss_context_parallel\n        raise NotImplementedError(\"Context Parallel support is not implemented for this loss\")\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ESM2FineTuneTokenConfig","title":"<code>ESM2FineTuneTokenConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ESM2GenericConfig[ESM2FineTuneTokenModel, ClassifierLossReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>@dataclass\nclass ESM2FineTuneTokenConfig(\n    ESM2GenericConfig[ESM2FineTuneTokenModel, ClassifierLossReduction], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ESM2FineTuneTokenModel] = ESM2FineTuneTokenModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"classification_head\"])\n\n    encoder_frozen: bool = True  # freeze encoder parameters\n    cnn_num_classes: int = 3  # number of classes in each label\n    cnn_dropout: float = 0.25\n    cnn_hidden_dim: int = 32  # The number of output channels in the bottleneck layer of the convolution.\n\n    def get_loss_reduction_class(self) -&gt; Type[ClassifierLossReduction]:\n        \"\"\"The loss function type.\"\"\"\n        return ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ESM2FineTuneTokenConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>The loss function type.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[ClassifierLossReduction]:\n    \"\"\"The loss function type.\"\"\"\n    return ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ESM2FineTuneTokenModel","title":"<code>ESM2FineTuneTokenModel</code>","text":"<p>               Bases: <code>ESM2Model</code></p> <p>An ESM2 model that is suitable for fine tuning.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class ESM2FineTuneTokenModel(ESM2Model):\n    \"\"\"An ESM2 model that is suitable for fine tuning.\"\"\"\n\n    def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        self.include_hiddens_finetuning = (\n            include_hiddens  # this include_hiddens is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.classification_head = MegatronConvNetHead(config)\n\n    def forward(self, *args, **kwargs) -&gt; Tensor | BioBertOutput | Esm2FineTuneTokenOutput:\n        \"\"\"Inference.\"\"\"\n        output: Tensor | BioBertOutput | Esm2FineTuneTokenOutput = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or \"hidden_states\" not in output:\n            raise ValueError(\n                f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_hiddens=True in the call to super().__init__\"\n            )\n        # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n        hidden_states: Tensor = output[\"hidden_states\"]\n        # Predict our 1d regression target\n        classification_output = self.classification_head(hidden_states)\n        if not self.include_hiddens_finetuning:\n            del output[\"hidden_states\"]\n        output[\"classification_output\"] = classification_output\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ESM2FineTuneTokenModel.__init__","title":"<code>__init__(config, *args, include_hiddens=False, post_process=True, **kwargs)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n\n    # freeze encoder parameters\n    if config.encoder_frozen:\n        for _, param in self.named_parameters():\n            param.requires_grad = False\n\n    self.include_hiddens_finetuning = (\n        include_hiddens  # this include_hiddens is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.classification_head = MegatronConvNetHead(config)\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.ESM2FineTuneTokenModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Tensor | BioBertOutput | Esm2FineTuneTokenOutput:\n    \"\"\"Inference.\"\"\"\n    output: Tensor | BioBertOutput | Esm2FineTuneTokenOutput = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or \"hidden_states\" not in output:\n        raise ValueError(\n            f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_hiddens=True in the call to super().__init__\"\n        )\n    # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n    hidden_states: Tensor = output[\"hidden_states\"]\n    # Predict our 1d regression target\n    classification_output = self.classification_head(hidden_states)\n    if not self.include_hiddens_finetuning:\n        del output[\"hidden_states\"]\n    output[\"classification_output\"] = classification_output\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.Esm2FineTuneTokenOutput","title":"<code>Esm2FineTuneTokenOutput</code>","text":"<p>               Bases: <code>BioBertOutput</code></p> <p>Inference output from ESM2FineTuneTokenModel.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class Esm2FineTuneTokenOutput(BioBertOutput):\n    \"\"\"Inference output from ESM2FineTuneTokenModel.\"\"\"\n\n    classification_output: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.InMemoryPerTokenValueDataset","title":"<code>InMemoryPerTokenValueDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>An in-memory dataset of labeled strings, which are tokenized on demand.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class InMemoryPerTokenValueDataset(Dataset):\n    \"\"\"An in-memory dataset of labeled strings, which are tokenized on demand.\"\"\"\n\n    def __init__(\n        self,\n        data: Sequence[Tuple[str, str]],\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        \"\"\"Initializes a dataset for per-token classification fine-tuning.\n\n        This is an in-memory dataset that does not apply masking to the sequence.\n\n        Args:\n            data: A sequence of tuples containing the sequence and target data.\n            tokenizer: The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n            seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to\n                ensure that __getitem__ is deterministic, but can be random across different runs. If None, a random\n                seed is generated.\n        \"\"\"\n        self.data = data\n        self.seed = seed\n        self._len = len(self.data)\n        self.tokenizer = tokenizer\n        label_tokenizer = Label2IDTokenizer()\n        self.label_tokenizer = label_tokenizer.build_vocab(\"CHE\")\n\n    def __len__(self) -&gt; int:\n        \"\"\"Length of dataset.\"\"\"\n        return self._len\n\n    def __getitem__(self, index: int) -&gt; BertSample:\n        \"\"\"Gets a BertSample associated to the supplied index.\"\"\"\n        sequence, target = self.data[index]\n        tokenized_sequence = self._tokenize(sequence)\n        # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n        loss_mask = ~torch.isin(tokenized_sequence, torch.tensor(self.tokenizer.all_special_ids))\n        labels = self._tokenize_labels(target)\n\n        return {\n            \"text\": tokenized_sequence,\n            \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n            \"labels\": labels,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        }\n\n    def _tokenize_labels(self, labels_sequence: str) -&gt; Tensor:\n        label_ids = torch.tensor(self.label_tokenizer.text_to_ids(labels_sequence))\n\n        # # for multi-label classification with BCEWithLogitsLoss\n        # tokenized_labels = torch.nn.functional.one_hot(label_ids, num_classes=self.label_tokenizer.vocab_size)\n        # cls_eos = torch.full((1, self.label_tokenizer.vocab_size), -1, dtype=tokenized_labels.dtype)\n\n        # for multi-class (mutually exclusive) classification with CrossEntropyLoss\n        tokenized_labels = label_ids\n        cls_eos = torch.tensor([-1], dtype=tokenized_labels.dtype)\n\n        # add cls / eos labels with padding value -1 to have the same shape as tokenized_sequence\n        labels = torch.cat((cls_eos, tokenized_labels, cls_eos))\n        return labels\n\n    def _tokenize(self, sequence: str) -&gt; Tensor:\n        \"\"\"Tokenize a protein sequence.\n\n        Args:\n            sequence: The protein sequence.\n\n        Returns:\n            The tokenized sequence.\n        \"\"\"\n        tensor = self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n        return tensor.flatten()  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.InMemoryPerTokenValueDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Gets a BertSample associated to the supplied index.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def __getitem__(self, index: int) -&gt; BertSample:\n    \"\"\"Gets a BertSample associated to the supplied index.\"\"\"\n    sequence, target = self.data[index]\n    tokenized_sequence = self._tokenize(sequence)\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    loss_mask = ~torch.isin(tokenized_sequence, torch.tensor(self.tokenizer.all_special_ids))\n    labels = self._tokenize_labels(target)\n\n    return {\n        \"text\": tokenized_sequence,\n        \"types\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n        \"attention_mask\": torch.ones_like(tokenized_sequence, dtype=torch.int64),\n        \"labels\": labels,\n        \"loss_mask\": loss_mask,\n        \"is_random\": torch.zeros_like(tokenized_sequence, dtype=torch.int64),\n    }\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.InMemoryPerTokenValueDataset.__init__","title":"<code>__init__(data, tokenizer=tokenizer.get_tokenizer(), seed=np.random.SeedSequence().entropy)</code>","text":"<p>Initializes a dataset for per-token classification fine-tuning.</p> <p>This is an in-memory dataset that does not apply masking to the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Tuple[str, str]]</code> <p>A sequence of tuples containing the sequence and target data.</p> required <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to tokenizer.get_tokenizer().</p> <code>get_tokenizer()</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to ensure that getitem is deterministic, but can be random across different runs. If None, a random seed is generated.</p> <code>entropy</code> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def __init__(\n    self,\n    data: Sequence[Tuple[str, str]],\n    tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n    seed: int = np.random.SeedSequence().entropy,  # type: ignore\n):\n    \"\"\"Initializes a dataset for per-token classification fine-tuning.\n\n    This is an in-memory dataset that does not apply masking to the sequence.\n\n    Args:\n        data: A sequence of tuples containing the sequence and target data.\n        tokenizer: The tokenizer to use. Defaults to tokenizer.get_tokenizer().\n        seed: Random seed for reproducibility. This seed is mixed with the index of the sample to retrieve to\n            ensure that __getitem__ is deterministic, but can be random across different runs. If None, a random\n            seed is generated.\n    \"\"\"\n    self.data = data\n    self.seed = seed\n    self._len = len(self.data)\n    self.tokenizer = tokenizer\n    label_tokenizer = Label2IDTokenizer()\n    self.label_tokenizer = label_tokenizer.build_vocab(\"CHE\")\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.InMemoryPerTokenValueDataset.__len__","title":"<code>__len__()</code>","text":"<p>Length of dataset.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Length of dataset.\"\"\"\n    return self._len\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.MegatronConvNetHead","title":"<code>MegatronConvNetHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>A convolutional neural network class for residue-level classification.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>class MegatronConvNetHead(MegatronModule):\n    \"\"\"A convolutional neural network class for residue-level classification.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n\n        self.finetune_model = torch.nn.Sequential(\n            torch.nn.Conv2d(config.hidden_size, config.cnn_hidden_dim, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n            torch.nn.ReLU(),\n            torch.nn.Dropout(config.cnn_dropout),\n        )\n        # class_heads (torch.nn.ModuleList): A list of convolutional layers, each corresponding to a different class head.\n        # These are used for producing logits scores of varying sizes as specified in `output_sizes`.\n        self.class_heads = torch.nn.Conv2d(32, config.cnn_num_classes, kernel_size=(7, 1), padding=(3, 0))\n\n    def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n        \"\"\"Inference.\"\"\"\n        # [b, s, h] -&gt; [b, h, s, 1]\n        hidden_states = hidden_states.permute(0, 2, 1).unsqueeze(dim=-1)\n        hidden_states = self.finetune_model(hidden_states)  # [b, 32, s, 1]\n        output = self.class_heads(hidden_states).squeeze(dim=-1).permute(0, 2, 1)  # [b, s, output_size]\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.MegatronConvNetHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n\n    self.finetune_model = torch.nn.Sequential(\n        torch.nn.Conv2d(config.hidden_size, config.cnn_hidden_dim, kernel_size=(7, 1), padding=(3, 0)),  # 7x32\n        torch.nn.ReLU(),\n        torch.nn.Dropout(config.cnn_dropout),\n    )\n    # class_heads (torch.nn.ModuleList): A list of convolutional layers, each corresponding to a different class head.\n    # These are used for producing logits scores of varying sizes as specified in `output_sizes`.\n    self.class_heads = torch.nn.Conv2d(32, config.cnn_num_classes, kernel_size=(7, 1), padding=(3, 0))\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/finetune_token_classifier/#bionemo.esm2.model.finetune.finetune_token_classifier.MegatronConvNetHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/esm2/model/finetune/finetune_token_classifier.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; List[Tensor]:\n    \"\"\"Inference.\"\"\"\n    # [b, s, h] -&gt; [b, h, s, 1]\n    hidden_states = hidden_states.permute(0, 2, 1).unsqueeze(dim=-1)\n    hidden_states = self.finetune_model(hidden_states)  # [b, 32, s, 1]\n    output = self.class_heads(hidden_states).squeeze(dim=-1).permute(0, 2, 1)  # [b, s, output_size]\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/infer/","title":"Infer","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/infer/#bionemo.esm2.model.finetune.infer.infer_model","title":"<code>infer_model(config, data_module, tokenizer=get_tokenizer())</code>","text":"<p>Infers a BioNeMo ESM2 model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ESM2GenericConfig</code> <p>The configuration for the ESM2 model.</p> required <code>data_module</code> <code>LightningDataModule</code> <p>The data module for training and validation.</p> required <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to <code>get_tokenizer()</code>.</p> <code>get_tokenizer()</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>A list of tensors containing the predictions of predict_dataset in datamodule</p> Source code in <code>bionemo/esm2/model/finetune/infer.py</code> <pre><code>def infer_model(\n    config: ESM2GenericConfig,\n    data_module: pl.LightningDataModule,\n    tokenizer: BioNeMoESMTokenizer = get_tokenizer(),\n) -&gt; list[Tensor]:\n    \"\"\"Infers a BioNeMo ESM2 model using PyTorch Lightning.\n\n    Parameters:\n        config: The configuration for the ESM2 model.\n        data_module: The data module for training and validation.\n        tokenizer: The tokenizer to use. Defaults to `get_tokenizer()`.\n\n    Returns:\n        A list of tensors containing the predictions of predict_dataset in datamodule\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1, pipeline_model_parallel_size=1, ddp=\"megatron\", find_unused_parameters=True\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        num_nodes=1,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n    module = biobert_lightning_module(config=config, tokenizer=tokenizer)\n    results = batch_collator(trainer.predict(module, datamodule=data_module))\n\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/","title":"Peft","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA","title":"<code>ESM2LoRA</code>","text":"<p>               Bases: <code>LoRA</code></p> <p>LoRA for the BioNeMo2 ESM Model.</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>class ESM2LoRA(LoRA):\n    \"\"\"LoRA for the BioNeMo2 ESM Model.\"\"\"\n\n    def __call__(self, model: nn.Module) -&gt; nn.Module:\n        \"\"\"This method is called when the object is called as a function.\n\n        Args:\n            model: The input model.\n\n        Returns:\n            The modified model.\n        \"\"\"\n        fn.walk(model, self.selective_freeze)\n        fn.walk(model, self.transform)\n        return model\n\n    def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n        \"\"\"Freezes specific modules in the given model.\n\n        Args:\n            m (nn.Module): The model to selectively freeze.\n            name (str): The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".\n            prefix (str): The prefix of the module to freeze.\n\n        Returns:\n            nn.Module: The modified model with the specified modules frozen.\n\n        See Also:\n            nemo.collections.llm.fn.mixin.FNMixin\n        \"\"\"\n        if name in [\"encoder\", \"embedding\"]:\n            FNMixin.freeze(m)\n        return m\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA.__call__","title":"<code>__call__(model)</code>","text":"<p>This method is called when the object is called as a function.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The input model.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The modified model.</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; nn.Module:\n    \"\"\"This method is called when the object is called as a function.\n\n    Args:\n        model: The input model.\n\n    Returns:\n        The modified model.\n    \"\"\"\n    fn.walk(model, self.selective_freeze)\n    fn.walk(model, self.transform)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/peft/#bionemo.esm2.model.finetune.peft.ESM2LoRA.selective_freeze","title":"<code>selective_freeze(m, name=None, prefix=None)</code>","text":"<p>Freezes specific modules in the given model.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Module</code> <p>The model to selectively freeze.</p> required <code>name</code> <code>str</code> <p>The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The prefix of the module to freeze.</p> <code>None</code> <p>Returns:</p> Type Description <p>nn.Module: The modified model with the specified modules frozen.</p> See Also <p>nemo.collections.llm.fn.mixin.FNMixin</p> Source code in <code>bionemo/esm2/model/finetune/peft.py</code> <pre><code>def selective_freeze(self, m: nn.Module, name=None, prefix=None):\n    \"\"\"Freezes specific modules in the given model.\n\n    Args:\n        m (nn.Module): The model to selectively freeze.\n        name (str): The name of the module to freeze. Valid values are \"encoder\" and \"embedding\".\n        prefix (str): The prefix of the module to freeze.\n\n    Returns:\n        nn.Module: The modified model with the specified modules frozen.\n\n    See Also:\n        nemo.collections.llm.fn.mixin.FNMixin\n    \"\"\"\n    if name in [\"encoder\", \"embedding\"]:\n        FNMixin.freeze(m)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/esm2/model/finetune/train/","title":"Train","text":""},{"location":"API_reference/bionemo/esm2/model/finetune/train/#bionemo.esm2.model.finetune.train.train_model","title":"<code>train_model(experiment_name, experiment_dir, config, data_module, n_steps_train, metric_tracker=None, tokenizer=get_tokenizer(), peft=None)</code>","text":"<p>Trains a BioNeMo ESM2 model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>The name of the experiment.</p> required <code>experiment_dir</code> <code>Path</code> <p>The directory where the experiment will be saved.</p> required <code>config</code> <code>ESM2GenericConfig</code> <p>The configuration for the ESM2 model.</p> required <code>data_module</code> <code>LightningDataModule</code> <p>The data module for training and validation.</p> required <code>n_steps_train</code> <code>int</code> <p>The number of training steps.</p> required <code>metric_tracker</code> <code>Callback | None</code> <p>Optional callback to track metrics</p> <code>None</code> <code>tokenizer</code> <code>BioNeMoESMTokenizer</code> <p>The tokenizer to use. Defaults to <code>get_tokenizer()</code>.</p> <code>get_tokenizer()</code> <code>peft</code> <code>PEFT | None</code> <p>The PEFT (Parameter-Efficient Fine-Tuning) module. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>A tuple containing the path to the saved checkpoint, a MetricTracker</p> <code>Callback | None</code> <p>object, and the PyTorch Lightning Trainer object.</p> Source code in <code>bionemo/esm2/model/finetune/train.py</code> <pre><code>def train_model(\n    experiment_name: str,\n    experiment_dir: Path,\n    config: ESM2GenericConfig,\n    data_module: pl.LightningDataModule,\n    n_steps_train: int,\n    metric_tracker: Callback | None = None,\n    tokenizer: BioNeMoESMTokenizer = get_tokenizer(),\n    peft: PEFT | None = None,\n) -&gt; Tuple[Path, Callback | None, nl.Trainer]:\n    \"\"\"Trains a BioNeMo ESM2 model using PyTorch Lightning.\n\n    Parameters:\n        experiment_name: The name of the experiment.\n        experiment_dir: The directory where the experiment will be saved.\n        config: The configuration for the ESM2 model.\n        data_module: The data module for training and validation.\n        n_steps_train: The number of training steps.\n        metric_tracker: Optional callback to track metrics\n        tokenizer: The tokenizer to use. Defaults to `get_tokenizer()`.\n        peft: The PEFT (Parameter-Efficient Fine-Tuning) module. Defaults to None.\n\n    Returns:\n        A tuple containing the path to the saved checkpoint, a MetricTracker\n        object, and the PyTorch Lightning Trainer object.\n    \"\"\"\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=True,\n        save_on_train_epoch_end=True,\n        monitor=\"reduced_train_loss\",  # TODO find out how to get val_loss logged and use \"val_loss\",\n        every_n_train_steps=n_steps_train // 2,\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n    )\n\n    # Setup the logger and train the model\n    nemo_logger = NeMoLogger(\n        log_dir=str(experiment_dir),\n        name=experiment_name,\n        tensorboard=TensorBoardLogger(save_dir=experiment_dir, name=experiment_name),\n        ckpt=checkpoint_callback,\n    )\n    # Needed so that the trainer can find an output directory for the profiler\n    # ckpt_path needs to be a string for SerDe\n    optimizer = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=5e-4,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            fp16=config.fp16,\n            bf16=config.bf16,\n        )\n    )\n    module = biobert_lightning_module(config=config, tokenizer=tokenizer, optimizer=optimizer, model_transform=peft)\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        enable_nemo_ckpt_io=True,\n    )\n\n    callbacks: list[Callback] = [RichModelSummary(max_depth=4)]\n    if metric_tracker is not None:\n        callbacks.append(metric_tracker)\n    if peft is not None:\n        callbacks.append(\n            ModelTransform()\n        )  # Callback needed for PEFT fine-tuning using NeMo2, i.e. biobert_lightning_module(model_transform=peft).\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=2,\n        val_check_interval=n_steps_train // 2,\n        max_steps=n_steps_train,\n        num_nodes=1,\n        log_every_n_steps=n_steps_train // 2,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n    nllm.train(\n        model=module,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    ckpt_path = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n    return ckpt_path, metric_tracker, trainer\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ESM2DataConfig","title":"<code>ESM2DataConfig</code>","text":"<p>               Bases: <code>DataConfig[ESMDataModule]</code></p> <p>ESM2DataConfig is a configuration class for setting up the pre-training data module for ESM2.</p> <p>The ESM2DataModule implements the cluster oriented sampling method defined in the ESM2 publication.</p> <p>Attributes:</p> Name Type Description <code>train_cluster_path</code> <code>Path</code> <p>Path to the training cluster data.</p> <code>train_database_path</code> <code>Path</code> <p>Path to the training database.</p> <code>valid_cluster_path</code> <code>Path</code> <p>Path to the validation cluster data.</p> <code>valid_database_path</code> <code>Path</code> <p>Path to the validation database.</p> <code>micro_batch_size</code> <code>int</code> <p>Size of the micro-batch. Default is 8.</p> <code>result_dir</code> <code>str</code> <p>Directory to store results. Default is \"./results\".</p> <code>min_seq_length</code> <code>int</code> <p>Minimum sequence length. Default is 128.</p> <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Default is 128.</p> <code>random_mask_strategy</code> <code>RandomMaskStrategy</code> <p>Strategy for random masking. Default is RandomMaskStrategy.ALL_TOKENS.</p> <code>num_dataset_workers</code> <code>int</code> <p>Number of workers for the dataset. Default is 0.</p> <p>Methods:</p> Name Description <code>construct_data_module</code> <p>int) -&gt; ESMDataModule: Constructs and returns an ESMDataModule instance with the provided global batch size.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>class ESM2DataConfig(DataConfig[ESMDataModule]):\n    \"\"\"ESM2DataConfig is a configuration class for setting up the pre-training data module for ESM2.\n\n    The ESM2DataModule implements the cluster oriented sampling method defined in the ESM2 publication.\n\n    Attributes:\n        train_cluster_path (Path): Path to the training cluster data.\n        train_database_path (Path): Path to the training database.\n        valid_cluster_path (Path): Path to the validation cluster data.\n        valid_database_path (Path): Path to the validation database.\n        micro_batch_size (int): Size of the micro-batch. Default is 8.\n        result_dir (str): Directory to store results. Default is \"./results\".\n        min_seq_length (int): Minimum sequence length. Default is 128.\n        max_seq_length (int): Maximum sequence length. Default is 128.\n        random_mask_strategy (RandomMaskStrategy): Strategy for random masking. Default is RandomMaskStrategy.ALL_TOKENS.\n        num_dataset_workers (int): Number of workers for the dataset. Default is 0.\n\n    Methods:\n        construct_data_module(global_batch_size: int) -&gt; ESMDataModule:\n            Constructs and returns an ESMDataModule instance with the provided global batch size.\n    \"\"\"\n\n    train_cluster_path: Path\n    train_database_path: Path\n    valid_cluster_path: Path\n    valid_database_path: Path\n\n    micro_batch_size: int = 8\n    result_dir: str = \"./results\"\n    min_seq_length: int = 128\n    max_seq_length: int = 128\n    random_mask_strategy: RandomMaskStrategy = RandomMaskStrategy.ALL_TOKENS\n    num_dataset_workers: int = 0\n\n    def construct_data_module(self, global_batch_size: int) -&gt; ESMDataModule:\n        \"\"\"Constructs and returns an ESMDataModule instance with the provided global batch size.\n\n        This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be\n        aquired here. For example, tokenizers, preprocessing, may want to live in this method.\n\n        Args:\n            global_batch_size (int): Global batch size for the data module. Global batch size must be a function of\n                parallelism settings and the `micro_batch_size` attribute. Since the DataConfig has no ownership over\n                parallelism configuration, we expect someone higher up on the ownership chain to provide the value to\n                this method.\n\n        \"\"\"\n        tokenizer = get_tokenizer()\n        data = ESMDataModule(\n            train_cluster_path=self.train_cluster_path,\n            train_database_path=self.train_database_path,\n            valid_cluster_path=self.valid_cluster_path,\n            valid_database_path=self.valid_database_path,\n            global_batch_size=global_batch_size,\n            micro_batch_size=self.micro_batch_size,\n            min_seq_length=self.min_seq_length,\n            max_seq_length=self.max_seq_length,\n            num_workers=self.num_dataset_workers,\n            random_mask_strategy=self.random_mask_strategy,\n            tokenizer=tokenizer,\n        )\n        return data\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ESM2DataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>","text":"<p>Constructs and returns an ESMDataModule instance with the provided global batch size.</p> <p>This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be aquired here. For example, tokenizers, preprocessing, may want to live in this method.</p> <p>Parameters:</p> Name Type Description Default <code>global_batch_size</code> <code>int</code> <p>Global batch size for the data module. Global batch size must be a function of parallelism settings and the <code>micro_batch_size</code> attribute. Since the DataConfig has no ownership over parallelism configuration, we expect someone higher up on the ownership chain to provide the value to this method.</p> required Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def construct_data_module(self, global_batch_size: int) -&gt; ESMDataModule:\n    \"\"\"Constructs and returns an ESMDataModule instance with the provided global batch size.\n\n    This method provides means for constructing the datamodule, any pre-requisites for the DataModule should be\n    aquired here. For example, tokenizers, preprocessing, may want to live in this method.\n\n    Args:\n        global_batch_size (int): Global batch size for the data module. Global batch size must be a function of\n            parallelism settings and the `micro_batch_size` attribute. Since the DataConfig has no ownership over\n            parallelism configuration, we expect someone higher up on the ownership chain to provide the value to\n            this method.\n\n    \"\"\"\n    tokenizer = get_tokenizer()\n    data = ESMDataModule(\n        train_cluster_path=self.train_cluster_path,\n        train_database_path=self.train_database_path,\n        valid_cluster_path=self.valid_cluster_path,\n        valid_database_path=self.valid_database_path,\n        global_batch_size=global_batch_size,\n        micro_batch_size=self.micro_batch_size,\n        min_seq_length=self.min_seq_length,\n        max_seq_length=self.max_seq_length,\n        num_workers=self.num_dataset_workers,\n        random_mask_strategy=self.random_mask_strategy,\n        tokenizer=tokenizer,\n    )\n    return data\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig","title":"<code>ExposedESM2PretrainConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[ESM2Config]</code></p> <p>Configuration class for ESM2 pretraining with select exposed parameters.</p> <p>See the inherited ExposedModelConfig for attributes and methods from the base class. Use this class either as a template or extension for custom configurations. Importantly, these kinds of classes should do two things, select attributes to expose to the user, and provide validation and serialization any attributes.</p> <p>Attributes:</p> Name Type Description <code>use_esm_attention</code> <code>bool</code> <p>Flag to skip ESM2 custom attention for TE acceleration. Defaults to False.</p> <code>token_dropout</code> <code>bool</code> <p>Flag to enable token dropout. Defaults to True.</p> <code>normalize_attention_scores</code> <code>bool</code> <p>Flag to normalize attention scores. Defaults to False.</p> <code>variable_seq_lengths</code> <code>bool</code> <p>Flag to enable variable sequence lengths. Defaults to False.</p> <code>core_attention_override</code> <code>Optional[Type[Module]]</code> <p>Optional override for core attention module. Defaults to None.</p> <p>Methods:</p> Name Description <code>restrict_biobert_spec_to_esm2</code> <p>BiobertSpecOption) -&gt; BiobertSpecOption: Validates the BiobertSpecOption to ensure it is compatible with ESM2.</p> <code>serialize_core_attention_override</code> <p>Optional[Type[torch.nn.Module]]) -&gt; Optional[str]: Serializes the core attention override module to a string.</p> <code>validate_core_attention_override</code> <p>Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.</p> <code>validate_and_set_attention_and_scaling</code> <p>Validates and sets the attention and scaling parameters based on the biobert_spec_option.</p> <code>model_validator</code> <p>MainConfig) -&gt; MainConfig: Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.</p> <code>model_class</code> <p>Returns the model class associated with this configuration.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>class ExposedESM2PretrainConfig(ExposedModelConfig[ESM2Config]):\n    \"\"\"Configuration class for ESM2 pretraining with select exposed parameters.\n\n    See the inherited ExposedModelConfig for attributes and methods from the base class. Use this class either\n    as a template or extension for custom configurations. Importantly, these kinds of classes should do two things,\n    select attributes to expose to the user, and provide validation and serialization any attributes.\n\n    Attributes:\n        use_esm_attention (bool): Flag to skip ESM2 custom attention for TE acceleration. Defaults to False.\n        token_dropout (bool): Flag to enable token dropout. Defaults to True.\n        normalize_attention_scores (bool): Flag to normalize attention scores. Defaults to False.\n        variable_seq_lengths (bool): Flag to enable variable sequence lengths. Defaults to False.\n        core_attention_override (Optional[Type[torch.nn.Module]]): Optional override for core attention module. Defaults to None.\n\n    Methods:\n        restrict_biobert_spec_to_esm2(cls, biobert_spec_option: BiobertSpecOption) -&gt; BiobertSpecOption:\n            Validates the BiobertSpecOption to ensure it is compatible with ESM2.\n        serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n            Serializes the core attention override module to a string.\n        validate_core_attention_override(cls, value):\n            Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\n        validate_and_set_attention_and_scaling(self):\n            Validates and sets the attention and scaling parameters based on the biobert_spec_option.\n        model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n            Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n        model_class(self) -&gt; Type[ESM2Config]:\n            Returns the model class associated with this configuration.\n    \"\"\"\n\n    use_esm_attention: bool = False  # Skip ESM2 custom attention for TE acceleration. Still passes golden value test.\n    token_dropout: bool = True\n    normalize_attention_scores: bool = False\n    variable_seq_lengths: bool = False\n    core_attention_override: Type[torch.nn.Module] | None = None\n\n    @field_serializer(\"core_attention_override\")\n    def serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n        \"\"\"Serializes the core attention override module to a string.\"\"\"\n        if value is None:\n            return None\n        return f\"{value.__module__}.{value.__name__}\"\n\n    @field_validator(\"core_attention_override\", mode=\"before\")\n    def validate_core_attention_override(cls, value):\n        \"\"\"Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\"\"\"\n        if value is None:\n            return None\n        if isinstance(value, str):\n            module_name, class_name = value.rsplit(\".\", 1)\n            try:\n                module = importlib.import_module(module_name)\n                cls = getattr(module, class_name)\n                if not issubclass(cls, torch.nn.Module):\n                    raise ValueError(f\"{cls} is not a subclass of torch.nn.Module\")\n                return cls\n            except (ImportError, AttributeError):\n                raise ValueError(f\"Cannot import {value}\")\n        return value\n\n    @model_validator(mode=\"after\")\n    def validate_and_set_attention_and_scaling(self):\n        \"\"\"Validates and sets the attention and scaling parameters based on the biobert_spec_option.\"\"\"\n        logging.info(\n            \"Mutating apply_query_key_layer_scaling and core_attention_override based on biobert_spec_option..\"\n        )\n        if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            self.apply_query_key_layer_scaling = False\n            self.core_attention_override = ESM2TEDotProductAttention\n        elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n            logging.warning(\n                \"BiobertSpecOption.esm2_bert_layer_local_spec is deprecated. \"\n                \"Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n            )\n            self.apply_query_key_layer_scaling = True\n            self.core_attention_override = ESM2DotProductAttention\n        return self\n\n    def model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n        \"\"\"Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n\n        The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig.\n        Additionally, it provides validation for sequence length and parallelism settings.\n\n        Args:\n            global_cfg (MainConfig): The global configuration object.\n        \"\"\"\n        global_cfg = super().model_validator(global_cfg)\n        # Need to ensure that at the least we have access to min_seq_length and max_seq_length\n        if not isinstance(global_cfg.data_config, ESM2DataConfig):\n            raise TypeError(f\"ESM2PretrainConfig requires ESM2DataConfig, got {global_cfg.data_config=}\")\n\n        pipeline_model_parallel_size, tensor_model_parallel_size = (\n            global_cfg.parallel_config.pipeline_model_parallel_size,\n            global_cfg.parallel_config.tensor_model_parallel_size,\n        )\n        min_seq_length, max_seq_length = global_cfg.data_config.min_seq_length, global_cfg.data_config.max_seq_length\n        assert (\n            self.variable_seq_lengths\n            == (pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length)\n        ), \"Must set variable_seq_lengths to True when min_seq_length != max_seq_length under pipeline or tensor parallelism.\"\n        return global_cfg\n\n    def model_class(self) -&gt; Type[ESM2Config]:\n        \"\"\"Returns the model class associated with this configuration.\"\"\"\n        return ESM2Config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.model_class","title":"<code>model_class()</code>","text":"<p>Returns the model class associated with this configuration.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[ESM2Config]:\n    \"\"\"Returns the model class associated with this configuration.\"\"\"\n    return ESM2Config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.model_validator","title":"<code>model_validator(global_cfg)</code>","text":"<p>Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.</p> <p>The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig. Additionally, it provides validation for sequence length and parallelism settings.</p> <p>Parameters:</p> Name Type Description Default <code>global_cfg</code> <code>MainConfig</code> <p>The global configuration object.</p> required Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>def model_validator(self, global_cfg: MainConfig) -&gt; MainConfig:\n    \"\"\"Validates the global configuration, ensuring compatibility with ESM2DataConfig and parallel settings.\n\n    The global validator acts on the MainConfig, this couples together the ESM2DataConfig with ESM2PretrainingConfig.\n    Additionally, it provides validation for sequence length and parallelism settings.\n\n    Args:\n        global_cfg (MainConfig): The global configuration object.\n    \"\"\"\n    global_cfg = super().model_validator(global_cfg)\n    # Need to ensure that at the least we have access to min_seq_length and max_seq_length\n    if not isinstance(global_cfg.data_config, ESM2DataConfig):\n        raise TypeError(f\"ESM2PretrainConfig requires ESM2DataConfig, got {global_cfg.data_config=}\")\n\n    pipeline_model_parallel_size, tensor_model_parallel_size = (\n        global_cfg.parallel_config.pipeline_model_parallel_size,\n        global_cfg.parallel_config.tensor_model_parallel_size,\n    )\n    min_seq_length, max_seq_length = global_cfg.data_config.min_seq_length, global_cfg.data_config.max_seq_length\n    assert (\n        self.variable_seq_lengths\n        == (pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length)\n    ), \"Must set variable_seq_lengths to True when min_seq_length != max_seq_length under pipeline or tensor parallelism.\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.serialize_core_attention_override","title":"<code>serialize_core_attention_override(value)</code>","text":"<p>Serializes the core attention override module to a string.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@field_serializer(\"core_attention_override\")\ndef serialize_core_attention_override(self, value: Optional[Type[torch.nn.Module]]) -&gt; Optional[str]:\n    \"\"\"Serializes the core attention override module to a string.\"\"\"\n    if value is None:\n        return None\n    return f\"{value.__module__}.{value.__name__}\"\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.validate_and_set_attention_and_scaling","title":"<code>validate_and_set_attention_and_scaling()</code>","text":"<p>Validates and sets the attention and scaling parameters based on the biobert_spec_option.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_and_set_attention_and_scaling(self):\n    \"\"\"Validates and sets the attention and scaling parameters based on the biobert_spec_option.\"\"\"\n    logging.info(\n        \"Mutating apply_query_key_layer_scaling and core_attention_override based on biobert_spec_option..\"\n    )\n    if self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n        self.apply_query_key_layer_scaling = False\n        self.core_attention_override = ESM2TEDotProductAttention\n    elif self.biobert_spec_option == BiobertSpecOption.esm2_bert_layer_local_spec:\n        logging.warning(\n            \"BiobertSpecOption.esm2_bert_layer_local_spec is deprecated. \"\n            \"Use BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec instead.\"\n        )\n        self.apply_query_key_layer_scaling = True\n        self.core_attention_override = ESM2DotProductAttention\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/config_models/#bionemo.esm2.run.config_models.ExposedESM2PretrainConfig.validate_core_attention_override","title":"<code>validate_core_attention_override(value)</code>","text":"<p>Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.</p> Source code in <code>bionemo/esm2/run/config_models.py</code> <pre><code>@field_validator(\"core_attention_override\", mode=\"before\")\ndef validate_core_attention_override(cls, value):\n    \"\"\"Validates the core attention override module, ensuring it is a subclass of torch.nn.Module.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, str):\n        module_name, class_name = value.rsplit(\".\", 1)\n        try:\n            module = importlib.import_module(module_name)\n            cls = getattr(module, class_name)\n            if not issubclass(cls, torch.nn.Module):\n                raise ValueError(f\"{cls} is not a subclass of torch.nn.Module\")\n            return cls\n        except (ImportError, AttributeError):\n            raise ValueError(f\"Cannot import {value}\")\n    return value\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/main/","title":"Main","text":""},{"location":"API_reference/bionemo/esm2/run/recipes/","title":"Recipes","text":""},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.default_adam_optimizer_with_cosine_annealing_recipe","title":"<code>default_adam_optimizer_with_cosine_annealing_recipe()</code>","text":"<p>Default optimizer scheduler config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def default_adam_optimizer_with_cosine_annealing_recipe() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Default optimizer scheduler config for ESM2.\"\"\"\n    return OptimizerSchedulerConfig()\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_experiment_config","title":"<code>esm2_3b_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 650m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,\n        result_dir=result_dir,\n        experiment_name=\"esm2-3b-pretraining\",\n        # TODO should this be exposed?\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_model_config","title":"<code>esm2_3b_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 3b.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=36,\n        hidden_size=2560,\n        ffn_hidden_size=2560 * 4,\n        num_attention_heads=40,\n        seq_length=1024,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_parallel_config","title":"<code>esm2_3b_parallel_config()</code>","text":"<p>Parallel config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Parallel config for ESM2 3b.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=2,\n        pipeline_model_parallel_size=1,\n        # TODO: is this correct?\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        # NOTE assumes 8xGPU node. Can always edit the config.\n        num_devices=8,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_recipe","title":"<code>esm2_3b_recipe(args)</code>","text":"<p>Recipe for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 3b.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_3b_parallel_config(),\n        training_config=esm2_base_training_config(),  # no changes for 8m\n        bionemo_model_config=esm2_3b_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(),  # no changes for 8m\n        experiment_config=esm2_3b_experiment_config(args.result_dir),\n        wandb_config=esm2_3b_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_3b_wandb_config","title":"<code>esm2_3b_wandb_config()</code>","text":"<p>Wandb config for ESM2 3b.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_3b_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 3b.\"\"\"\n    return WandbConfig(\n        entity=\"esm2-3b_pretraining\",\n        project=\"esm2-3b_pretraining\",\n        group=\"esm2-3b\",\n        tags=[\"esm2-650m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_experiment_config","title":"<code>esm2_650m_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 650m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,\n        result_dir=result_dir,\n        experiment_name=\"esm2-650m-pretraining\",\n        # TODO should this be exposed?\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_model_config","title":"<code>esm2_650m_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 650m.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=33,\n        hidden_size=1280,\n        ffn_hidden_size=1280 * 4,\n        seq_length=1024,\n        num_attention_heads=20,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_recipe","title":"<code>esm2_650m_recipe(args)</code>","text":"<p>Recipe for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 650m.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_base_parallel_config(),\n        training_config=esm2_base_training_config(),  # no changes for 8m\n        bionemo_model_config=esm2_650m_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(),  # no changes for 8m\n        experiment_config=esm2_650m_experiment_config(args.result_dir),\n        wandb_config=esm2_650m_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_650m_wandb_config","title":"<code>esm2_650m_wandb_config()</code>","text":"<p>Wandb config for ESM2 650m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_650m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 650m.\"\"\"\n    return WandbConfig(\n        entity=\"esm2-650m_pretraining\",\n        project=\"esm2-650m_pretraining\",\n        group=\"esm2-650m\",\n        tags=[\"esm2\", \"pretraining\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_experiment_config","title":"<code>esm2_8m_experiment_config(result_dir)</code>","text":"<p>Experiment config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2 8m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=50,  # default set in previous script.\n        result_dir=result_dir,\n        experiment_name=\"esm2-8m-pretraining\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_model_config","title":"<code>esm2_8m_model_config(initial_ckpt_path=None)</code>","text":"<p>Model config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_model_config(initial_ckpt_path=None) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 8m.\"\"\"\n    return ExposedESM2PretrainConfig(\n        num_layers=6,\n        hidden_size=320,\n        ffn_hidden_size=320 * 4,\n        num_attention_heads=20,\n        seq_length=1024,\n        biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n        initial_ckpt_path=initial_ckpt_path,\n        get_attention_mask_from_fusion=True,\n        params_dtype=\"bf16-mixed\",\n        pipeline_dtype=\"bf16-mixed\",\n        autocast_dtype=\"bf16-mixed\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_recipe","title":"<code>esm2_8m_recipe(args)</code>","text":"<p>Recipe for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_recipe(args) -&gt; MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig]:\n    \"\"\"Recipe for ESM2 8m.\"\"\"\n    return MainConfig(\n        data_config=esm2_base_data_config(args),\n        parallel_config=esm2_base_parallel_config(),\n        training_config=esm2_base_training_config(),  # no changes for 8m\n        bionemo_model_config=esm2_8m_model_config(args.initial_ckpt_path),\n        optim_config=esm2_base_optimizer_scheduler_config(),  # no changes for 8m\n        experiment_config=esm2_8m_experiment_config(args.result_dir),\n        wandb_config=esm2_8m_wandb_config(),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_8m_wandb_config","title":"<code>esm2_8m_wandb_config()</code>","text":"<p>Wandb config for ESM2 8m.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_8m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for ESM2 8m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"esm2-8m_pretraining\",\n        project=\"esm2-8m_pretraining\",\n        group=\"esm2-8m\",\n        tags=[\"esm2\", \"pretraining\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_data_config","title":"<code>esm2_base_data_config(args)</code>","text":"<p>Base data config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_data_config(args) -&gt; ESM2DataConfig:\n    \"\"\"Base data config for ESM2.\"\"\"\n    data_config = ESM2DataConfig(\n        min_seq_length=1024,\n        max_seq_length=1024,\n        micro_batch_size=1,\n        num_dataset_workers=8,\n        train_cluster_path=args.train_cluster_path,\n        train_database_path=args.train_database_path,\n        valid_cluster_path=args.valid_cluster_path,\n        valid_database_path=args.valid_database_path,\n    )\n    return data_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_optimizer_scheduler_config","title":"<code>esm2_base_optimizer_scheduler_config()</code>","text":"<p>Base optimizer scheduler config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_optimizer_scheduler_config() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Base optimizer scheduler config for ESM2.\"\"\"\n    return OptimizerSchedulerConfig(\n        optimizer=\"adam\", lr=4e-4, interval=\"step\", monitor=\"val_loss\", lr_scheduler=\"warmup_anneal\", warmup_steps=2000\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_parallel_config","title":"<code>esm2_base_parallel_config()</code>","text":"<p>Base parallel config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for ESM2.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=1,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_base_training_config","title":"<code>esm2_base_training_config()</code>","text":"<p>Base training config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_base_training_config() -&gt; TrainingConfig:\n    \"\"\"Base training config for ESM2.\"\"\"\n    return TrainingConfig(\n        max_steps=500000,\n        limit_val_batches=1.0,\n        val_check_interval=10_000,\n        precision=\"bf16-mixed\",\n        include_perplexity=True,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_tiny_model_config","title":"<code>esm2_tiny_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec, variable_seq_lengths=False)</code>","text":"<p>Model config for ESM2 tiny, used for testing.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_tiny_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec,\n    variable_seq_lengths: bool = False,\n) -&gt; ExposedESM2PretrainConfig:\n    \"\"\"Model config for ESM2 tiny, used for testing.\"\"\"\n    return ExposedESM2PretrainConfig(\n        seq_length=seq_length,\n        num_layers=2,\n        hidden_size=32,\n        num_attention_heads=2,\n        ffn_hidden_size=4 * 32,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        biobert_spec_option=biobert_spec_option,\n        get_attention_mask_from_fusion=True,\n        nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(initial_ckpt_path) if initial_ckpt_path is not None else None,\n        variable_seq_lengths=variable_seq_lengths,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.esm2_tiny_test_recipe","title":"<code>esm2_tiny_test_recipe(args)</code>","text":"<p>Test recipe for ESM2 tiny, used for testing.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def esm2_tiny_test_recipe(args):\n    \"\"\"Test recipe for ESM2 tiny, used for testing.\"\"\"\n    parallel_config = simple_parallel_recipe()\n    training_config = tiny_train_config_recipe()\n\n    data_config = ESM2DataConfig(\n        min_seq_length=128,\n        max_seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=1,\n        train_cluster_path=args.train_cluster_path,\n        train_database_path=args.train_database_path,\n        valid_cluster_path=args.valid_cluster_path,\n        valid_database_path=args.valid_database_path,\n    )\n    bionemo_model_config = esm2_tiny_model_config(\n        seq_length=data_config.max_seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    optim_config = default_adam_optimizer_with_cosine_annealing_recipe()\n    experiment_config = experiment_config_recipe(args.result_dir)\n    wandb_config = WandbConfig(\n        project=\"bionemo2-demo\",\n        entity=\"nvidia\",\n        offline=True,\n        tags=[],\n        group=\"dev\",\n        id=\"dev\",\n        log_model=False,\n        anonymous=True,\n    )\n    main_config = MainConfig[ExposedESM2PretrainConfig, ESM2DataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.experiment_config_recipe","title":"<code>experiment_config_recipe(result_dir='./results')</code>","text":"<p>Experiment config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def experiment_config_recipe(result_dir=\"./results\") -&gt; ExperimentConfig:\n    \"\"\"Experiment config for ESM2.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"default_experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"val_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.simple_parallel_recipe","title":"<code>simple_parallel_recipe(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=1)</code>","text":"<p>Simple parallel recipe for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def simple_parallel_recipe(\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    num_devices: int = 1,\n    accumulate_grad_batches: int = 1,\n) -&gt; ParallelConfig:\n    \"\"\"Simple parallel recipe for ESM2.\"\"\"\n    assert (\n        num_devices &gt;= tensor_model_parallel_size * pipeline_model_parallel_size\n    ), \"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\"\n    return ParallelConfig(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        num_devices=num_devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/esm2/run/recipes/#bionemo.esm2.run.recipes.tiny_train_config_recipe","title":"<code>tiny_train_config_recipe()</code>","text":"<p>Tiny training config for ESM2.</p> Source code in <code>bionemo/esm2/run/recipes.py</code> <pre><code>def tiny_train_config_recipe() -&gt; TrainingConfig:\n    \"\"\"Tiny training config for ESM2.\"\"\"\n    return TrainingConfig(max_steps=10, limit_val_batches=2, val_check_interval=2)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/","title":"Lightning basic","text":"<p>This is intended to be a minimal self-container NeMo2 example.</p>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>IOMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class BionemoLightningModule(pl.LightningModule, io.IOMixin, LightningPassthroughPredictionMixin):\n    \"\"\"A very basic lightning module for testing the megatron strategy and the megatron-nemo2-bionemo contract.\"\"\"\n\n    def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n        \"\"\"Initializes the model.\n\n        Args:\n            config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.optim = MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=1e-4,\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                bf16=config.bf16,\n                fp16=config.fp16,\n                params_dtype=config.params_dtype,\n            ),\n        )\n        # Bind the configure_optimizers method to the model\n        self.optim.connect(self)\n\n    def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n        \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n        !!! note\n\n            The `training_step` defines the training loop and is independent of the `forward` method here.\n\n        Args:\n            batch: A dictionary of data.\n            batch_idx: The index of the batch.\n\n        Returns:\n            The output of the model.\n        \"\"\"\n        x = batch[\"data\"]\n        return self.module(x)\n\n    def training_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n        Background:\n        - NeMo's Strategy overrides this method.\n        - The strategies' training step will call the forward method of the model.\n        - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n        - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n            MegatronParallel class.\n        - Which then calls the training_step function here.\n\n        In this particular use case, we simply call the forward method of this class, the lightning module.\n\n        Args:\n            batch: A dictionary of data. requires `batch_idx` as default None.\n            batch_idx: The index of the batch.\n        \"\"\"\n        # Forward pass\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the training loss reduction function\n        loss_reduction = self.training_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n\n        # Log the training loss\n        self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return predictions\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at validation.\"\"\"\n        predictions = self(batch, batch_idx)\n\n        # Calculate loss using the validation loss reduction function\n        loss_reduction = self.validation_loss_reduction()\n        loss_reduction.setup(batch)\n        loss = loss_reduction(predictions)\n        # Log the validation loss\n        self.log(\n            \"val_loss\",\n            loss[1][\"avg\"],\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n        return predictions\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None):\n        \"\"\"Alias for forward step at prediction.\"\"\"\n        return self(batch, batch_idx)\n\n    def training_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def test_loss_reduction(self) -&gt; MegatronLossReduction:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n        Returns:\n        A MegatronLossReduction\n        \"\"\"\n        return self.loss_reduction_class()()\n\n    def configure_model(self) -&gt; None:\n        \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n        self.module = self.config.configure_model()\n\n    def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n        \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n        return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MegatronBioNeMoTrainableModelConfig</code> <p>a Config object necessary to construct the actual nn.Module (the thing that has the parameters).</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: MegatronBioNeMoTrainableModelConfig):\n    \"\"\"Initializes the model.\n\n    Args:\n        config: a Config object necessary to construct the actual nn.Module (the thing that has the parameters).\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.optim = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=1e-4,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            bf16=config.bf16,\n            fp16=config.fp16,\n            params_dtype=config.params_dtype,\n        ),\n    )\n    # Bind the configure_optimizers method to the model\n    self.optim.connect(self)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>This configures the model. It is called lazily by the megatron strategy.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"This configures the model. It is called lazily by the megatron strategy.\"\"\"\n    self.module = self.config.configure_model()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.forward","title":"<code>forward(batch, batch_idx)</code>","text":"<p>This forward will be called by the megatron scheduler and it will be wrapped.</p> <p>Note</p> <p>The <code>training_step</code> defines the training loop and is independent of the <code>forward</code> method here.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict</code> <p>A dictionary of data.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The output of the model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: Dict, batch_idx: int) -&gt; Any:\n    \"\"\"This forward will be called by the megatron scheduler and it will be wrapped.\n\n    !!! note\n\n        The `training_step` defines the training loop and is independent of the `forward` method here.\n\n    Args:\n        batch: A dictionary of data.\n        batch_idx: The index of the batch.\n\n    Returns:\n        The output of the model.\n    \"\"\"\n    x = batch[\"data\"]\n    return self.module(x)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.loss_reduction_class","title":"<code>loss_reduction_class()</code>","text":"<p>Get the loss reduction class the user has specified in their config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n    \"\"\"Get the loss reduction class the user has specified in their config.\"\"\"\n    return self.config.get_loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at prediction.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at prediction.\"\"\"\n    return self(batch, batch_idx)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.test_loss_reduction","title":"<code>test_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def test_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>The training step is where the loss is calculated and the backpropagation is done.</p> <p>Background: - NeMo's Strategy overrides this method. - The strategies' training step will call the forward method of the model. - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model. - That wrapped forward step is then executed inside the Mcore scheduler, which calls the <code>_forward_step</code> method from the     MegatronParallel class. - Which then calls the training_step function here.</p> <p>In this particular use case, we simply call the forward method of this class, the lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>A dictionary of data. requires <code>batch_idx</code> as default None.</p> required <code>batch_idx</code> <code>Optional[int]</code> <p>The index of the batch.</p> <code>None</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"The training step is where the loss is calculated and the backpropagation is done.\n\n    Background:\n    - NeMo's Strategy overrides this method.\n    - The strategies' training step will call the forward method of the model.\n    - That forward method then calls the wrapped forward step of MegatronParallel which wraps the forward method of the model.\n    - That wrapped forward step is then executed inside the Mcore scheduler, which calls the `_forward_step` method from the\n        MegatronParallel class.\n    - Which then calls the training_step function here.\n\n    In this particular use case, we simply call the forward method of this class, the lightning module.\n\n    Args:\n        batch: A dictionary of data. requires `batch_idx` as default None.\n        batch_idx: The index of the batch.\n    \"\"\"\n    # Forward pass\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the training loss reduction function\n    loss_reduction = self.training_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n\n    # Log the training loss\n    self.log(\"train_loss\", loss[1][\"avg\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return predictions\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_loss_reduction","title":"<code>validation_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> <p>Returns: A MegatronLossReduction</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_loss_reduction(self) -&gt; MegatronLossReduction:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\n\n    Returns:\n    A MegatronLossReduction\n    \"\"\"\n    return self.loss_reduction_class()()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward step at validation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None):\n    \"\"\"Alias for forward step at validation.\"\"\"\n    predictions = self(batch, batch_idx)\n\n    # Calculate loss using the validation loss reduction function\n    loss_reduction = self.validation_loss_reduction()\n    loss_reduction.setup(batch)\n    loss = loss_reduction(predictions)\n    # Log the validation loss\n    self.log(\n        \"val_loss\",\n        loss[1][\"avg\"],\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n\n    return predictions\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction","title":"<code>ClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        digits = batch[\"label\"]\n        digit_logits = forward_out\n        loss = nn.functional.cross_entropy(digit_logits, digits)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Tensor</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Tensor) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    digits = batch[\"label\"]\n    digit_logits = forward_out\n    loss = nn.functional.cross_entropy(digit_logits, digits)\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothConfig","title":"<code>ExampleFineTuneBothConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneBothModel', 'MSEPlusClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneBothConfig(\n    ExampleGenericConfig[\"ExampleFineTuneBothModel\", \"MSEPlusClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneBothModel] = ExampleFineTuneBothModel\n    loss_cls: Type[MSEPlusClassifierLossReduction] = MSEPlusClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneBothModel","title":"<code>ExampleFineTuneBothModel</code>","text":"<p>               Bases: <code>ExampleModel</code></p> <p>Example of taking the example model and adding an output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneBothModel(ExampleModel):\n    \"\"\"Example of taking the example model and adding an output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; ExampleFineTuneOutput:\n        parent_out: ExampleModelOutput = super().forward(x)\n        digit_logits = self.digit_classifier(parent_out[\"z\"])\n        return {\n            \"x_hat\": parent_out[\"x_hat\"],\n            \"z\": parent_out[\"z\"],\n            \"digit_logits\": digit_logits,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneConfig","title":"<code>ExampleFineTuneConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleFineTuneConfig', 'ClassifierLossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>ExampleConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleFineTuneConfig(\n    ExampleGenericConfig[\"ExampleFineTuneConfig\", \"ClassifierLossReduction\"], iom.IOMixinWithGettersSetters\n):\n    \"\"\"ExampleConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleFineTuneModel] = ExampleFineTuneModel\n    loss_cls: Type[ClassifierLossReduction] = ClassifierLossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneModel","title":"<code>ExampleFineTuneModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>Example of taking the example model and replacing output task.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneModel(ExampleModelTrunk):\n    \"\"\"Example of taking the example model and replacing output task.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig):\n        super().__init__(config)\n        # 10 output digits, and use the latent output layer (z) for making predictions\n        self.digit_classifier = nn.Linear(self.linear2.out_features, 10)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        z: Tensor = super().forward(x)\n        digit_logits = self.digit_classifier(z)  # to demonstrate flexibility, in this case we return a tensor\n        return digit_logits\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleFineTuneOutput","title":"<code>ExampleFineTuneOutput</code>","text":"<p>               Bases: <code>ExampleModelOutput</code></p> <p>Output for the fine-tuned example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleFineTuneOutput(ExampleModelOutput):\n    \"\"\"Output for the fine-tuned example model implementation.\"\"\"\n\n    digit_logits: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig","title":"<code>ExampleGenericConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[ExampleModelT, MegatronLossType]</code>, <code>MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]</code></p> <p>ExampleGenericConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass ExampleGenericConfig(\n    Generic[ExampleModelT, MegatronLossType], MegatronBioNeMoTrainableModelConfig[ExampleModelT, MegatronLossType]\n):\n    \"\"\"ExampleGenericConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    loss_cls: Type[MegatronLossType] = MSELossReduction  # type: ignore  # this will get overriden by children\n    hidden_size: int = 64  # Needs to be set to avoid zero division error in megatron :(\n    num_attention_heads: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    num_layers: int = 1  # Needs to be set to avoid zero division error in megatron :(\n    # IMPORTANT: Since we're adding/overriding the loss_cls, and that's not how we generally track this, we need to\n    #   add this into the list of config settings that we do not draw from the loaded checkpoint when restoring.\n    override_parent_fields: List[str] = field(default_factory=lambda: OVERRIDE_BIONEMO_CONFIG_DEFAULTS + [\"loss_cls\"])\n\n    def configure_model(self) -&gt; ExampleModelT:\n        \"\"\"Uses model_cls and loss_cls to configure the model.\n\n        Note: Must pass self into Model since model requires having a config object.\n\n        Returns:\n            The model object.\n        \"\"\"\n        # 1. first load any settings that may exist in the checkpoint related to the model.\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n        # 2. then initialize the model\n        model = self.model_cls(self)\n        # 3. Load weights from the checkpoint into the model\n        if self.initial_ckpt_path:\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n        \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n        return self.loss_cls\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.configure_model","title":"<code>configure_model()</code>","text":"<p>Uses model_cls and loss_cls to configure the model.</p> <p>Note: Must pass self into Model since model requires having a config object.</p> <p>Returns:</p> Type Description <code>ExampleModelT</code> <p>The model object.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def configure_model(self) -&gt; ExampleModelT:\n    \"\"\"Uses model_cls and loss_cls to configure the model.\n\n    Note: Must pass self into Model since model requires having a config object.\n\n    Returns:\n        The model object.\n    \"\"\"\n    # 1. first load any settings that may exist in the checkpoint related to the model.\n    if self.initial_ckpt_path:\n        self.load_settings_from_checkpoint(self.initial_ckpt_path)\n    # 2. then initialize the model\n    model = self.model_cls(self)\n    # 3. Load weights from the checkpoint into the model\n    if self.initial_ckpt_path:\n        self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleGenericConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:\n    \"\"\"Use loss_cls to configure the loss, since we do not change the settings of the loss based on the config.\"\"\"\n    return self.loss_cls\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel","title":"<code>ExampleModel</code>","text":"<p>               Bases: <code>ExampleModelTrunk</code></p> <p>An example model.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModel(ExampleModelTrunk):\n    \"\"\"An example model.\"\"\"\n\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        self.linear3 = nn.Linear(3, 64)\n        self.relu2 = nn.ReLU()\n        self.linear4 = nn.Linear(64, 28 * 28)\n\n    def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x: The input data.\n\n        Returns:\n            x_hat: The result of the last linear layer of the network.\n        \"\"\"\n        z: Tensor = super().forward(x)\n        x_hat = self.linear3(z)\n        x_hat = self.relu2(x_hat)\n        x_hat = self.linear4(x_hat)\n        return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    self.linear3 = nn.Linear(3, 64)\n    self.relu2 = nn.ReLU()\n    self.linear4 = nn.Linear(64, 28 * 28)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input data.</p> required <p>Returns:</p> Name Type Description <code>x_hat</code> <code>ExampleModelOutput</code> <p>The result of the last linear layer of the network.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, x: Tensor) -&gt; ExampleModelOutput:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        x: The input data.\n\n    Returns:\n        x_hat: The result of the last linear layer of the network.\n    \"\"\"\n    z: Tensor = super().forward(x)\n    x_hat = self.linear3(z)\n    x_hat = self.relu2(x_hat)\n    x_hat = self.linear4(x_hat)\n    return {\"x_hat\": x_hat, \"z\": z}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelOutput","title":"<code>ExampleModelOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output for the example model implementation.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelOutput(TypedDict):\n    \"\"\"Output for the example model implementation.\"\"\"\n\n    x_hat: Tensor\n    z: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk","title":"<code>ExampleModelTrunk</code>","text":"<p>               Bases: <code>MegatronModule</code></p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class ExampleModelTrunk(MegatronModule):\n    def __init__(self, config: ModelParallelConfig) -&gt; None:\n        \"\"\"Constructor of the model.\n\n        Args:\n            config: The config object is responsible for telling the strategy what model to create.\n        \"\"\"\n        super().__init__(config)\n        # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n        #  parallelizable megatron linear layers.\n        self.model_type: ModelType = ModelType.encoder_or_decoder\n        self.linear1 = nn.Linear(28 * 28, 64)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(64, 3)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # we could return a dictionary of strings to tensors here, but let's demonstrate this is not necessary\n        x = x.view(x.size(0), -1)\n        z = self.linear1(x)\n        z = self.relu(z)\n        z = self.linear2(z)\n        return z\n\n    def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n        \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor of the model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ModelParallelConfig</code> <p>The config object is responsible for telling the strategy what model to create.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(self, config: ModelParallelConfig) -&gt; None:\n    \"\"\"Constructor of the model.\n\n    Args:\n        config: The config object is responsible for telling the strategy what model to create.\n    \"\"\"\n    super().__init__(config)\n    # FIXME add an assertion that the user is not trying to do tensor parallelism since this doesn't use\n    #  parallelizable megatron linear layers.\n    self.model_type: ModelType = ModelType.encoder_or_decoder\n    self.linear1 = nn.Linear(28 * 28, 64)\n    self.relu = nn.ReLU()\n    self.linear2 = nn.Linear(64, 3)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.ExampleModelTrunk.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>This would be needed for model parallel and other kinds of more complicated forward passes in megatron.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def set_input_tensor(self, input_tensor: Optional[Tensor]) -&gt; None:\n    \"\"\"This _would_ be needed for model parallel and other kinds of more complicated forward passes in megatron.\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset","title":"<code>MNISTCustomDataset</code>","text":"<p>               Bases: <code>MNIST</code></p> <p>A Wrapper for the MNIST Dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTCustomDataset(MNIST):\n    \"\"\"A Wrapper for the MNIST Dataset.\"\"\"\n\n    def __getitem__(self, idx: int) -&gt; MnistItem:\n        \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n        This is instead of a Tuple or tensor.\n\n        Args:\n            idx: The index we want to grab, an int.\n\n        Returns:\n            A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n        \"\"\"\n        data, label = super().__getitem__(idx)\n\n        return {\n            \"data\": data,\n            \"label\": label,\n            \"idx\": idx,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTCustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Wraps the getitem method of the MNIST dataset such that we return a Dict.</p> <p>This is instead of a Tuple or tensor.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index we want to grab, an int.</p> required <p>Returns:</p> Type Description <code>MnistItem</code> <p>A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; MnistItem:\n    \"\"\"Wraps the getitem method of the MNIST dataset such that we return a Dict.\n\n    This is instead of a Tuple or tensor.\n\n    Args:\n        idx: The index we want to grab, an int.\n\n    Returns:\n        A dict containing the data (\"x\"), label (\"y\"), and index (\"idx\").\n    \"\"\"\n    data, label = super().__getitem__(idx)\n\n    return {\n        \"data\": data,\n        \"label\": label,\n        \"idx\": idx,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule","title":"<code>MNISTDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A Megatron Compatible Data Module for MNIST.</p> <p>Attributes: data_dir: data directory micro_batch_size: batch_size global_batch_size: global batch size max_len: maximal sequence length for megatron sampler rampup_batch_size: ramp up batch size num_workers: number of workers data_sampler: data_sampler set to be a megatron one</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    \"\"\"A Megatron Compatible Data Module for MNIST.\n\n    Attributes:\n    data_dir: data directory\n    micro_batch_size: batch_size\n    global_batch_size: global batch size\n    max_len: maximal sequence length for megatron sampler\n    rampup_batch_size: ramp up batch size\n    num_workers: number of workers\n    data_sampler: data_sampler set to be a megatron one\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n        batch_size: int = 32,\n        num_workers: int = 0,\n        global_batch_size: int | None = None,\n        output_log: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize class.\n\n        Args:\n            data_dir: data directory\n            batch_size: batch_size\n            global_batch_size: global batch size\n            num_workers: number of workers\n            output_log: whether to output logs\n\n        \"\"\"\n        super().__init__()\n        self.data_dir = data_dir\n        self.micro_batch_size = batch_size\n        self.global_batch_size = global_batch_size or batch_size\n        self.max_len = 1048\n        self.rampup_batch_size = None\n        self.num_workers = num_workers\n        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n        # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n        # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n        # the place where the global batch size is constructed.\n        self.data_sampler = MegatronDataSampler(\n            seq_len=self.max_len,\n            micro_batch_size=self.micro_batch_size,\n            global_batch_size=self.global_batch_size,\n            rampup_batch_size=self.rampup_batch_size,\n            output_log=output_log,\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Sets up the datasets.\n\n        Args:\n            stage: can be one of train / test / predict.\n        \"\"\"\n        self.mnist_test = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(\n                MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n            ),\n            seed=43,\n            shuffle=False,\n        )\n        mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n        mnist_train, mnist_val = torch.utils.data.random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n        self.mnist_train = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n        )\n\n        self.mnist_val = MultiEpochDatasetResampler(\n            IdentityMultiEpochDatasetWrapper(mnist_val),\n            seed=45,\n            shuffle=False,\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the training dataloader.\"\"\"\n        return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the validation dataloader.\"\"\"\n        return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"Returns the prediction dataloader.\"\"\"\n        return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.__init__","title":"<code>__init__(data_dir=str(BIONEMO_CACHE_DIR), batch_size=32, num_workers=0, global_batch_size=None, output_log=True)</code>","text":"<p>Initialize class.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | PathLike</code> <p>data directory</p> <code>str(BIONEMO_CACHE_DIR)</code> <code>batch_size</code> <code>int</code> <p>batch_size</p> <code>32</code> <code>global_batch_size</code> <code>int | None</code> <p>global batch size</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>number of workers</p> <code>0</code> <code>output_log</code> <code>bool</code> <p>whether to output logs</p> <code>True</code> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str | os.PathLike = str(BIONEMO_CACHE_DIR),\n    batch_size: int = 32,\n    num_workers: int = 0,\n    global_batch_size: int | None = None,\n    output_log: bool = True,\n) -&gt; None:\n    \"\"\"Initialize class.\n\n    Args:\n        data_dir: data directory\n        batch_size: batch_size\n        global_batch_size: global batch size\n        num_workers: number of workers\n        output_log: whether to output logs\n\n    \"\"\"\n    super().__init__()\n    self.data_dir = data_dir\n    self.micro_batch_size = batch_size\n    self.global_batch_size = global_batch_size or batch_size\n    self.max_len = 1048\n    self.rampup_batch_size = None\n    self.num_workers = num_workers\n    #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n    # Wraps the datasampler with the MegatronDataSampler. The MegatronDataSampler is a wrapper that allows the sampler\n    # to be used with megatron. It sets up the capability to utilize micro-batching and gradient accumulation. It is also\n    # the place where the global batch size is constructed.\n    self.data_sampler = MegatronDataSampler(\n        seq_len=self.max_len,\n        micro_batch_size=self.micro_batch_size,\n        global_batch_size=self.global_batch_size,\n        rampup_batch_size=self.rampup_batch_size,\n        output_log=output_log,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns the prediction dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the prediction dataloader.\"\"\"\n    return DataLoader(self.mnist_test, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Sets up the datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>can be one of train / test / predict.</p> required Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Sets up the datasets.\n\n    Args:\n        stage: can be one of train / test / predict.\n    \"\"\"\n    self.mnist_test = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(\n            MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=False)\n        ),\n        seed=43,\n        shuffle=False,\n    )\n    mnist_full = MNISTCustomDataset(self.data_dir, download=True, transform=transforms.ToTensor(), train=True)\n    mnist_train, mnist_val = torch.utils.data.random_split(\n        mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n    )\n    self.mnist_train = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_train), seed=44, shuffle=True\n    )\n\n    self.mnist_val = MultiEpochDatasetResampler(\n        IdentityMultiEpochDatasetWrapper(mnist_val),\n        seed=45,\n        shuffle=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the training dataloader.\"\"\"\n    return DataLoader(self.mnist_train, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MNISTDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Returns the validation dataloader.\"\"\"\n    return DataLoader(self.mnist_val, batch_size=self.micro_batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction","title":"<code>MSELossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSELossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        x_hat = forward_out[\"x_hat\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        loss = nn.functional.mse_loss(x_hat, xview)\n\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: Dict[str, Tensor]) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    x_hat = forward_out[\"x_hat\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    loss = nn.functional.mse_loss(x_hat, xview)\n\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSELossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction","title":"<code>MSEPlusClassifierLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code></p> <p>A class used for calculating the loss, and for logging the reduced loss across micro batches.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MSEPlusClassifierLossReduction(MegatronLossReduction):\n    \"\"\"A class used for calculating the loss, and for logging the reduced loss across micro batches.\"\"\"\n\n    def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n        \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n        Args:\n            batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n            forward_out: the output of the forward method inside LitAutoEncoder.\n\n        Returns:\n            A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n                backpropagation and the ReductionT will be passed to the reduce method\n                (which currently only works for logging.).\n        \"\"\"\n        x = batch[\"data\"]\n        digits = batch[\"label\"]\n        x_hat = forward_out[\"x_hat\"]\n        digit_logits = forward_out[\"digit_logits\"]\n        xview = x.view(x.size(0), -1).to(x_hat.dtype)\n        mse_loss = nn.functional.mse_loss(x_hat, xview)\n        classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n        loss = classifier_loss + mse_loss\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n        \"\"\"Works across micro-batches. (data on single gpu).\n\n        Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n        Args:\n            losses_reduced_per_micro_batch: a list of the outputs of forward\n\n        Returns:\n            A tensor that is the mean of the losses. (used for logging).\n        \"\"\"\n        mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>MnistItem</code> <p>A batch of data that gets passed to the original forward inside LitAutoEncoder.</p> required <code>forward_out</code> <code>ExampleFineTuneOutput</code> <p>the output of the forward method inside LitAutoEncoder.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, SameSizeLossDict]</code> <p>A tuple containing [, ReductionT] where the loss tensor will be used for backpropagation and the ReductionT will be passed to the reduce method (which currently only works for logging.). Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def forward(self, batch: MnistItem, forward_out: ExampleFineTuneOutput) -&gt; Tuple[Tensor, SameSizeLossDict]:\n    \"\"\"Calculates the loss within a micro-batch. A micro-batch is a batch of data on a single GPU.\n\n    Args:\n        batch: A batch of data that gets passed to the original forward inside LitAutoEncoder.\n        forward_out: the output of the forward method inside LitAutoEncoder.\n\n    Returns:\n        A tuple containing [&lt;loss_tensor&gt;, ReductionT] where the loss tensor will be used for\n            backpropagation and the ReductionT will be passed to the reduce method\n            (which currently only works for logging.).\n    \"\"\"\n    x = batch[\"data\"]\n    digits = batch[\"label\"]\n    x_hat = forward_out[\"x_hat\"]\n    digit_logits = forward_out[\"digit_logits\"]\n    xview = x.view(x.size(0), -1).to(x_hat.dtype)\n    mse_loss = nn.functional.mse_loss(x_hat, xview)\n    classifier_loss = nn.functional.cross_entropy(digit_logits, digits)\n    loss = classifier_loss + mse_loss\n    return loss, {\"avg\": loss}\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MSEPlusClassifierLossReduction.reduce","title":"<code>reduce(losses_reduced_per_micro_batch)</code>","text":"<p>Works across micro-batches. (data on single gpu).</p> <p>Note: This currently only works for logging and this loss will not be used for backpropagation.</p> <p>Parameters:</p> Name Type Description Default <code>losses_reduced_per_micro_batch</code> <code>Sequence[SameSizeLossDict]</code> <p>a list of the outputs of forward</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor that is the mean of the losses. (used for logging).</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>def reduce(self, losses_reduced_per_micro_batch: Sequence[SameSizeLossDict]) -&gt; Tensor:\n    \"\"\"Works across micro-batches. (data on single gpu).\n\n    Note: This currently only works for logging and this loss will not be used for backpropagation.\n\n    Args:\n        losses_reduced_per_micro_batch: a list of the outputs of forward\n\n    Returns:\n        A tensor that is the mean of the losses. (used for logging).\n    \"\"\"\n    mse_losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n    return mse_losses.mean()\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.MnistItem","title":"<code>MnistItem</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Training input for the MNIST dataset.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class MnistItem(TypedDict):\n    \"\"\"Training input for the MNIST dataset.\"\"\"\n\n    data: Tensor\n    label: Tensor\n    idx: int\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.PretrainConfig","title":"<code>PretrainConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ExampleGenericConfig['ExampleModel', 'MSELossReduction']</code>, <code>IOMixinWithGettersSetters</code></p> <p>PretrainConfig is a dataclass that is used to configure the model.</p> <p>Timers from ModelParallelConfig are required for megatron forward compatibility.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>@dataclass\nclass PretrainConfig(ExampleGenericConfig[\"ExampleModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n    \"\"\"PretrainConfig is a dataclass that is used to configure the model.\n\n    Timers from ModelParallelConfig are required for megatron forward compatibility.\n    \"\"\"\n\n    model_cls: Type[ExampleModel] = ExampleModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre>"},{"location":"API_reference/bionemo/example_model/lightning/lightning_basic/#bionemo.example_model.lightning.lightning_basic.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/example_model/lightning/lightning_basic.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/finetune_mnist/","title":"Finetune mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/finetune_mnist/#bionemo.example_model.training_scripts.finetune_mnist.run_finetune","title":"<code>run_finetune(checkpoint_dir, name, directory_name)</code>","text":"<p>Run the finetuning step.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>The directory with the previous model</p> required <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/finetune_mnist.py</code> <pre><code>def run_finetune(checkpoint_dir: str, name: str, directory_name: str):\n    \"\"\"Run the finetuning step.\n\n    Args:\n        checkpoint_dir: The directory with the previous model\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    save_dir = Path(directory_name) / \"classifier\"\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=True,\n        save_on_train_epoch_end=True,\n        monitor=\"reduced_train_loss\",\n        every_n_train_steps=25,\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n    )\n\n    nemo_logger2 = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=save_dir, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    lightning_module2 = BionemoLightningModule(\n        config=ExampleFineTuneConfig(\n            initial_ckpt_path=checkpoint_dir,\n            initial_ckpt_skip_keys_with_these_prefixes={\"digit_classifier\"},\n        )\n    )\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n    llm.train(\n        model=lightning_module2,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger2,\n        resume=resume.AutoResume(\n            resume_if_exists=True,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n    finetune_dir = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n    return finetune_dir\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/predict_mnist/","title":"Predict mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/predict_mnist/#bionemo.example_model.training_scripts.predict_mnist.run_predict","title":"<code>run_predict(finetune_dir, test_length)</code>","text":"<p>Run the prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>finetune_dir</code> <code>str</code> <p>The directory with the previous step</p> required <code>test_length</code> <code>int</code> <p>The length of the test step.</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>the outputs of the model.</p> Source code in <code>bionemo/example_model/training_scripts/predict_mnist.py</code> <pre><code>def run_predict(finetune_dir: str, test_length: int):\n    \"\"\"Run the prediction step.\n\n    Args:\n        finetune_dir: The directory with the previous step\n        test_length: The length of the test step.\n\n    Returns:\n        tensor: the outputs of the model.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    test_run_trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        num_nodes=1,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    lightning_module3 = BionemoLightningModule(config=ExampleFineTuneConfig(initial_ckpt_path=finetune_dir))\n    new_data_module = MNISTDataModule(data_dir=str(BIONEMO_CACHE_DIR), batch_size=test_length, output_log=False)\n\n    results = test_run_trainer.predict(lightning_module3, datamodule=new_data_module)\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/example_model/training_scripts/pretrain_mnist/","title":"Pretrain mnist","text":""},{"location":"API_reference/bionemo/example_model/training_scripts/pretrain_mnist/#bionemo.example_model.training_scripts.pretrain_mnist.run_pretrain","title":"<code>run_pretrain(name, directory_name)</code>","text":"<p>Run the pretraining step.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The experiment name.</p> required <code>directory_name</code> <code>str</code> <p>The directory to write the output</p> required <p>Returns:     str: the path of the trained model.</p> Source code in <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code> <pre><code>def run_pretrain(name: str, directory_name: str):\n    \"\"\"Run the pretraining step.\n\n    Args:\n        name: The experiment name.\n        directory_name: The directory to write the output\n    Returns:\n        str: the path of the trained model.\n    \"\"\"\n    # Setup the logger train the model\n    save_dir = Path(directory_name) / \"pretrain\"\n\n    nemo_logger = NeMoLogger(\n        log_dir=str(save_dir),\n        name=name,\n        tensorboard=TensorBoardLogger(save_dir=directory_name, name=name),\n        ckpt=checkpoint_callback,\n        extra_loggers=[CSVLogger(save_dir / \"logs\", name=name)],\n    )\n\n    # Set up the training module\n    lightning_module = BionemoLightningModule(config=PretrainConfig())\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        always_save_context=True,\n    )\n\n    trainer = nl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        strategy=strategy,\n        limit_val_batches=5,\n        val_check_interval=5,\n        max_steps=100,\n        max_epochs=10,\n        num_nodes=1,\n        log_every_n_steps=5,\n        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n    )\n\n    # This trains the model\n    llm.train(\n        model=lightning_module,\n        data=data_module,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n    return Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/","title":"Api","text":""},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.BERTMLMLossWithReductionNoForward","title":"<code>BERTMLMLossWithReductionNoForward</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> Source code in <code>bionemo/geneformer/api.py</code> <pre><code>class BERTMLMLossWithReductionNoForward(BERTMLMLossWithReduction):\n    def __init__(\n        self,\n        validation_step: bool = False,\n        val_drop_last: bool = True,\n        send_train_output: bool = False,\n        send_val_output: bool = False,\n    ) -&gt; None:\n        \"\"\"Same as BERTMLMLossWithReduction but set send_val_output=False by default since we do not use perplexity.\"\"\"\n        super().__init__(validation_step, val_drop_last, send_train_output, send_val_output)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.BERTMLMLossWithReductionNoForward.__init__","title":"<code>__init__(validation_step=False, val_drop_last=True, send_train_output=False, send_val_output=False)</code>","text":"<p>Same as BERTMLMLossWithReduction but set send_val_output=False by default since we do not use perplexity.</p> Source code in <code>bionemo/geneformer/api.py</code> <pre><code>def __init__(\n    self,\n    validation_step: bool = False,\n    val_drop_last: bool = True,\n    send_train_output: bool = False,\n    send_val_output: bool = False,\n) -&gt; None:\n    \"\"\"Same as BERTMLMLossWithReduction but set send_val_output=False by default since we do not use perplexity.\"\"\"\n    super().__init__(validation_step, val_drop_last, send_train_output, send_val_output)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.FineTuneSeqLenBioBertConfig","title":"<code>FineTuneSeqLenBioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>BioBert fine-tuning sequence length model configuration.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>@dataclass\nclass FineTuneSeqLenBioBertConfig(\n    BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction],\n    iom.IOMixinWithGettersSetters,\n):\n    \"\"\"BioBert fine-tuning sequence length model configuration.\"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[MegatronBioBertFineTuneSeqLengthModel] = MegatronBioBertFineTuneSeqLengthModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n        \"\"\"Loss function type.\"\"\"\n        return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.FineTuneSeqLenBioBertConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Loss function type.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n    \"\"\"Loss function type.\"\"\"\n    return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/api/#bionemo.geneformer.api.GeneformerConfig","title":"<code>GeneformerConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[GeneformerModel, MegatronLossType]</code>, <code>IOMixinWithGettersSetters</code></p> <p>A geneformer config.</p> <p>The geneformer config overrides the parent config, and adds a leaf-level iomixin, please do not inherit from this directly, as your parameters will likely be reset to this method's parameters silently.</p> Source code in <code>bionemo/geneformer/api.py</code> <pre><code>@dataclass\nclass GeneformerConfig(BioBertConfig[GeneformerModel, MegatronLossType], iom.IOMixinWithGettersSetters):\n    \"\"\"A geneformer config.\n\n    The geneformer config overrides the parent config, and adds a leaf-level iomixin, please do not inherit from this\n    directly, as your parameters will likely be reset to this method's parameters silently.\n    \"\"\"\n\n    num_layers: int = 6\n    hidden_size: int = 256\n    ffn_hidden_size: int = 512\n    num_attention_heads: int = 4\n    seq_length: int = 2048\n    fp32_residual_connection: bool = False\n    # Dropout\n    attention_dropout: float = 0.1  # NeMo1 hard-coded, differs from publication of ReLU\n    hidden_dropout: float = 0.02\n    init_method_std: float = 0.02\n    apply_query_key_layer_scaling: bool = False\n    make_vocab_size_divisible_by: int = 128\n    fp16_lm_cross_entropy: bool = False\n    layernorm_zero_centered_gamma: bool = False\n    layernorm_epsilon: float = 1.0e-12\n    activation_func: Callable = F.gelu  # NeMo1 hard-coded, differes from publication of ReLU\n    qk_layernorm: bool = False\n    apply_residual_connection_post_layernorm: bool = False  # False is new default, True was BERT pub.\n    share_embeddings_and_output_weights: bool = True\n    # FUSION SETTINGS\n    parallel_output: bool = True\n    bias_dropout_fusion: bool = True\n    bias_activation_fusion: bool = True\n    masked_softmax_fusion: bool = True\n    persist_layer_norm: bool = True\n    get_attention_mask_from_fusion: bool = True\n\n    position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\"\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n    qk_layernorm: bool = False\n\n    enable_autocast: bool = False\n    model_cls: Type[GeneformerModel] = GeneformerModel\n    loss_reduction_class: Type[MegatronLossType] = BERTMLMLossWithReductionNoForward\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/","title":"Preprocess","text":""},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor","title":"<code>ResourcePreprocessor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface defining a ResourcePreprocessor. Implementors promise to provide both a complete RemoteResource and a freeform preprocess method. This interface can be used to generically define a workflow from a config file.</p> <pre><code>remote -&gt; prepare -&gt; prepared data.\n</code></pre> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@dataclass\nclass ResourcePreprocessor(ABC):\n    \"\"\"Interface defining a ResourcePreprocessor. Implementors promise to provide both a complete RemoteResource and a freeform\n    preprocess method. This interface can be used to generically define a workflow from a config file.\n\n        remote -&gt; prepare -&gt; prepared data.\n    \"\"\"  # noqa: D205\n\n    root_directory: Optional[str] = field(default_factory=RemoteResource.get_env_tmpdir)\n    dest_directory: str = \"data\"\n\n    def get_checksums(self) -&gt; List[str]:  # noqa: D102\n        return [resource.checksum for resource in self.get_remote_resources()]\n\n    def get_urls(self) -&gt; List[str]:  # noqa: D102\n        return [resource.url for resource in self.get_remote_resources()]\n\n    @abstractmethod\n    def get_remote_resources(self) -&gt; List[RemoteResource]:\n        \"\"\"Gets the remote resources associated with this preparor.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def prepare(self) -&gt; List:\n        \"\"\"Returns a list of prepared filenames.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor.get_remote_resources","title":"<code>get_remote_resources()</code>  <code>abstractmethod</code>","text":"<p>Gets the remote resources associated with this preparor.</p> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@abstractmethod\ndef get_remote_resources(self) -&gt; List[RemoteResource]:\n    \"\"\"Gets the remote resources associated with this preparor.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/preprocess/#bionemo.geneformer.data.preprocess.ResourcePreprocessor.prepare","title":"<code>prepare()</code>  <code>abstractmethod</code>","text":"<p>Returns a list of prepared filenames.</p> Source code in <code>bionemo/geneformer/data/preprocess.py</code> <pre><code>@abstractmethod\ndef prepare(self) -&gt; List:\n    \"\"\"Returns a list of prepared filenames.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/datamodule/#bionemo.geneformer.data.singlecell.datamodule.SingleCellDataModule","title":"<code>SingleCellDataModule</code>","text":"<p>               Bases: <code>MegatronDataModule</code></p> <p>LightningDataModule wrapper of <code>SingleCellDataset</code></p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, PosixPath]</code> <p>Path to preprocessed single-cell data files</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Maps gene names to ids and vice-versa</p> required <code>collator</code> <p>Used to batch samples</p> required <code>process_item</code> <p>Function defining how each item should be processed</p> required <code>num_workers</code> <code>int</code> <p>Number of workers to use</p> <code>10</code> <code>num_mask_per_sample</code> <code>int</code> <p>Number of masked versions of a single sample to be returned by each worker</p> required <code>train_batch_size</code> <code>int</code> <p>Batch size for training</p> required <code>val_batch_size</code> <code>int</code> <p>Batch size for validation</p> required <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Config</code> <p>Configuration object</p> <code>data_path</code> <code>Union[str, PosixPath]</code> <p>Path to preprocessed single-cell data files</p> <code>median_dict</code> <code>dict</code> <p>Dictionary containing median values</p> <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object</p> <code>setup_called</code> <code>bool</code> <p>Flag indicating if the setup method has been called</p> <code>dataset</code> <code>SingleCellDataset</code> <p>Single-cell dataset object</p> Source code in <code>bionemo/geneformer/data/singlecell/datamodule.py</code> <pre><code>class SingleCellDataModule(MegatronDataModule):\n    \"\"\"LightningDataModule wrapper of `SingleCellDataset`\n\n    Args:\n        data_path (Union[str, PosixPath]): Path to preprocessed single-cell data files\n        tokenizer (Tokenizer): Maps gene names to ids and vice-versa\n        collator: Used to batch samples\n        process_item: Function defining how each item should be processed\n        num_workers (int): Number of workers to use\n        num_mask_per_sample (int): Number of masked versions of a single sample to be returned by each worker\n        train_batch_size (int): Batch size for training\n        val_batch_size (int): Batch size for validation\n\n    Attributes:\n        cfg (Config): Configuration object\n        data_path (Union[str, PosixPath]): Path to preprocessed single-cell data files\n        median_dict (dict): Dictionary containing median values\n        tokenizer (Tokenizer): Tokenizer object\n        setup_called (bool): Flag indicating if the setup method has been called\n        dataset (SingleCellDataset): Single-cell dataset object\n\n    \"\"\"  # noqa: D415\n\n    # Nothing says we cant pass in the dataset...\n    def __init__(  # noqa: D107\n        self,\n        tokenizer: Tokenizer,\n        median_dict: dict[str, float],\n        train_dataset_path: str | Path | None = None,\n        val_dataset_path: str | Path | None = None,\n        test_dataset_path: str | Path | None = None,\n        predict_dataset_path: str | Path | None = None,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,  # 80% mask token\n        random_token_prob: float = 0.1,  # 10% random token, remaining 1-(mask+random) will be identity.\n        seq_length: int = 2048,\n        micro_batch_size: int = 4,\n        global_batch_size: int = 8,\n        rampup_batch_size: Optional[List[int]] = None,\n        seed: int = 42,\n        num_workers: int = 10,  # TODO can this be automatically set?\n        persistent_workers: bool = True,\n        pin_memory: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        if predict_dataset_path is None:\n            assert (\n                train_dataset_path is not None and val_dataset_path is not None and test_dataset_path is not None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n        elif train_dataset_path is None:\n            assert (\n                val_dataset_path is None and test_dataset_path is None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n            assert (\n                predict_dataset_path is not None\n            ), \"Provide either predict_dataset_path or (train_dataset_path, val_dataset_path, and test_dataset_path)\"\n        self.data_path_predict = predict_dataset_path\n        self.data_path_train = train_dataset_path\n        self.data_path_val = val_dataset_path\n        self.data_path_test = test_dataset_path\n        self.tokenizer = tokenizer\n        self.median_dict = median_dict\n        self.max_len = seq_length\n        self.mask_prob = mask_prob\n        self.mask_token_prob = mask_token_prob\n        self.random_token_prob = random_token_prob\n        self.seed = seed\n        self.num_workers = num_workers\n        self.persistent_workers = persistent_workers\n        self.pin_memory = pin_memory\n\n        rng = np.random.default_rng(seed)\n        if self.data_path_train is not None:\n            assert self.data_path_val is not None and self.data_path_test is not None\n            self._train_dataset_ori = SingleCellDataset(\n                self.data_path_train,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n            )\n            self._val_dataset_ori = SingleCellDataset(\n                self.data_path_val,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n            )\n            self._test_dataset_ori = SingleCellDataset(\n                self.data_path_test,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n            )\n            self._predict_dataset_ori = None\n        else:\n            assert self.data_path_predict is not None\n            self._predict_dataset_ori = SingleCellDataset(\n                self.data_path_predict,\n                self.tokenizer,\n                self.median_dict,\n                self.max_len,\n                mask_prob=self.mask_prob,\n                mask_token_prob=self.mask_token_prob,\n                random_token_prob=self.random_token_prob,\n                seed=random_utils.get_seed_from_rng(rng),\n            )\n            self._train_dataset_ori = None\n            self._val_dataset_ori = None\n            self._test_dataset_ori = None\n\n        # This is needed here, or you need to specify it in the megatron adapter thing TODO name?\n        #  Note that this sampler is sequential, meaning it does not do any shuffling. Let's wrap our data in a shuffler.\n        if self.data_path_predict is not None:\n            n_predict = len(self._predict_dataset_ori)\n            self.data_sampler = MegatronDataSampler(\n                seq_len=self.max_len,\n                micro_batch_size=min(micro_batch_size, n_predict),\n                global_batch_size=min(global_batch_size, n_predict),\n                rampup_batch_size=rampup_batch_size,\n                output_log=False,  # this is needed for predict step to work\n            )\n        else:\n            self.data_sampler = MegatronDataSampler(\n                seq_len=self.max_len,\n                micro_batch_size=micro_batch_size,\n                global_batch_size=global_batch_size,\n                rampup_batch_size=rampup_batch_size,\n            )\n\n    def setup(self, stage: str = \"\") -&gt; None:  # noqa: D102\n        assert getattr(self, \"trainer\", None) is not None, \"Please only call setup after trainer is attached.\"\n\n        if self._train_dataset_ori is not None:\n            assert self._val_dataset_ori is not None and self._test_dataset_ori is not None\n            # Trainer API\n            max_train_steps = self.trainer.max_steps\n            if self.trainer.max_epochs &gt; 1:\n                logging.warning(\n                    \"Trainer is set to run for multiple epochs. This is not recommended due to the same shuffle being used in each. Instead set max_epochs to 1 and increase the number of max_steps.\"\n                )\n            assert max_train_steps &gt; 0, \"Please specify trainer.max_steps\"\n\n            num_train_samples = int(max_train_steps * self.data_sampler.global_batch_size)\n            num_val_samples = infer_num_samples(\n                limit_batches=self.trainer.limit_val_batches,\n                num_samples_in_dataset=len(self._val_dataset_ori),\n                global_batch_size=self.data_sampler.global_batch_size,\n                stage=\"val\",\n            )\n            num_test_samples = infer_num_samples(\n                limit_batches=self.trainer.limit_test_batches,\n                num_samples_in_dataset=len(self._test_dataset_ori),\n                global_batch_size=self.data_sampler.global_batch_size,\n                stage=\"test\",\n            )\n\n            # This happens exactly once during setup.\n            self._train_ds = MultiEpochDatasetResampler(\n                self._train_dataset_ori,\n                num_samples=num_train_samples,\n                shuffle=True,\n                seed=self.seed,\n            )\n            self._validation_ds = MultiEpochDatasetResampler(\n                self._val_dataset_ori,\n                num_samples=num_val_samples,\n                shuffle=False,\n                seed=self.seed,\n            )\n            self._test_ds = MultiEpochDatasetResampler(\n                self._test_dataset_ori,\n                num_samples=num_test_samples,\n                shuffle=False,\n                seed=self.seed,\n            )\n        else:\n            assert self._predict_dataset_ori is not None\n            self._predict_ds = MultiEpochDatasetResampler(\n                self._predict_dataset_ori,\n                shuffle=False,\n                seed=self.seed,\n            )\n\n    def train_dataloader(self) -&gt; TRAIN_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._train_ds, mode=\"train\")\n\n    def val_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._validation_ds, mode=\"validation\")\n\n    def test_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._test_ds, mode=\"test\")\n\n    def predict_dataloader(self) -&gt; EVAL_DATALOADERS:  # noqa: D102\n        return self._create_dataloader(self._predict_ds, mode=\"predict\", drop_last=False)\n\n    def _create_dataloader(self, dataset, mode: Mode, **kwargs) -&gt; WrappedDataLoader:\n        \"\"\"Create dataloader for train, validation, and test stages.\n\n        Args:\n            dataset: The dataset to create the dataloader for.\n            mode: Stage of training, which is used to determined if consumed_samples in MegatronPretrainingSampler should be initialized to 0 (validation/test), or be set to the previous value from state_dict in case of checkpoint resumption (train).\n            **kwargs: Additional arguments to pass to the dataloader.\n        \"\"\"\n        self.update_init_global_step()\n        return WrappedDataLoader(\n            mode=mode,\n            dataset=dataset,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            persistent_workers=self.persistent_workers,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=self.tokenizer.token_to_id(GeneTokenizer.pad_token),\n                min_length=self.max_len,\n                max_length=self.max_len,\n            ),\n            **kwargs,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/","title":"Dataset","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.SingleCellDataset","title":"<code>SingleCellDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset class for single-cell pre-training. These can be generated using the sc_memmap.py script. Future updates will contain more comprehensive workflows for generating a Sparse Memmap from scRNA-seq.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path where the single cell files are stored. It should contain the following files: - <code>metadata.json</code>: Path containing feature subset associated with each dataset. - <code>features.csv</code>: Feature subset associated with each sample. - Gene expression matrix stored in CSR format as <code>numpy.memmap</code>:     - <code>gene_expression_data.npy</code>: Gene expression values.     - <code>gene_expression_ind.npy</code>: Gene indices associated with gene values.     - <code>gene_expression_ptr.npy</code>: Column indices for each sample.</p> required <code>tokenizer</code> <code>Any</code> <p>The tokenizer to use for tokenizing the input data.</p> required <code>median_dict</code> <code>dict</code> <p>A dictionary containing median values for each gene. Defaults to None.</p> <code>None</code> <code>max_len</code> <code>int</code> <p>The maximum length of the input sequence. Defaults to 1024.</p> <code>1024</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Path where the single cell files are stored.</p> <code>max_len</code> <code>int</code> <p>The maximum length of the input sequence.</p> <code>metadata</code> <code>dict</code> <p>Metadata loaded from <code>metadata.json</code>.</p> <code>gene_medians</code> <code>dict</code> <p>A dictionary containing median values for each gene. If None, a median of '1' is assumed for all genes.</p> <code>num_train</code> <code>int</code> <p>The number of samples in the training split.</p> <code>num_val</code> <code>int</code> <p>The number of samples in the validation split.</p> <code>num_test</code> <code>int</code> <p>The number of samples in the test split.</p> <code>index_offset</code> <code>int</code> <p>The offset to apply to the indices.</p> <code>length</code> <code>int</code> <p>The total number of samples in the dataset.</p> <code>gene_data</code> <code>memmap</code> <p>Gene expression values stored in CSR format.</p> <code>gene_data_indices</code> <code>memmap</code> <p>Gene indices associated with gene values.</p> <code>gene_data_ptr</code> <code>memmap</code> <p>Column indices for each sample.</p> <code>tokenizer</code> <p>The tokenizer used for tokenizing the input data.</p> <code>dataset_ccum</code> <code>ndarray</code> <p>Cumulative sum of row counts to map row indices to dataset id.</p> <code>dataset_map</code> <code>dict</code> <p>Mapping of dataset id to dataset name.</p> <p>Methods:</p> Name Description <code>__len__</code> <p>Returns the length of the dataset.</p> <code>__getitem__</code> <p>Returns the item at the given index.</p> See Also <p>bionemo/data/singlecell/sc_memmap.py - creates the artifacts required for instantiating a singlecell dataset from hdf5 files.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>class SingleCellDataset(Dataset):\n    \"\"\"A dataset class for single-cell pre-training. These can be generated using the sc_memmap.py script. Future\n    updates will contain more comprehensive workflows for generating a Sparse Memmap from scRNA-seq.\n\n    Args:\n        data_path (str): Path where the single cell files are stored. It should contain the following files:\n            - `metadata.json`: Path containing feature subset associated with each dataset.\n            - `features.csv`: Feature subset associated with each sample.\n            - Gene expression matrix stored in CSR format as `numpy.memmap`:\n                - `gene_expression_data.npy`: Gene expression values.\n                - `gene_expression_ind.npy`: Gene indices associated with gene values.\n                - `gene_expression_ptr.npy`: Column indices for each sample.\n        tokenizer: The tokenizer to use for tokenizing the input data.\n        median_dict (dict, optional): A dictionary containing median values for each gene. Defaults to None.\n        max_len (int, optional): The maximum length of the input sequence. Defaults to 1024.\n\n    Attributes:\n        data_path (str): Path where the single cell files are stored.\n        max_len (int): The maximum length of the input sequence.\n        metadata (dict): Metadata loaded from `metadata.json`.\n        gene_medians (dict): A dictionary containing median values for each gene. If None, a median of '1' is assumed for all genes.\n        num_train (int): The number of samples in the training split.\n        num_val (int): The number of samples in the validation split.\n        num_test (int): The number of samples in the test split.\n        index_offset (int): The offset to apply to the indices.\n        length (int): The total number of samples in the dataset.\n        gene_data (numpy.memmap): Gene expression values stored in CSR format.\n        gene_data_indices (numpy.memmap): Gene indices associated with gene values.\n        gene_data_ptr (numpy.memmap): Column indices for each sample.\n        tokenizer: The tokenizer used for tokenizing the input data.\n        dataset_ccum (numpy.ndarray): Cumulative sum of row counts to map row indices to dataset id.\n        dataset_map (dict): Mapping of dataset id to dataset name.\n\n    Methods:\n        __len__(): Returns the length of the dataset.\n        __getitem__(idx): Returns the item at the given index.\n\n    See Also:\n        bionemo/data/singlecell/sc_memmap.py - creates the artifacts required for instantiating a singlecell dataset from hdf5 files.\n    \"\"\"  # noqa: D205\n\n    def __init__(  # noqa: D107\n        self,\n        data_path: str | Path,\n        tokenizer: Any,\n        median_dict: Optional[dict] = None,\n        max_len: int = 1024,\n        mask_prob: float = 0.15,\n        mask_token_prob: float = 0.8,\n        random_token_prob: float = 0.1,\n        prepend_cls_token: bool = True,\n        eos_token: int | None = None,\n        assert_increasing_columns: bool = True,\n        seed: int = np.random.SeedSequence().entropy,  # type: ignore\n    ):\n        super().__init__()\n        self.data_path = data_path\n        self.max_len = max_len\n        self.random_token_prob = random_token_prob\n        self.mask_token_prob = mask_token_prob\n        self.mask_prob = mask_prob\n        self.prepend_cls_token = prepend_cls_token\n        self._seed = seed\n        self.eos_token = eos_token\n        # check if column indices are increasing for looking up genes. This is a way of spotting if the sc_memmap.py\n        #  script produced properly strctured sparse files.\n        self.assert_increasing_columns = assert_increasing_columns\n        path = Path(data_path)\n\n        # - metadata\n        metadata = json.load(open(path / \"metadata.json\", \"r\"))\n\n        # - median dict\n        self.gene_medians = median_dict\n\n        # - train/val idxs sampled contiguously\n        total_el = sum([v[\"num_el\"] for _, v in metadata.items()])\n        self.num_samples = sum([v[\"shape\"][0] for _, v in metadata.items()])\n        # - load data\n        self.gene_data = np.memmap(path / \"gene_expression_data.npy\", dtype=\"float32\", mode=\"r\", shape=(total_el,))\n\n        self.gene_data_indices = np.memmap(\n            path / \"gene_expression_ind.npy\", dtype=\"int32\", mode=\"r\", shape=(total_el,)\n        )\n\n        self.gene_data_ptr = np.memmap(\n            path / \"gene_expression_ptr.npy\", dtype=\"int64\", mode=\"r\", shape=(self.num_samples + 1,)\n        )\n        self.tokenizer = tokenizer\n        rnd_key = next(iter(metadata))\n        feature_ids = np.array(metadata[rnd_key][\"feature_ids\"])\n\n        # Determine if we need to store the full metadata (per file feature_ids) or just a single feature_id\n        #  vector for all files. If we can do the later this is much more memory efficient.\n        #  without this change, if num_workers&gt;0, we seem to hit a memory leak after a relatively small number\n        #  of steps. Online discussion points to native python objects like dictionaries of a lot of data\n        #  being a primary culprit behind large RAM usage in dataloaders that use multiprocessing.\n        features_all_same = True\n        for m in metadata.values():\n            if np.any(np.char.not_equal(np.array(m[\"feature_ids\"]), feature_ids)):\n                features_all_same = False\n                break\n\n        if not features_all_same:\n            # We need to store per-file metadata of feature_ids. Make sure you run with a lot of RAM or few dataset workers.\n            #  we need to store per-file metadata in this case because some of the files have different subsets of the\n            #  feature_ids.\n            logging.warning(\n                \"Feature ids are not the same across datasets. This can cause heavy RAM usage \"\n                \"for large datasets, try setting num_workers to 0.\"\n            )\n            self.metadata = metadata\n            self.feature_ids = None\n\n            # map row indices to dataset id\n            self.dataset_ccum = np.zeros(\n                len(self.metadata),\n            )\n            # Maps dataset ids to dataset names (used in the metadata dict)\n            self.dataset_map = {}\n            count = 0\n            for i, k in enumerate(self.metadata):\n                self.dataset_ccum[i] = count\n                self.dataset_map[i] = k\n                count += self.metadata[k][\"shape\"][0]\n            self.dataset_ccum[0] = -1\n        else:\n            # We can store a single feature_id vector for all datasets, and do not need to store the full metadata array.\n            logging.warning(\n                \"Feature ids are the same across datasets. This is good, using the same feature_ids for all datasets.\"\n            )\n            self.feature_ids = feature_ids\n            self.metadata = None\n\n    def __len__(self):  # noqa: D105\n        return self.num_samples\n\n    def metadata_lookup(self, idx) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Go from a cell idx to the file-level metadata associated with that cell.\"\"\"\n        did = sum(~(self.dataset_ccum &gt; idx)) - 1\n        metadata = self.metadata[self.dataset_map[did]]\n        return metadata\n\n    def lookup_cell_by_idx(self, idx) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:  # noqa: D102\n        ptr = slice(int(self.gene_data_ptr[idx]), int(self.gene_data_ptr[idx + 1]))\n        # col idxs poin to offsets in the original sparse metadata, this is for looking up metadata eg gene names\n        col_idxs = np.asarray(self.gene_data_indices[ptr]).astype(int)  # keyed by ptr\n        if self.assert_increasing_columns and len(col_idxs) &gt; 1:\n            is_increasing = np.diff(col_idxs) &gt; 0\n            if not np.all(is_increasing):\n                raise ValueError(f\"Column indices are not increasing for {np.sum(~is_increasing)} pairs of genes\")\n        gene_data = np.asarray(self.gene_data[ptr]).astype(int)  # keyed by ptr\n        # Get feature_ids for this particular cell. Eitehr lookup by index if we need to, or if we already verified that\n        #  metadata is not needed because feature_ids are the same for every file, then we can just use the single feature_ids\n        #  vector instead.\n        feature_ids: np.ndarray = (\n            self.feature_ids if self.metadata is None else self.metadata_lookup(idx)[\"feature_ids\"]\n        )\n        return gene_data, col_idxs, feature_ids\n\n    def __getitem__(self, index: EpochIndex) -&gt; types.BertSample:\n        \"\"\"Performs a lookup and the required transformation for the model.\"\"\"\n        rng = np.random.default_rng([self._seed, index.epoch, index.idx])\n        gene_data, col_idxs, feature_ids = self.lookup_cell_by_idx(index.idx)\n        return process_item(\n            gene_data,\n            col_idxs,\n            feature_ids,\n            self.tokenizer,\n            gene_median=self.gene_medians,\n            rng=rng,\n            max_len=self.max_len,\n            mask_token_prob=self.mask_token_prob,\n            mask_prob=self.mask_prob,\n            random_token_prob=self.random_token_prob,\n            prepend_cls_token=self.prepend_cls_token,\n            eos_token=self.eos_token,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.SingleCellDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Performs a lookup and the required transformation for the model.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def __getitem__(self, index: EpochIndex) -&gt; types.BertSample:\n    \"\"\"Performs a lookup and the required transformation for the model.\"\"\"\n    rng = np.random.default_rng([self._seed, index.epoch, index.idx])\n    gene_data, col_idxs, feature_ids = self.lookup_cell_by_idx(index.idx)\n    return process_item(\n        gene_data,\n        col_idxs,\n        feature_ids,\n        self.tokenizer,\n        gene_median=self.gene_medians,\n        rng=rng,\n        max_len=self.max_len,\n        mask_token_prob=self.mask_token_prob,\n        mask_prob=self.mask_prob,\n        random_token_prob=self.random_token_prob,\n        prepend_cls_token=self.prepend_cls_token,\n        eos_token=self.eos_token,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.SingleCellDataset.metadata_lookup","title":"<code>metadata_lookup(idx)</code>","text":"<p>Go from a cell idx to the file-level metadata associated with that cell.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def metadata_lookup(self, idx) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Go from a cell idx to the file-level metadata associated with that cell.\"\"\"\n    did = sum(~(self.dataset_ccum &gt; idx)) - 1\n    metadata = self.metadata[self.dataset_map[did]]\n    return metadata\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/dataset/#bionemo.geneformer.data.singlecell.dataset.process_item","title":"<code>process_item(gene_data, gene_idxs, feature_ids, tokenizer, gene_median, rng, max_len=1024, mask_prob=0.15, mask_token_prob=0.8, random_token_prob=0.1, target_sum=10000, normalize=True, prepend_cls_token=True, eos_token=None)</code>","text":"<p>Process a single item in the dataset.</p> <p>Optionally performs median normalization and rank ordering. The tokenizers CLS token is added to the beginning of every sample. Converts gene names to ensemble ids before tokenizing. Expects gene_medians to contain ensembl ids as keys.</p> <p>Parameters:</p> Name Type Description Default <code>gene_data</code> <code>list</code> <p>List of gene data, these are expression counts.</p> required <code>gene_idxs</code> <code>list</code> <p>List of gene indices, these are keys in 'metadata['feature_ids']' and correspdong the CSR entry. These are computed by sc_memmap.</p> required <code>feature_ids</code> <code>list</code> <p>Feature ids for the full dataset.</p> required <code>tokenizer</code> <code>Tokenizer</code> <p>Tokenizer object.</p> required <code>gene_median</code> <code>optional(dict</code> <p>Dictionary of gene medians. Defaults to None. Expects ensembl IDs to be keys.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator to ensure deterministic results.</p> required <code>max_len</code> <code>int</code> <p>Maximum length of the item. Defaults to 1024. Applies padding to any sequence shorter than max_len and truncates any sequence longer than max_len.</p> <code>1024</code> <code>mask_prob</code> <code>float</code> <p>Probability of masking a token. Defaults to 0.15.</p> <code>0.15</code> <code>target_sum</code> <code>int</code> <p>Target sum for normalization. Defaults to 10000.</p> <code>10000</code> <code>normalize</code> <code>bool</code> <p>Flag to normalize the gene data. Defaults to True. When set, this re-orders the gene tokens by their median expression value.</p> <code>True</code> <code>probabilistic_dirichlet_sampling</code> <code>bool</code> <p>Flag to enable probabilistic dirichlet sampling. Defaults to False.</p> required <code>dirichlet_alpha</code> <code>float</code> <p>Alpha value for dirichlet sampling if set by <code>probabilistic_dirichlet_sampling</code>. Defaults to 0.5.</p> required <code>same_length</code> <code>bool</code> <p>when true, sample the same length of genes as you originally had before the dirichlet sampler.</p> required <code>recompute_globals</code> <code>bool</code> <p>when true, global arrays are always recomputed. this is only useful for testing.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>BertSample</code> <p>Processed item dictionary.</p> this method is very important and very useful. To generalize thiswwe should add an abstraction for <p>Datasets that have some kind of functor transformation.</p> Source code in <code>bionemo/geneformer/data/singlecell/dataset.py</code> <pre><code>def process_item(  # noqa: D417\n    gene_data: np.ndarray,\n    gene_idxs: np.ndarray,\n    feature_ids: np.ndarray,\n    tokenizer: GeneTokenizer,\n    gene_median: dict,\n    rng: np.random.Generator,\n    max_len: int = 1024,\n    mask_prob: float = 0.15,\n    mask_token_prob: float = 0.8,\n    random_token_prob: float = 0.1,\n    target_sum: int = 10000,\n    normalize: bool = True,\n    prepend_cls_token: bool = True,\n    eos_token: None | int = None,\n) -&gt; types.BertSample:\n    \"\"\"Process a single item in the dataset.\n\n    Optionally performs median normalization and rank ordering. The tokenizers CLS token is added to the beginning\n    of every sample. Converts gene names to ensemble ids before tokenizing. Expects gene_medians to contain ensembl ids as keys.\n\n    Args:\n        gene_data (list): List of gene data, these are expression counts.\n        gene_idxs (list): List of gene indices, these are keys in 'metadata['feature_ids']' and correspdong the CSR entry. These are computed by sc_memmap.\n        feature_ids (list): Feature ids for the full dataset.\n        tokenizer (Tokenizer): Tokenizer object.\n        gene_median (optional(dict)): Dictionary of gene medians. Defaults to None. Expects ensembl IDs to be keys.\n        rng: Random number generator to ensure deterministic results.\n        max_len (int): Maximum length of the item. Defaults to 1024. Applies padding to any sequence shorter than max_len and truncates any sequence longer than max_len.\n        mask_prob (float): Probability of masking a token. Defaults to 0.15.\n        target_sum (int): Target sum for normalization. Defaults to 10000.\n        normalize (bool): Flag to normalize the gene data. Defaults to True.\n            When set, this re-orders the gene tokens by their median expression value.\n        probabilistic_dirichlet_sampling (bool): Flag to enable probabilistic dirichlet sampling. Defaults to False.\n        dirichlet_alpha (float): Alpha value for dirichlet sampling if set by `probabilistic_dirichlet_sampling`. Defaults to 0.5.\n        same_length (bool): when true, sample the same length of genes as you originally had before the dirichlet sampler.\n        recompute_globals (bool): when true, global arrays are always recomputed. this is only useful for testing.\n\n    Returns:\n        dict: Processed item dictionary.\n\n    NOTE: this method is very important and very useful. To generalize thiswwe should add an abstraction for\n        Datasets that have some kind of functor transformation.\n    \"\"\"\n    if max_len &lt; 1:\n        raise ValueError(f\"max_len must be greater than 1, {max_len=}\")\n\n    if gene_median is None:\n        raise ValueError(\"gene_median must be provided for this tokenizer\")\n\n    if prepend_cls_token:\n        max_len = max_len - 1  # - minus 1 for [CLS] token\n    if eos_token is not None:\n        max_len = max_len - 1  # - minus 1 for [EOS] token\n\n    gene_names = [feature_ids[idx] for idx in gene_idxs]\n    genes, tokens, medians = [], [], []\n    for tok, gene in zip(gene_names, gene_data):\n        if tok in tokenizer.vocab:\n            tokens.append(tokenizer.token_to_id(tok))\n            genes.append(gene)\n            if normalize:\n                med = gene_median.get(tok, 1)  # If not in the dictionary we default to no normalization (1)\n                medians.append(med)\n\n    genes = np.asarray(genes)\n    token_ids = np.asarray(tokens)\n    medians = np.asarray(medians)\n\n    if normalize:\n        # re-order according to expression median normalized rank. descending order.\n\n        genes = genes / genes.sum() * target_sum\n        genes = genes / medians.astype(float)\n        idxs = np.argsort(-genes)  # sort in descending order so that the 0th position is the highest value.\n        genes = genes[idxs]\n        token_ids = token_ids[idxs]\n\n    # - select max_len subset, set sample to false so it doesnt permute the already rank ordered expression values.\n    token_ids = sample_or_truncate(token_ids, max_len, sample=False)\n    with torch.no_grad(), torch.device(\"cpu\"):\n        masked_tokens, labels, loss_mask = masking.apply_bert_pretraining_mask(\n            tokenized_sequence=torch.from_numpy(token_ids),\n            random_seed=int(random_utils.get_seed_from_rng(rng)),\n            mask_config=masking.BertMaskConfig(\n                tokenizer=tokenizer,\n                random_tokens=range(len(tokenizer.special_tokens), len(tokenizer.vocab)),\n                mask_prob=mask_prob,\n                mask_token_prob=mask_token_prob,\n                random_token_prob=random_token_prob,\n            ),\n        )\n        cls_token = tokenizer.token_to_id(tokenizer.cls_token) if prepend_cls_token else None\n        if cls_token is not None or eos_token is not None:\n            masked_tokens, labels, loss_mask = masking.add_cls_and_eos_tokens(\n                sequence=masked_tokens,\n                labels=labels,\n                loss_mask=loss_mask,\n                cls_token=cls_token,\n                eos_token=eos_token,\n            )\n\n        # NeMo megatron assumes this return structure.\n        return {\n            \"text\": masked_tokens,\n            \"types\": torch.zeros_like(masked_tokens, dtype=torch.int64),\n            \"attention_mask\": torch.ones_like(masked_tokens, dtype=torch.int64),\n            \"labels\": labels,\n            \"loss_mask\": loss_mask,\n            \"is_random\": torch.zeros_like(masked_tokens, dtype=torch.int64),\n        }\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/","title":"Preprocess","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess","title":"<code>GeneformerPreprocess</code>","text":"Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>class GeneformerPreprocess:  # noqa: D101\n    def __init__(self, download_directory: Path, medians_file_path: Path, tokenizer_vocab_path: Path):\n        \"\"\"Downloads HGNC symbols\n\n        preproc_dir (str): Directory to store the reference preproc in\n        tokenizer_vocab_path (str): Filepath to store the tokenizer vocab\n        dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing\n            the names of preprocessed train/val/test files to use for training.\n        \"\"\"  # noqa: D415\n        self.download_directory = download_directory\n        self.medians_file_path = medians_file_path\n        self.tokenizer_vocab_path = tokenizer_vocab_path\n        self._validate_tokenizer_args(\n            self.tokenizer_vocab_path,\n        )\n\n    def build_and_save_tokenizer(self, median_dict, gene_to_ens, vocab_output_name):\n        \"\"\"Builds the GeneTokenizer using the median dictionary\n        then serializes and saves the dictionary to disk.\n        \"\"\"  # noqa: D205\n        tokenizer = GeneTokenizer.from_medians_and_genes_dicts(median_dict, gene_to_ens)\n        tokenizer.save_vocab(vocab_output_name)\n        return tokenizer\n\n    def _validate_tokenizer_args(self, vocab_output_name):\n        vocab_exists = os.path.exists(vocab_output_name)\n        if vocab_exists:\n            logging.warning(f\"Tokenizer vocab file: {vocab_output_name} already exists. Overwriting...\")\n\n    def preprocess(self) -&gt; dict[Literal[\"tokenizer\", \"median_dict\"], Any]:\n        \"\"\"Preprocesses for the Geneformer model\"\"\"  # noqa: D415\n        gene_name_dict_fn, gene_median_dict_fn = GeneformerResourcePreprocessor(\n            dest_directory=self.download_directory,\n        ).prepare()\n\n        # Load artifacts\n        with open(gene_name_dict_fn, \"rb\") as fd:\n            gene_ens = pickle.load(fd)\n\n        with open(gene_median_dict_fn, \"rb\") as fd:\n            median_dict = pickle.load(fd)\n\n        # Save converted artifacts to JSON to prevent pickle issues.\n        medians_dir = os.path.dirname(self.medians_file_path)\n        if not os.path.exists(medians_dir):\n            os.makedirs(medians_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n        with open(self.medians_file_path, \"w\") as fp:\n            json.dump(median_dict, fp)\n\n        if self.tokenizer_vocab_path is not None:\n            tokenizer = self.build_and_save_tokenizer(\n                median_dict,\n                gene_ens,\n                self.tokenizer_vocab_path,\n            )\n        else:\n            tokenizer = None\n\n        return {\"tokenizer\": tokenizer, \"median_dict\": median_dict}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.__init__","title":"<code>__init__(download_directory, medians_file_path, tokenizer_vocab_path)</code>","text":"<p>Downloads HGNC symbols</p> <p>preproc_dir (str): Directory to store the reference preproc in tokenizer_vocab_path (str): Filepath to store the tokenizer vocab dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing     the names of preprocessed train/val/test files to use for training.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def __init__(self, download_directory: Path, medians_file_path: Path, tokenizer_vocab_path: Path):\n    \"\"\"Downloads HGNC symbols\n\n    preproc_dir (str): Directory to store the reference preproc in\n    tokenizer_vocab_path (str): Filepath to store the tokenizer vocab\n    dataset_conf (OmegaConf): has 'train', 'val', 'test' keys containing\n        the names of preprocessed train/val/test files to use for training.\n    \"\"\"  # noqa: D415\n    self.download_directory = download_directory\n    self.medians_file_path = medians_file_path\n    self.tokenizer_vocab_path = tokenizer_vocab_path\n    self._validate_tokenizer_args(\n        self.tokenizer_vocab_path,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.build_and_save_tokenizer","title":"<code>build_and_save_tokenizer(median_dict, gene_to_ens, vocab_output_name)</code>","text":"<p>Builds the GeneTokenizer using the median dictionary then serializes and saves the dictionary to disk.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def build_and_save_tokenizer(self, median_dict, gene_to_ens, vocab_output_name):\n    \"\"\"Builds the GeneTokenizer using the median dictionary\n    then serializes and saves the dictionary to disk.\n    \"\"\"  # noqa: D205\n    tokenizer = GeneTokenizer.from_medians_and_genes_dicts(median_dict, gene_to_ens)\n    tokenizer.save_vocab(vocab_output_name)\n    return tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerPreprocess.preprocess","title":"<code>preprocess()</code>","text":"<p>Preprocesses for the Geneformer model</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def preprocess(self) -&gt; dict[Literal[\"tokenizer\", \"median_dict\"], Any]:\n    \"\"\"Preprocesses for the Geneformer model\"\"\"  # noqa: D415\n    gene_name_dict_fn, gene_median_dict_fn = GeneformerResourcePreprocessor(\n        dest_directory=self.download_directory,\n    ).prepare()\n\n    # Load artifacts\n    with open(gene_name_dict_fn, \"rb\") as fd:\n        gene_ens = pickle.load(fd)\n\n    with open(gene_median_dict_fn, \"rb\") as fd:\n        median_dict = pickle.load(fd)\n\n    # Save converted artifacts to JSON to prevent pickle issues.\n    medians_dir = os.path.dirname(self.medians_file_path)\n    if not os.path.exists(medians_dir):\n        os.makedirs(medians_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n    with open(self.medians_file_path, \"w\") as fp:\n        json.dump(median_dict, fp)\n\n    if self.tokenizer_vocab_path is not None:\n        tokenizer = self.build_and_save_tokenizer(\n            median_dict,\n            gene_ens,\n            self.tokenizer_vocab_path,\n        )\n    else:\n        tokenizer = None\n\n    return {\"tokenizer\": tokenizer, \"median_dict\": median_dict}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerResourcePreprocessor","title":"<code>GeneformerResourcePreprocessor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ResourcePreprocessor</code></p> <p>ResourcePreprocessor for the Geneformer model. Downloads the gene_name_id_dict.pkl and gene_median_dictionary.pkl files.</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>@dataclass\nclass GeneformerResourcePreprocessor(ResourcePreprocessor):\n    \"\"\"ResourcePreprocessor for the Geneformer model. Downloads the gene_name_id_dict.pkl and gene_median_dictionary.pkl files.\"\"\"\n\n    dest_directory: str = \"geneformer\"\n\n    def get_remote_resources(self) -&gt; List[RemoteResource]:  # noqa: D102\n        url_fn = {\n            \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\": \"gene_name_id_dict.pkl\",\n            \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\": \"gene_median_dictionary.pkl\",\n        }\n\n        resources = []\n        for url, filename in url_fn.items():\n            resource = RemoteResource(\n                dest_directory=self.dest_directory,\n                dest_filename=filename,\n                root_directory=self.root_directory,\n                checksum=None,\n                url=url,\n            )\n            resources.append(resource)\n        return resources\n\n    def prepare_resource(self, resource: RemoteResource) -&gt; str:\n        \"\"\"Logs and downloads the passed resource.\n\n        resource: RemoteResource - Resource to be prepared.\n\n        Returns - the absolute destination path for the downloaded resource\n        \"\"\"\n        return resource.download_resource()\n\n    def prepare(self):  # noqa: D102\n        return [self.prepare_resource(resource) for resource in self.get_remote_resources()]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/preprocess/#bionemo.geneformer.data.singlecell.preprocess.GeneformerResourcePreprocessor.prepare_resource","title":"<code>prepare_resource(resource)</code>","text":"<p>Logs and downloads the passed resource.</p> <p>resource: RemoteResource - Resource to be prepared.</p> <p>Returns - the absolute destination path for the downloaded resource</p> Source code in <code>bionemo/geneformer/data/singlecell/preprocess.py</code> <pre><code>def prepare_resource(self, resource: RemoteResource) -&gt; str:\n    \"\"\"Logs and downloads the passed resource.\n\n    resource: RemoteResource - Resource to be prepared.\n\n    Returns - the absolute destination path for the downloaded resource\n    \"\"\"\n    return resource.download_resource()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/data/singlecell/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/geneformer/data/singlecell/utils/#bionemo.geneformer.data.singlecell.utils.sample_or_truncate","title":"<code>sample_or_truncate(gene_ids, max_length, sample=True)</code>","text":"<p>Truncate and pad samples.</p> <p>Parameters:</p> Name Type Description Default <code>gene_ids</code> <code>ndarray</code> <p>Array of gene IDs.</p> required <code>max_length</code> <code>int</code> <p>Maximum length of the samples.</p> required <code>sample</code> <code>bool</code> <p>Whether to sample or truncate the samples. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.array: Tuple containing the truncated or padded gene IDs.</p> Source code in <code>bionemo/geneformer/data/singlecell/utils.py</code> <pre><code>def sample_or_truncate(\n    gene_ids: np.ndarray,\n    max_length: int,\n    sample: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Truncate and pad samples.\n\n    Args:\n        gene_ids (np.ndarray): Array of gene IDs.\n        max_length (int): Maximum length of the samples.\n        sample (bool, optional): Whether to sample or truncate the samples. Defaults to True.\n\n    Returns:\n        np.array: Tuple containing the truncated or padded gene IDs.\n    \"\"\"\n    if len(gene_ids) &lt;= max_length:\n        return gene_ids\n\n    if sample:\n        indices = np.random.permutation(len(gene_ids))[:max_length]\n        return gene_ids[indices]\n    else:\n        return gene_ids[:max_length]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/","title":"Finetune token regressor","text":""},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.FineTuneSeqLenBioBertConfig","title":"<code>FineTuneSeqLenBioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction]</code>, <code>IOMixinWithGettersSetters</code></p> <p>BioBert fine-tuning sequence length model configuration.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>@dataclass\nclass FineTuneSeqLenBioBertConfig(\n    BioBertConfig[MegatronBioBertFineTuneSeqLengthModel, SequenceLengthRMSEPlusBERTMLMLossWithReduction],\n    iom.IOMixinWithGettersSetters,\n):\n    \"\"\"BioBert fine-tuning sequence length model configuration.\"\"\"\n\n    # When overriding fields in a dataclass _always_ declare types: https://github.com/python/cpython/issues/123269\n    model_cls: Type[MegatronBioBertFineTuneSeqLengthModel] = MegatronBioBertFineTuneSeqLengthModel\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n        \"\"\"Loss function type.\"\"\"\n        return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.FineTuneSeqLenBioBertConfig.get_loss_reduction_class","title":"<code>get_loss_reduction_class()</code>","text":"<p>Loss function type.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def get_loss_reduction_class(self) -&gt; Type[SequenceLengthRMSEPlusBERTMLMLossWithReduction]:\n    \"\"\"Loss function type.\"\"\"\n    return SequenceLengthRMSEPlusBERTMLMLossWithReduction\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor","title":"<code>LoRAForGeneFormerTokenRegressor</code>","text":"<p>               Bases: <code>LoRA</code></p> <p>LoRA for Genformer Token Regression.</p> <p>There are a few tricky things here to get everything to work right:</p> <ol> <li>Freezing logic for the transformer has to be updated in order to not freeze the new head layers.</li> <li>The LoRA adapter logic has to be updated to pull the input/output sizes of the layers to be adapted from the    modules that are passed (the previous method was compatible with nn and TE, but not megatrons tensor_parallel    modules that are currently used by geneformer). This method contains a suggested refactor to make these methods    a little more general and extensible with structural pattern matching as well. We should push this    requirement onto NeMo, since we shouldn't duplicate the adapter method.</li> <li>There's a ton of assumptions in NeMo about which module is being called and that it inherits specific mixins.    This could break the if it is updated from a megatron module to a torch module or something else. Functional    calls are generally favored for this reason and some have been made here to avoid updating inheritance throughout    the code base.</li> </ol> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class LoRAForGeneFormerTokenRegressor(LoRA):\n    \"\"\"LoRA for Genformer Token Regression.\n\n    There are a few tricky things here to get everything to work right:\n\n    1. Freezing logic for the transformer has to be updated in order to not freeze the new head layers.\n    2. The LoRA adapter logic has to be updated to pull the input/output sizes of the layers to be adapted from the\n       modules that are passed (the previous method was compatible with nn and TE, but not megatrons tensor_parallel\n       modules that are currently used by geneformer). This method contains a suggested refactor to make these methods\n       a little more general and extensible with structural pattern matching as well. We should push this\n       requirement onto NeMo, since we shouldn't duplicate the adapter method.\n    3. There's a ton of assumptions in NeMo about which module is being called and that it inherits specific mixins.\n       This could break the if it is updated from a megatron module to a torch module or something else. Functional\n       calls are generally favored for this reason and some have been made here to avoid updating inheritance throughout\n       the code base.\n    \"\"\"\n\n    def input_size_getter(self, m: nn.Module) -&gt; int:\n        \"\"\"Gets the input size of the supplied model.\"\"\"\n        match m:\n            case object(input_size=n):\n                return n\n            case object(in_features=n):\n                return n\n            case _:\n                raise ValueError(f\"Module {m} does not have a supported input size calculation.\")\n\n    def output_size_getter(self, m: nn.Module) -&gt; int:\n        \"\"\"Gets the output size of the supplied model.\"\"\"\n        match m:\n            case object(output_size=n):\n                return n\n            case object(out_features=n):\n                return n\n            case _:\n                raise ValueError(f\"Module {m} does not have a supported output size calculation.\")\n\n    def __call__(self, model: nn.Module) -&gt; nn.Module:\n        \"\"\"Inference.\"\"\"\n        fn.walk(model, self.selective_freeze)\n        fn.walk(model, self.transform)\n        return model\n\n    def selective_freeze(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module:\n        \"\"\"Freezes either 'encoder' or 'embedding' parameters of the input model (`m`) iff name is one of these.\"\"\"\n        if name in [\"encoder\", \"embedding\"]:\n            FNMixin.freeze(m)\n        return m\n\n    def transform(\n        self, m: nn.Module, name: str | None = None, prefix: str | None = None\n    ) -&gt; nn.Module | AdapterParallelAdd:\n        \"\"\"Transforms the input model if the name is in the target modules.\"\"\"\n        tp_size = parallel_state.get_tensor_model_parallel_world_size()\n        if name in self.target_modules:\n            # m.in_features and m.out_features are divided by tp_size already,\n            # but in_features and out_features passed to ParallelLinearAdapter are not.\n            if prefix is not None and \"regression_head\" in prefix:\n                return m\n            if name in [\"linear_qkv\", \"linear_fc1\"]:\n                # Column Parallel Linear\n                input_is_parallel = False\n                in_features = self.input_size_getter(\n                    m\n                )  # TODO(@georgea) note that this could break depending on the impl of `m`\n                out_features = self.output_size_getter(m) * tp_size\n                # LoRA is applied after layernorm, so layernorm output must be returned\n                m.return_layernorm_output = True\n                # perf optimization for LoRA + SP\n                if m.config.sequence_parallel and not m.ub_overlap_ag:\n                    m.return_layernorm_output_gathered = True\n            else:  # name in ['linear_proj', 'linear_fc2']\n                # Row Parallel Linear\n                input_is_parallel = True\n                in_features = (\n                    self.input_size_getter(m) * tp_size\n                )  # TODO(@georgea) note this could break depending on the impl of `m`\n                out_features = self.output_size_getter(m)\n\n            adapter = ParallelLinearAdapter(\n                in_features,\n                out_features,\n                self.dim,\n                activation=\"identity\",\n                norm_position=None,\n                norm_type=None,\n                column_init_method=self.lora_A_init_method,\n                row_init_method=self.lora_B_init_method,\n                gather_output=False,\n                input_is_parallel=input_is_parallel,\n                dropout=self.dropout,\n                dropout_position=self.dropout_position,\n                model_parallel_config=getattr(m, \"config\", None),\n                alpha=self.alpha,\n            )\n            return AdapterParallelAdd(m, adapter)\n        return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.__call__","title":"<code>__call__(model)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __call__(self, model: nn.Module) -&gt; nn.Module:\n    \"\"\"Inference.\"\"\"\n    fn.walk(model, self.selective_freeze)\n    fn.walk(model, self.transform)\n    return model\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.input_size_getter","title":"<code>input_size_getter(m)</code>","text":"<p>Gets the input size of the supplied model.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def input_size_getter(self, m: nn.Module) -&gt; int:\n    \"\"\"Gets the input size of the supplied model.\"\"\"\n    match m:\n        case object(input_size=n):\n            return n\n        case object(in_features=n):\n            return n\n        case _:\n            raise ValueError(f\"Module {m} does not have a supported input size calculation.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.output_size_getter","title":"<code>output_size_getter(m)</code>","text":"<p>Gets the output size of the supplied model.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def output_size_getter(self, m: nn.Module) -&gt; int:\n    \"\"\"Gets the output size of the supplied model.\"\"\"\n    match m:\n        case object(output_size=n):\n            return n\n        case object(out_features=n):\n            return n\n        case _:\n            raise ValueError(f\"Module {m} does not have a supported output size calculation.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.selective_freeze","title":"<code>selective_freeze(m, name=None, prefix=None)</code>","text":"<p>Freezes either 'encoder' or 'embedding' parameters of the input model (<code>m</code>) iff name is one of these.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def selective_freeze(self, m: nn.Module, name: str | None = None, prefix: str | None = None) -&gt; nn.Module:\n    \"\"\"Freezes either 'encoder' or 'embedding' parameters of the input model (`m`) iff name is one of these.\"\"\"\n    if name in [\"encoder\", \"embedding\"]:\n        FNMixin.freeze(m)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.LoRAForGeneFormerTokenRegressor.transform","title":"<code>transform(m, name=None, prefix=None)</code>","text":"<p>Transforms the input model if the name is in the target modules.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def transform(\n    self, m: nn.Module, name: str | None = None, prefix: str | None = None\n) -&gt; nn.Module | AdapterParallelAdd:\n    \"\"\"Transforms the input model if the name is in the target modules.\"\"\"\n    tp_size = parallel_state.get_tensor_model_parallel_world_size()\n    if name in self.target_modules:\n        # m.in_features and m.out_features are divided by tp_size already,\n        # but in_features and out_features passed to ParallelLinearAdapter are not.\n        if prefix is not None and \"regression_head\" in prefix:\n            return m\n        if name in [\"linear_qkv\", \"linear_fc1\"]:\n            # Column Parallel Linear\n            input_is_parallel = False\n            in_features = self.input_size_getter(\n                m\n            )  # TODO(@georgea) note that this could break depending on the impl of `m`\n            out_features = self.output_size_getter(m) * tp_size\n            # LoRA is applied after layernorm, so layernorm output must be returned\n            m.return_layernorm_output = True\n            # perf optimization for LoRA + SP\n            if m.config.sequence_parallel and not m.ub_overlap_ag:\n                m.return_layernorm_output_gathered = True\n        else:  # name in ['linear_proj', 'linear_fc2']\n            # Row Parallel Linear\n            input_is_parallel = True\n            in_features = (\n                self.input_size_getter(m) * tp_size\n            )  # TODO(@georgea) note this could break depending on the impl of `m`\n            out_features = self.output_size_getter(m)\n\n        adapter = ParallelLinearAdapter(\n            in_features,\n            out_features,\n            self.dim,\n            activation=\"identity\",\n            norm_position=None,\n            norm_type=None,\n            column_init_method=self.lora_A_init_method,\n            row_init_method=self.lora_B_init_method,\n            gather_output=False,\n            input_is_parallel=input_is_parallel,\n            dropout=self.dropout,\n            dropout_position=self.dropout_position,\n            model_parallel_config=getattr(m, \"config\", None),\n            alpha=self.alpha,\n        )\n        return AdapterParallelAdd(m, adapter)\n    return m\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel","title":"<code>MegatronBioBertFineTuneSeqLengthModel</code>","text":"<p>               Bases: <code>MegatronBioBertModel</code></p> <p>Megatron model for biobert finetuning with sequence length.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronBioBertFineTuneSeqLengthModel(MegatronBioBertModel):\n    \"\"\"Megatron model for biobert finetuning with sequence length.\"\"\"\n\n    def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n        self.include_hiddens_finetuning = (\n            include_hiddens  # this include_hiddens is for the final output of fine-tuning\n        )\n        # If post_process is True that means that we are at the last megatron parallelism stage and we can\n        #   apply the head.\n        if post_process:\n            # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n            self.regression_head = MegatronRegressionMLPHead(config)\n\n    def forward(self, *args, **kwargs) -&gt; MegatronFineTuneOutput | BioBertOutput | Tensor:\n        \"\"\"Inference.\"\"\"\n        output: MegatronFineTuneOutput | BioBertOutput | Tensor = super().forward(*args, **kwargs)\n        # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n        if not self.post_process:\n            return output  # we are not at the last pipeline stage so just return what the parent has\n        # Double check that the output from the parent has everything we need to do prediction in this head.\n        if not isinstance(output, dict) or (\"hidden_states\" not in output):\n            raise ValueError(\n                f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n                \"Make sure include_hiddens=True in the call to super().__init__\"\n            )\n        # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n        hidden_states: Tensor = output[\"hidden_states\"][:, 0]  # [b s h] =&gt; [b h], use [CLS] (first) token for reg\n        # Predict our 1d regression target\n        regression_output = self.regression_head(hidden_states)\n        if not self.include_hiddens_finetuning:\n            del output[\"hidden_states\"]\n        output[\"regression_output\"] = regression_output\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel.__init__","title":"<code>__init__(config, *args, include_hiddens=False, post_process=True, **kwargs)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __init__(self, config, *args, include_hiddens: bool = False, post_process: bool = True, **kwargs):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config, *args, include_hiddens=True, post_process=post_process, **kwargs)\n    self.include_hiddens_finetuning = (\n        include_hiddens  # this include_hiddens is for the final output of fine-tuning\n    )\n    # If post_process is True that means that we are at the last megatron parallelism stage and we can\n    #   apply the head.\n    if post_process:\n        # if we are doing post process (eg pipeline last stage) then we need to add the output layers\n        self.regression_head = MegatronRegressionMLPHead(config)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronBioBertFineTuneSeqLengthModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; MegatronFineTuneOutput | BioBertOutput | Tensor:\n    \"\"\"Inference.\"\"\"\n    output: MegatronFineTuneOutput | BioBertOutput | Tensor = super().forward(*args, **kwargs)\n    # Stop early if we are not in post_process mode (for example if we are in the middle of model parallelism)\n    if not self.post_process:\n        return output  # we are not at the last pipeline stage so just return what the parent has\n    # Double check that the output from the parent has everything we need to do prediction in this head.\n    if not isinstance(output, dict) or (\"hidden_states\" not in output):\n        raise ValueError(\n            f\"Expected to find 'hidden_states' in the output, and output to be dictionary-like, found {output},\\n\"\n            \"Make sure include_hiddens=True in the call to super().__init__\"\n        )\n    # Get the hidden state from the parent output, and pull out the [CLS] token for this task\n    hidden_states: Tensor = output[\"hidden_states\"][:, 0]  # [b s h] =&gt; [b h], use [CLS] (first) token for reg\n    # Predict our 1d regression target\n    regression_output = self.regression_head(hidden_states)\n    if not self.include_hiddens_finetuning:\n        del output[\"hidden_states\"]\n    output[\"regression_output\"] = regression_output\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronFineTuneOutput","title":"<code>MegatronFineTuneOutput</code>","text":"<p>               Bases: <code>BioBertOutput</code></p> <p>Inference output type for MegatronBioBertFineTuneSeqLengthModel.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronFineTuneOutput(BioBertOutput):\n    \"\"\"Inference output type for MegatronBioBertFineTuneSeqLengthModel.\"\"\"\n\n    regression_output: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead","title":"<code>MegatronRegressionMLPHead</code>","text":"<p>               Bases: <code>MegatronModule</code></p> <p>A megatron MLP head.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class MegatronRegressionMLPHead(MegatronModule):\n    \"\"\"A megatron MLP head.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(config)\n        # FC layer over just the [CLS] token embedding\n        # TODO use bias/activation fusion if requested\n        self.linear_fc1 = nn.Linear(in_features=config.hidden_size, out_features=config.ffn_hidden_size)\n        self.activation_function = config.activation_func\n        self.linear_fc2 = nn.Linear(in_features=config.ffn_hidden_size, out_features=1)\n\n    def forward(self, hidden_states: Tensor) -&gt; Tensor:\n        \"\"\"Inference.\"\"\"\n        return self.linear_fc2(self.activation_function(self.linear_fc1(hidden_states)))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def __init__(self, config: TransformerConfig):\n    \"\"\"Constructor.\"\"\"\n    super().__init__(config)\n    # FC layer over just the [CLS] token embedding\n    # TODO use bias/activation fusion if requested\n    self.linear_fc1 = nn.Linear(in_features=config.hidden_size, out_features=config.ffn_hidden_size)\n    self.activation_function = config.activation_func\n    self.linear_fc2 = nn.Linear(in_features=config.ffn_hidden_size, out_features=1)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.MegatronRegressionMLPHead.forward","title":"<code>forward(hidden_states)</code>","text":"<p>Inference.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(self, hidden_states: Tensor) -&gt; Tensor:\n    \"\"\"Inference.\"\"\"\n    return self.linear_fc2(self.activation_function(self.linear_fc1(hidden_states)))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.SequenceLengthRMSEPlusBERTMLMLossWithReduction","title":"<code>SequenceLengthRMSEPlusBERTMLMLossWithReduction</code>","text":"<p>               Bases: <code>BERTMLMLossWithReduction</code></p> <p>Loss function.</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>class SequenceLengthRMSEPlusBERTMLMLossWithReduction(BERTMLMLossWithReduction):\n    \"\"\"Loss function.\"\"\"\n\n    def forward(\n        self,\n        batch: SeqLenRmsepBatch,\n        forward_out: Dict[str, Tensor],\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n        \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently.\n\n        In the future this will be extended to handle other loss types like sequence loss if it is present in the\n        forward_out and batch.\n\n        Args:\n            batch: The batch of data. Each tensor should be of shape [batch_size, *, *],\n                and match the corresponding dimension for that particular key in the batch output.\n                For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n            forward_out: The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n        Taken from:\n        https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976\n        \"\"\"\n        if \"labels\" not in batch:\n            raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n        unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])\n        regression_output = forward_out[\"regression_output\"]\n        n_tokens = batch[\"attention_mask\"].sum(dim=-1, keepdim=True).to(dtype=regression_output.dtype)\n        assert len(n_tokens.shape) == 2\n        assert n_tokens.shape[-1] == 1\n        rmse_loss = torch.nn.functional.mse_loss(regression_output, n_tokens)\n\n        # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            # reduce the loss across the micro batch\n            loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n        else:\n            # reduce the loss across the micro batch.\n            # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n            #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n            #  other necessary keys to the batch. Thanks!\n            loss_for_microbatch = masked_token_loss_context_parallel(\n                unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n            )\n\n        # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n        #  reducing the loss across the data parallel group.\n        if self.validation_step and not self.val_drop_last:\n            num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n            if loss_for_microbatch.isnan():\n                # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n                #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n                #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n                if batch[\"loss_mask\"].count_nonzero() != 0:\n                    raise ValueError(\"Got NaN loss with non-empty input\")\n                loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n            else:\n                loss_sum_for_microbatch = num_valid_tokens_in_microbatch * loss_for_microbatch\n\n            # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n            loss_sum_and_microbatch_size_all_gpu = torch.cat(\n                [\n                    loss_sum_for_microbatch.clone().detach().view(1),\n                    torch.tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n                ]\n            )\n            torch.distributed.all_reduce(\n                loss_sum_and_microbatch_size_all_gpu, group=parallel_state.get_data_parallel_group()\n            )\n            return loss_for_microbatch * cp_size, {\n                \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n            }\n        loss_for_microbatch = loss_for_microbatch + rmse_loss  # add in the RMSE loss after reducing the logit loss\n        # average the losses across the data parallel group, but also return the unreduced loss\n        reduced_loss: Tensor = average_losses_across_data_parallel_group([loss_for_microbatch])\n        if (self.validation_step and self.send_val_output) or (not self.validation_step and self.send_train_output):\n            return loss_for_microbatch * cp_size, {\"avg\": reduced_loss, \"batch\": batch, \"forward_out\": forward_out}\n        else:\n            return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/model/finetune_token_regressor/#bionemo.geneformer.model.finetune_token_regressor.SequenceLengthRMSEPlusBERTMLMLossWithReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Computes loss of <code>labels</code> in the batch vs <code>token_logits</code> in the forward output currently.</p> <p>In the future this will be extended to handle other loss types like sequence loss if it is present in the forward_out and batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SeqLenRmsepBatch</code> <p>The batch of data. Each tensor should be of shape [batch_size, , ], and match the corresponding dimension for that particular key in the batch output. For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>The forward output from the model. Each tensor should be of shape [batch_size, , ]</p> required <p>Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976</p> Source code in <code>bionemo/geneformer/model/finetune_token_regressor.py</code> <pre><code>def forward(\n    self,\n    batch: SeqLenRmsepBatch,\n    forward_out: Dict[str, Tensor],\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n    \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently.\n\n    In the future this will be extended to handle other loss types like sequence loss if it is present in the\n    forward_out and batch.\n\n    Args:\n        batch: The batch of data. Each tensor should be of shape [batch_size, *, *],\n            and match the corresponding dimension for that particular key in the batch output.\n            For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n        forward_out: The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n    Taken from:\n    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976\n    \"\"\"\n    if \"labels\" not in batch:\n        raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n    unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])\n    regression_output = forward_out[\"regression_output\"]\n    n_tokens = batch[\"attention_mask\"].sum(dim=-1, keepdim=True).to(dtype=regression_output.dtype)\n    assert len(n_tokens.shape) == 2\n    assert n_tokens.shape[-1] == 1\n    rmse_loss = torch.nn.functional.mse_loss(regression_output, n_tokens)\n\n    # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        # reduce the loss across the micro batch\n        loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n    else:\n        # reduce the loss across the micro batch.\n        # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n        #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n        #  other necessary keys to the batch. Thanks!\n        loss_for_microbatch = masked_token_loss_context_parallel(\n            unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n        )\n\n    # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n    #  reducing the loss across the data parallel group.\n    if self.validation_step and not self.val_drop_last:\n        num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n        if loss_for_microbatch.isnan():\n            # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n            #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n            #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n            if batch[\"loss_mask\"].count_nonzero() != 0:\n                raise ValueError(\"Got NaN loss with non-empty input\")\n            loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n        else:\n            loss_sum_for_microbatch = num_valid_tokens_in_microbatch * loss_for_microbatch\n\n        # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n        loss_sum_and_microbatch_size_all_gpu = torch.cat(\n            [\n                loss_sum_for_microbatch.clone().detach().view(1),\n                torch.tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n            ]\n        )\n        torch.distributed.all_reduce(\n            loss_sum_and_microbatch_size_all_gpu, group=parallel_state.get_data_parallel_group()\n        )\n        return loss_for_microbatch * cp_size, {\n            \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n        }\n    loss_for_microbatch = loss_for_microbatch + rmse_loss  # add in the RMSE loss after reducing the logit loss\n    # average the losses across the data parallel group, but also return the unreduced loss\n    reduced_loss: Tensor = average_losses_across_data_parallel_group([loss_for_microbatch])\n    if (self.validation_step and self.send_val_output) or (not self.validation_step and self.send_train_output):\n        return loss_for_microbatch * cp_size, {\"avg\": reduced_loss, \"batch\": batch, \"forward_out\": forward_out}\n    else:\n        return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedFineTuneSeqLenBioBertConfig","title":"<code>ExposedFineTuneSeqLenBioBertConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[FineTuneSeqLenBioBertConfig]</code></p> <p>Config for models that fine-tune a BioBERT model from a pre-trained checkpoint.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class ExposedFineTuneSeqLenBioBertConfig(ExposedModelConfig[FineTuneSeqLenBioBertConfig]):\n    \"\"\"Config for models that fine-tune a BioBERT model from a pre-trained checkpoint.\n\n    Parameters:\n        initial_ckpt_path - path to a directory containing checkpoint files for initializing the model. This is only\n            required on the first execution of the model, any restored checkpoints should skip this step.\n        initial_ckpt_skip_keys_with_these_prefixes - skip any layer that contains this key during restoration. Useful\n            for ignoring extra additional layers used for finetuning. Layers with these keys are then randomly initialized.\n    \"\"\"\n\n    # Custom parameters for FineTuning\n    initial_ckpt_path: Optional[str] = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    def model_class(self) -&gt; Type[FineTuneSeqLenBioBertConfig]:\n        \"\"\"Binds the class to FineTuneSeqLenBioBertConfig.\"\"\"\n        return FineTuneSeqLenBioBertConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedFineTuneSeqLenBioBertConfig.model_class","title":"<code>model_class()</code>","text":"<p>Binds the class to FineTuneSeqLenBioBertConfig.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[FineTuneSeqLenBioBertConfig]:\n    \"\"\"Binds the class to FineTuneSeqLenBioBertConfig.\"\"\"\n    return FineTuneSeqLenBioBertConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.ExposedGeneformerPretrainConfig","title":"<code>ExposedGeneformerPretrainConfig</code>","text":"<p>               Bases: <code>ExposedModelConfig[GeneformerConfig]</code></p> <p>Exposes custom parameters for pretraining and binds the class to GeneformerConfig.</p> <p>Attributes:</p> Name Type Description <code>initial_ckpt_path</code> <code>str</code> <p>Path to a directory containing checkpoint files for initializing the model. This is only</p> <code>initial_ckpt_skip_keys_with_these_prefixes</code> <code>List[str]</code> <p>Skip any layer that contains this key during restoration. Useful for finetuning, set the names of the task heads so checkpoint restoration does not errorniously try to restore these.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class ExposedGeneformerPretrainConfig(ExposedModelConfig[GeneformerConfig]):\n    \"\"\"Exposes custom parameters for pretraining and binds the class to GeneformerConfig.\n\n    Attributes:\n        initial_ckpt_path (str): Path to a directory containing checkpoint files for initializing the model. This is only\n        initial_ckpt_skip_keys_with_these_prefixes (List[str]): Skip any layer that contains this key during restoration. Useful for finetuning, set the names of the task heads so checkpoint restoration does not errorniously try to restore these.\n    \"\"\"\n\n    # Custom parameters for FineTuning\n    initial_ckpt_path: Optional[str] = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n\n    def model_class(self) -&gt; Type[GeneformerConfig]:  # noqa: D102\n        return GeneformerConfig\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerDataArtifacts","title":"<code>GeneformerDataArtifacts</code>  <code>dataclass</code>","text":"<p>Data artifacts produced by the geneformer preprocess.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>@dataclass\nclass GeneformerDataArtifacts:\n    \"\"\"Data artifacts produced by the geneformer preprocess.\"\"\"\n\n    tokenizer: Tokenizer\n    median_dict: dict\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig","title":"<code>GeneformerPretrainingDataConfig</code>","text":"<p>               Bases: <code>DataConfig[SingleCellDataModule]</code></p> <p>Configuration class for Geneformer pretraining data.</p> <p>Expects train/test/val to be prior split by directory and processed by <code>sub-packages/bionemo-geneformer/src/bionemo/geneformer/data/singlecell/sc_memmap.py</code>.</p> <p>Attributes:</p> Name Type Description <code>data_dir</code> <code>str</code> <p>Directory where the data is stored.</p> <code>result_dir</code> <code>str | Path</code> <p>Directory where the results will be stored. Defaults to \"./results\".</p> <code>micro_batch_size</code> <code>int</code> <p>Size of the micro-batch. Defaults to 8.</p> <code>seq_length</code> <code>int</code> <p>Sequence length for the data. Defaults to 2048.</p> <code>num_dataset_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> Properties <p>train_data_path (str): Path to the training data. val_data_path (str): Path to the validation data. test_data_path (str): Path to the test data.</p> <p>Methods:</p> Name Description <code>geneformer_preprocess</code> <p>Preprocesses the data using a legacy preprocessor from BioNeMo 1 and returns the necessary artifacts.</p> <code>construct_data_module</code> <p>int) -&gt; SingleCellDataModule: Constructs and returns a SingleCellDataModule using the preprocessed data artifacts.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>class GeneformerPretrainingDataConfig(DataConfig[SingleCellDataModule]):\n    \"\"\"Configuration class for Geneformer pretraining data.\n\n    Expects train/test/val to be prior split by directory and processed by `sub-packages/bionemo-geneformer/src/bionemo/geneformer/data/singlecell/sc_memmap.py`.\n\n    Attributes:\n        data_dir (str): Directory where the data is stored.\n        result_dir (str | pathlib.Path): Directory where the results will be stored. Defaults to \"./results\".\n        micro_batch_size (int): Size of the micro-batch. Defaults to 8.\n        seq_length (int): Sequence length for the data. Defaults to 2048.\n        num_dataset_workers (int): Number of workers for data loading. Defaults to 0.\n\n    Properties:\n        train_data_path (str): Path to the training data.\n        val_data_path (str): Path to the validation data.\n        test_data_path (str): Path to the test data.\n\n    Methods:\n        geneformer_preprocess() -&gt; GeneformerDataArtifacts:\n            Preprocesses the data using a legacy preprocessor from BioNeMo 1 and returns the necessary artifacts.\n        construct_data_module(global_batch_size: int) -&gt; SingleCellDataModule:\n            Constructs and returns a SingleCellDataModule using the preprocessed data artifacts.\n    \"\"\"\n\n    # Shadow two attributes from the parent for visibility.\n    data_dir: str\n    result_dir: str | pathlib.Path = \"./results\"\n    micro_batch_size: int = 8\n\n    seq_length: int = 2048\n    num_dataset_workers: int = 0\n\n    @property\n    def train_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/train\"\n\n    @property\n    def val_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/val\"\n\n    @property\n    def test_data_path(self) -&gt; str:  # noqa: D102\n        return self.data_dir + \"/test\"\n\n    def geneformer_preprocess(self) -&gt; GeneformerDataArtifacts:\n        \"\"\"Geneformer datamodule expects certain artifacts to be present in the data directory.\n\n        This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.\n        \"\"\"\n        preprocessor = GeneformerPreprocess(\n            download_directory=pathlib.Path(self.train_data_path),\n            medians_file_path=pathlib.Path(self.train_data_path + \"/medians.json\"),\n            tokenizer_vocab_path=pathlib.Path(self.train_data_path + \"/geneformer.vocab\"),\n        )\n        result = preprocessor.preprocess()\n        if \"tokenizer\" in result and \"median_dict\" in result:\n            logging.info(\"*************** Preprocessing Finished ************\")\n            return GeneformerDataArtifacts(tokenizer=result[\"tokenizer\"], median_dict=result[\"median_dict\"])\n        else:\n            logging.error(\"Preprocessing failed.\")\n            raise ValueError(\"Preprocessing failed to create tokenizer and/or median dictionary.\")\n\n    def construct_data_module(self, global_batch_size: int) -&gt; SingleCellDataModule:\n        \"\"\"Downloads the requisite data artifacts and instantiates the DataModule.\"\"\"\n        geneformer_data_artifacts: GeneformerDataArtifacts = self.geneformer_preprocess()\n        data = SingleCellDataModule(\n            seq_length=self.seq_length,\n            tokenizer=geneformer_data_artifacts.tokenizer,\n            train_dataset_path=self.train_data_path,\n            val_dataset_path=self.val_data_path,\n            test_dataset_path=self.test_data_path,\n            random_token_prob=0.02,\n            median_dict=geneformer_data_artifacts.median_dict,\n            micro_batch_size=self.micro_batch_size,\n            global_batch_size=global_batch_size,\n            persistent_workers=self.num_dataset_workers &gt; 0,\n            pin_memory=False,\n            num_workers=self.num_dataset_workers,\n        )\n        return data\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>","text":"<p>Downloads the requisite data artifacts and instantiates the DataModule.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def construct_data_module(self, global_batch_size: int) -&gt; SingleCellDataModule:\n    \"\"\"Downloads the requisite data artifacts and instantiates the DataModule.\"\"\"\n    geneformer_data_artifacts: GeneformerDataArtifacts = self.geneformer_preprocess()\n    data = SingleCellDataModule(\n        seq_length=self.seq_length,\n        tokenizer=geneformer_data_artifacts.tokenizer,\n        train_dataset_path=self.train_data_path,\n        val_dataset_path=self.val_data_path,\n        test_dataset_path=self.test_data_path,\n        random_token_prob=0.02,\n        median_dict=geneformer_data_artifacts.median_dict,\n        micro_batch_size=self.micro_batch_size,\n        global_batch_size=global_batch_size,\n        persistent_workers=self.num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=self.num_dataset_workers,\n    )\n    return data\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/config_models/#bionemo.geneformer.run.config_models.GeneformerPretrainingDataConfig.geneformer_preprocess","title":"<code>geneformer_preprocess()</code>","text":"<p>Geneformer datamodule expects certain artifacts to be present in the data directory.</p> <p>This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.</p> Source code in <code>bionemo/geneformer/run/config_models.py</code> <pre><code>def geneformer_preprocess(self) -&gt; GeneformerDataArtifacts:\n    \"\"\"Geneformer datamodule expects certain artifacts to be present in the data directory.\n\n    This method uses a legacy 'preprocessor' from BioNeMo 1 to acquire the associated artifacts.\n    \"\"\"\n    preprocessor = GeneformerPreprocess(\n        download_directory=pathlib.Path(self.train_data_path),\n        medians_file_path=pathlib.Path(self.train_data_path + \"/medians.json\"),\n        tokenizer_vocab_path=pathlib.Path(self.train_data_path + \"/geneformer.vocab\"),\n    )\n    result = preprocessor.preprocess()\n    if \"tokenizer\" in result and \"median_dict\" in result:\n        logging.info(\"*************** Preprocessing Finished ************\")\n        return GeneformerDataArtifacts(tokenizer=result[\"tokenizer\"], median_dict=result[\"median_dict\"])\n    else:\n        logging.error(\"Preprocessing failed.\")\n        raise ValueError(\"Preprocessing failed to create tokenizer and/or median dictionary.\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/main/","title":"Main","text":""},{"location":"API_reference/bionemo/geneformer/run/recipes/","title":"Recipes","text":""},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.default_adam_optimizer_with_cosine_annealing_recipe","title":"<code>default_adam_optimizer_with_cosine_annealing_recipe()</code>","text":"<p>Default optimizer scheduler config for Geneformer. See OptimizerSchedulerConfig for defaults.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def default_adam_optimizer_with_cosine_annealing_recipe() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Default optimizer scheduler config for Geneformer. See OptimizerSchedulerConfig for defaults.\"\"\"\n    return OptimizerSchedulerConfig()\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.default_trainer_config_recipe","title":"<code>default_trainer_config_recipe()</code>","text":"<p>Default trainer config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def default_trainer_config_recipe() -&gt; TrainingConfig:\n    \"\"\"Default trainer config for Geneformer.\"\"\"\n    return TrainingConfig(max_steps=55000, limit_val_batches=2, val_check_interval=100)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.experiment_config_recipe","title":"<code>experiment_config_recipe()</code>","text":"<p>Default experiment config for Geneformer. Used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def experiment_config_recipe() -&gt; ExperimentConfig:\n    \"\"\"Default experiment config for Geneformer. Used in testing.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=\"./results\",\n        experiment_name=\"default_experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.finetune_test_recipe","title":"<code>finetune_test_recipe(args)</code>","text":"<p>Recipe for finetuning a regression head on the masked tokens.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def finetune_test_recipe(args) -&gt; MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for finetuning a regression head on the masked tokens.\"\"\"\n    data_path = args.data_path\n    result_dir = args.result_dir\n\n    parallel_config = ParallelConfig(\n        tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=2\n    )\n    training_config = TrainingConfig(\n        max_steps=10, limit_val_batches=2, val_check_interval=2, precision=\"bf16-mixed\", accelerator=\"gpu\"\n    )\n    data_config = GeneformerPretrainingDataConfig(\n        seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=0,\n        data_dir=data_path,\n    )\n    experiment_config = ExperimentConfig(\n        save_every_n_steps=training_config.val_check_interval,\n        result_dir=result_dir,\n        experiment_name=\"test-experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n\n    optim_config = OptimizerSchedulerConfig(lr_scheduler=\"cosine\")\n    geneformer_config = geneformer_10m_finetune_config(\n        seq_length=data_config.seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    return MainConfig(\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=geneformer_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_experiment_config","title":"<code>geneformer_106m_experiment_config(result_dir)</code>","text":"<p>Experiment config for Geneformer 106m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for Geneformer 106m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"geneformer-106m\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_model_config","title":"<code>geneformer_106m_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 106m model config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer 106m model config settings.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=12,\n        hidden_size=768,\n        ffn_hidden_size=3072,\n        num_attention_heads=12,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_parallel_config","title":"<code>geneformer_106m_parallel_config()</code>","text":"<p>Base parallel config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for Geneformer.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=8,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_pretrain_recipe","title":"<code>geneformer_106m_pretrain_recipe(args)</code>","text":"<p>Recipe for pretraining the 106m model. Uses 8 GPUs for data parallelism.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_pretrain_recipe(\n    args,\n) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining the 106m model. Uses 8 GPUs for data parallelism.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = geneformer_106m_parallel_config()\n    training_config = geneformer_base_training_config()\n    bionemo_model_config = geneformer_106m_model_config(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = geneformer_base_optimizer_scheduler_config()\n    experiment_config = geneformer_106m_experiment_config(result_dir=args.result_dir)\n    wandb_config = geneformer_106m_wandb_config()\n    main_config = MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_106m_wandb_config","title":"<code>geneformer_106m_wandb_config()</code>","text":"<p>Wandb config for Geneformer 106m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_106m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for Geneformer 106m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"geneformer-106m_pretraining\",\n        project=\"geneformer-106m_pretraining\",\n        group=\"geneformer-106m\",\n        tags=[\"geneformer-106m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_experiment_config","title":"<code>geneformer_10m_experiment_config(result_dir)</code>","text":"<p>Experiment config for Geneformer 10m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_experiment_config(result_dir) -&gt; ExperimentConfig:\n    \"\"\"Experiment config for Geneformer 10m.\"\"\"\n    return ExperimentConfig(\n        save_every_n_steps=100,\n        result_dir=result_dir,\n        experiment_name=\"geneformer-10m\",\n        restore_from_checkpoint_path=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_finetune_config","title":"<code>geneformer_10m_finetune_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 10m finetuning config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_finetune_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedFineTuneSeqLenBioBertConfig:\n    \"\"\"Geneformer 10m finetuning config settings.\"\"\"\n    geneformer_config = ExposedFineTuneSeqLenBioBertConfig(\n        num_layers=6,\n        hidden_size=256,\n        ffn_hidden_size=512,\n        num_attention_heads=4,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_finetune_recipe","title":"<code>geneformer_10m_finetune_recipe(args)</code>","text":"<p>Recipe for finetuning the 10m model on a token regression head. Used as an example and for testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_finetune_recipe(\n    args,\n) -&gt; MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for finetuning the 10m model on a token regression head. Used as an example and for testing.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = simple_parallel_recipe()\n    training_config = default_trainer_config_recipe()\n    bionemo_model_config = geneformer_finetuning_regression_head_recipe(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = default_adam_optimizer_with_cosine_annealing_recipe()\n    experiment_config = experiment_config_recipe()\n    wandb_config = WandbConfig(\n        project=\"bionemo2-demo\",\n        entity=\"nvidia\",\n        offline=True,\n        tags=[],\n        group=\"dev\",\n        id=\"dev\",\n        log_model=False,\n        anonymous=True,\n    )\n    main_config = MainConfig[ExposedFineTuneSeqLenBioBertConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_model_config","title":"<code>geneformer_10m_model_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer 10m model config settings.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_model_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer 10m model config settings.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=6,\n        hidden_size=256,\n        ffn_hidden_size=512,\n        num_attention_heads=4,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_pretrain_recipe","title":"<code>geneformer_10m_pretrain_recipe(args)</code>","text":"<p>Recipe for pretraining the 10m model.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_pretrain_recipe(\n    args,\n) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining the 10m model.\"\"\"\n    data_config: GeneformerPretrainingDataConfig = geneformer_data_recipe(data_dir=args.data_path)\n    parallel_config = simple_parallel_recipe()\n    training_config = geneformer_base_training_config()\n    bionemo_model_config = geneformer_10m_model_config(initial_ckpt_path=args.initial_ckpt_path)\n    optim_config = geneformer_base_optimizer_scheduler_config()\n    experiment_config = geneformer_10m_experiment_config(result_dir=args.result_dir)\n    wandb_config = geneformer_10m_wandb_config()\n    main_config = MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig](\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=bionemo_model_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n        wandb_config=wandb_config,\n    )\n    return main_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_10m_wandb_config","title":"<code>geneformer_10m_wandb_config()</code>","text":"<p>Wandb config for Geneformer 10m.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_10m_wandb_config() -&gt; WandbConfig:\n    \"\"\"Wandb config for Geneformer 10m.\"\"\"\n    wandb_config = WandbConfig(\n        entity=\"geneformer-10m_pretraining\",\n        project=\"geneformer-10m_pretraining\",\n        group=\"geneformer-10m\",\n        tags=[\"geneformer-10m\"],\n        offline=True,\n        anonymous=True,\n        id=\"1\",\n        log_model=False,\n    )\n    return wandb_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_optimizer_scheduler_config","title":"<code>geneformer_base_optimizer_scheduler_config()</code>","text":"<p>Base optimizer scheduler config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_optimizer_scheduler_config() -&gt; OptimizerSchedulerConfig:\n    \"\"\"Base optimizer scheduler config for Geneformer.\"\"\"\n    return OptimizerSchedulerConfig(lr=1e-3, lr_scheduler=\"cosine\")  # Matches bionemo1\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_parallel_config","title":"<code>geneformer_base_parallel_config()</code>","text":"<p>Base parallel config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_parallel_config() -&gt; ParallelConfig:\n    \"\"\"Base parallel config for Geneformer.\"\"\"\n    return ParallelConfig(\n        tensor_model_parallel_size=1,\n        pipeline_model_parallel_size=1,\n        accumulate_grad_batches=1,\n        ddp=\"megatron\",\n        num_devices=1,\n        num_nodes=1,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_base_training_config","title":"<code>geneformer_base_training_config()</code>","text":"<p>Base training config for Geneformer.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_base_training_config() -&gt; TrainingConfig:\n    \"\"\"Base training config for Geneformer.\"\"\"\n    return TrainingConfig(\n        max_steps=400000, limit_val_batches=8, val_check_interval=100, precision=\"bf16-mixed\"\n    )  # matches bionemo1\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_data_recipe","title":"<code>geneformer_data_recipe(data_dir)</code>","text":"<p>Recipe that produces the base geneformer small data configuration.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_data_recipe(data_dir) -&gt; GeneformerPretrainingDataConfig:\n    \"\"\"Recipe that produces the base geneformer small data configuration.\"\"\"\n    return GeneformerPretrainingDataConfig(data_dir=data_dir)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_finetuning_regression_head_recipe","title":"<code>geneformer_finetuning_regression_head_recipe(precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, initial_ckpt_skip_keys_with_these_prefixes=None)</code>","text":"<p>Recipe for finetuning a regression head on the masked tokens.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_finetuning_regression_head_recipe(\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    initial_ckpt_skip_keys_with_these_prefixes: Optional[List[str]] = None,\n) -&gt; ExposedFineTuneSeqLenBioBertConfig:\n    \"\"\"Recipe for finetuning a regression head on the masked tokens.\"\"\"\n    partial_finetuning_config = partial(\n        ExposedFineTuneSeqLenBioBertConfig,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n        biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n    )\n    if initial_ckpt_skip_keys_with_these_prefixes:\n        finetuning_config = partial_finetuning_config(\n            initial_ckpt_skip_keys_with_these_prefixes=initial_ckpt_skip_keys_with_these_prefixes\n        )\n    else:\n        # Use the sensible default when None is passed\n        finetuning_config = partial_finetuning_config()\n    return finetuning_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.geneformer_tiny_config","title":"<code>geneformer_tiny_config(seq_length=2048, precision='bf16-mixed', nemo1_init_path=None, initial_ckpt_path=None, biobert_spec_option=BiobertSpecOption.bert_layer_with_transformer_engine_spec)</code>","text":"<p>Geneformer tiny model config settings, used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def geneformer_tiny_config(\n    seq_length: int = 2048,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    nemo1_init_path: Optional[str] = None,\n    initial_ckpt_path: Optional[str] = None,\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec,\n) -&gt; ExposedGeneformerPretrainConfig:\n    \"\"\"Geneformer tiny model config settings, used in testing.\"\"\"\n    geneformer_config = ExposedGeneformerPretrainConfig(\n        num_layers=2,\n        hidden_size=32,\n        ffn_hidden_size=4 * 32,\n        num_attention_heads=2,\n        seq_length=seq_length,\n        fp32_residual_connection=False,\n        hidden_dropout=0.02,\n        init_method_std=0.02,\n        kv_channels=None,\n        apply_query_key_layer_scaling=False,\n        make_vocab_size_divisible_by=128,\n        masked_softmax_fusion=True,\n        fp16_lm_cross_entropy=False,\n        params_dtype=precision,\n        pipeline_dtype=precision,\n        autocast_dtype=precision,\n        gradient_accumulation_fusion=False,\n        layernorm_zero_centered_gamma=False,\n        layernorm_epsilon=1.0e-12,\n        activation_func=\"gelu\",\n        qk_layernorm=False,\n        apply_residual_connection_post_layernorm=False,\n        bias_activation_fusion=True,\n        bias_dropout_fusion=True,\n        get_attention_mask_from_fusion=True,\n        attention_dropout=0.1,\n        share_embeddings_and_output_weights=True,\n        enable_autocast=False,\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=nemo1_init_path,\n        initial_ckpt_path=initial_ckpt_path,\n    )\n    return geneformer_config\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.pretrain_tiny_test_recipe","title":"<code>pretrain_tiny_test_recipe(args)</code>","text":"<p>Recipe for pretraining a tiny model. Used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def pretrain_tiny_test_recipe(args) -&gt; MainConfig[ExposedGeneformerPretrainConfig, GeneformerPretrainingDataConfig]:\n    \"\"\"Recipe for pretraining a tiny model. Used in testing.\"\"\"\n    data_path = args.data_path\n    result_dir = args.result_dir\n\n    parallel_config = ParallelConfig(\n        tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=2\n    )\n    training_config = TrainingConfig(\n        max_steps=10, limit_val_batches=2, val_check_interval=2, precision=\"bf16-mixed\", accelerator=\"gpu\"\n    )\n    data_config = GeneformerPretrainingDataConfig(\n        seq_length=128,\n        micro_batch_size=2,\n        num_dataset_workers=0,\n        data_dir=data_path,\n    )\n    experiment_config = ExperimentConfig(\n        save_every_n_steps=training_config.val_check_interval,\n        result_dir=result_dir,\n        experiment_name=\"test-experiment\",\n        restore_from_checkpoint_path=None,\n        save_last_checkpoint=True,\n        metric_to_monitor_for_checkpoints=\"reduced_train_loss\",\n        save_top_k=2,\n        create_tensorboard_logger=False,\n    )\n\n    optim_config = OptimizerSchedulerConfig(lr_scheduler=\"cosine\")\n    geneformer_config = geneformer_tiny_config(\n        seq_length=data_config.seq_length, initial_ckpt_path=args.initial_ckpt_path\n    )\n\n    return MainConfig(\n        data_config=data_config,\n        parallel_config=parallel_config,\n        training_config=training_config,\n        bionemo_model_config=geneformer_config,\n        optim_config=optim_config,\n        experiment_config=experiment_config,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/run/recipes/#bionemo.geneformer.run.recipes.simple_parallel_recipe","title":"<code>simple_parallel_recipe(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, num_devices=1, accumulate_grad_batches=1)</code>","text":"<p>Simple parallel config for Geneformer, only used in testing.</p> Source code in <code>bionemo/geneformer/run/recipes.py</code> <pre><code>def simple_parallel_recipe(\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    num_devices: int = 1,\n    accumulate_grad_batches: int = 1,\n) -&gt; ParallelConfig:\n    \"\"\"Simple parallel config for Geneformer, only used in testing.\"\"\"\n    assert (\n        num_devices &gt;= tensor_model_parallel_size * pipeline_model_parallel_size\n    ), \"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\"\n    return ParallelConfig(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        accumulate_grad_batches=accumulate_grad_batches,\n        num_devices=num_devices,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/","title":"Index","text":""},{"location":"API_reference/bionemo/geneformer/scripts/#geneformer-scripts-directory","title":"Geneformer Scripts Directory","text":"<p>This is a collection for one-off scripts that can be ran through the command line. See the <code>[project.scripts]</code> section of the pyproject.toml file for how these are generated.</p>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/","title":"Geneformer mlm loss eval","text":""},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.GeneformerHFAdapter","title":"<code>GeneformerHFAdapter</code>","text":"<p>               Bases: <code>Module</code></p> <p>An adapter class for running the HF model against our subset of tokens.</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>class GeneformerHFAdapter(torch.nn.Module):\n    \"\"\"An adapter class for running the HF model against our subset of tokens.\"\"\"\n\n    def __init__(self, hf_path: str, my_token_dict: Dict[str, int], nv_tokenizer: GeneTokenizer):\n        \"\"\"An adapter that filters and re-orders tokens to match our tokenizer but with the original indices.\"\"\"\n        super().__init__()\n        self.model = AutoModelForMaskedLM.from_pretrained(hf_path)\n        self.my_token_dict = deepcopy(my_token_dict)\n        self.nv_tokenizer = deepcopy(nv_tokenizer)\n        self.n_tokens_nv = len(self.nv_tokenizer.vocab)\n        self.n_tokens_hf = len(my_token_dict)\n\n        # nvidia tokenizer has [cls] and [pad] first along with some others that do not overlap. This mapper\n        hf_ordered_nv_tokenizer = {\n            self.nv_tokenizer.pad_token: my_token_dict[\"&lt;pad&gt;\"],\n            self.nv_tokenizer.mask_token: my_token_dict[\"&lt;mask&gt;\"],\n            self.nv_tokenizer.cls_token: my_token_dict[\"&lt;cls&gt;\"],\n            self.nv_tokenizer.sep_token: my_token_dict[\"&lt;eos&gt;\"],  # name doesn't really matter here\n        }\n        tokens = list(my_token_dict.items())\n        for k, t in tokens[:4]:\n            assert k.startswith(\"&lt;\")\n\n        missing_nv_tokens = []\n        extra_tokens_not_covered = []\n        for ens, idx in list(my_token_dict.items())[4:]:\n            assert ens.startswith(\"ENSG\")\n            if ens in nv_tokenizer.vocab.keys():\n                hf_ordered_nv_tokenizer[ens] = idx\n            else:\n                if idx &lt; self.n_tokens_hf:\n                    missing_nv_tokens.append(idx)\n                else:\n                    extra_tokens_not_covered.append(idx)\n        self.hf_ordered_nv_tokenizer = hf_ordered_nv_tokenizer\n        self.extra_tokens_not_covered = extra_tokens_not_covered\n        self.register_buffer(\"missing_nv_tokens\", torch.tensor(missing_nv_tokens, dtype=int))\n\n    @property\n    def device(self) -&gt; torch.device:\n        \"\"\"Return the device of this model.\"\"\"\n        # This is populated through the self.register_buffer call in init.\n        return self.missing_nv_tokens.device\n\n    def get_tokenizer(self) -&gt; GeneTokenizer:\n        \"\"\"Return the filtered tokenizer with keys that match the order of the nv model.\"\"\"\n        nv_tok = deepcopy(self.nv_tokenizer)\n        # HF tokenizer only has pad and mask, no other special tokens.\n        nv_tok.special_tokens = (nv_tok.mask_token, nv_tok.pad_token)  # type: ignore\n        nv_tok.vocab = self.hf_ordered_nv_tokenizer\n        nv_tok.decode_vocab = {v: k for k, v in nv_tok.vocab.items()}\n        return nv_tok\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Run forward and return the logits.\"\"\"\n        logits = self.model(*args, **kwargs).logits\n        # logits[:, :, self.missing_nv_tokens] = -torch.inf\n        # breakpoint()\n        return logits\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.GeneformerHFAdapter.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Return the device of this model.</p>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.GeneformerHFAdapter.__init__","title":"<code>__init__(hf_path, my_token_dict, nv_tokenizer)</code>","text":"<p>An adapter that filters and re-orders tokens to match our tokenizer but with the original indices.</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>def __init__(self, hf_path: str, my_token_dict: Dict[str, int], nv_tokenizer: GeneTokenizer):\n    \"\"\"An adapter that filters and re-orders tokens to match our tokenizer but with the original indices.\"\"\"\n    super().__init__()\n    self.model = AutoModelForMaskedLM.from_pretrained(hf_path)\n    self.my_token_dict = deepcopy(my_token_dict)\n    self.nv_tokenizer = deepcopy(nv_tokenizer)\n    self.n_tokens_nv = len(self.nv_tokenizer.vocab)\n    self.n_tokens_hf = len(my_token_dict)\n\n    # nvidia tokenizer has [cls] and [pad] first along with some others that do not overlap. This mapper\n    hf_ordered_nv_tokenizer = {\n        self.nv_tokenizer.pad_token: my_token_dict[\"&lt;pad&gt;\"],\n        self.nv_tokenizer.mask_token: my_token_dict[\"&lt;mask&gt;\"],\n        self.nv_tokenizer.cls_token: my_token_dict[\"&lt;cls&gt;\"],\n        self.nv_tokenizer.sep_token: my_token_dict[\"&lt;eos&gt;\"],  # name doesn't really matter here\n    }\n    tokens = list(my_token_dict.items())\n    for k, t in tokens[:4]:\n        assert k.startswith(\"&lt;\")\n\n    missing_nv_tokens = []\n    extra_tokens_not_covered = []\n    for ens, idx in list(my_token_dict.items())[4:]:\n        assert ens.startswith(\"ENSG\")\n        if ens in nv_tokenizer.vocab.keys():\n            hf_ordered_nv_tokenizer[ens] = idx\n        else:\n            if idx &lt; self.n_tokens_hf:\n                missing_nv_tokens.append(idx)\n            else:\n                extra_tokens_not_covered.append(idx)\n    self.hf_ordered_nv_tokenizer = hf_ordered_nv_tokenizer\n    self.extra_tokens_not_covered = extra_tokens_not_covered\n    self.register_buffer(\"missing_nv_tokens\", torch.tensor(missing_nv_tokens, dtype=int))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.GeneformerHFAdapter.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Run forward and return the logits.</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Run forward and return the logits.\"\"\"\n    logits = self.model(*args, **kwargs).logits\n    # logits[:, :, self.missing_nv_tokens] = -torch.inf\n    # breakpoint()\n    return logits\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.GeneformerHFAdapter.get_tokenizer","title":"<code>get_tokenizer()</code>","text":"<p>Return the filtered tokenizer with keys that match the order of the nv model.</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>def get_tokenizer(self) -&gt; GeneTokenizer:\n    \"\"\"Return the filtered tokenizer with keys that match the order of the nv model.\"\"\"\n    nv_tok = deepcopy(self.nv_tokenizer)\n    # HF tokenizer only has pad and mask, no other special tokens.\n    nv_tok.special_tokens = (nv_tok.mask_token, nv_tok.pad_token)  # type: ignore\n    nv_tok.vocab = self.hf_ordered_nv_tokenizer\n    nv_tok.decode_vocab = {v: k for k, v in nv_tok.vocab.items()}\n    return nv_tok\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.entrypoint","title":"<code>entrypoint()</code>","text":"<p>Main entry point for running the evaluation.</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>def entrypoint():\n    \"\"\"Main entry point for running the evaluation.\"\"\"\n    parser = argparse.ArgumentParser(description=\"MLM Performance vs HF Script\")\n    parser.add_argument(\n        \"--model-path\",\n        type=Path,\n        help=\"Path to nvidia geneformer model checkpoint (unless you want random weights)\",\n        required=False,\n        default=None,\n    )\n    parser.add_argument(\n        \"--hf-token-dictionary-path\",\n        type=Path,\n        help=\"Path to token dictionary file. \"\n        \"Eg `wget https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/token_dictionary_gc95M.pkl`\"\n        \"then provide the path to the downloaded file.\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--hf-medians-dictionary-path\",\n        type=Path,\n        help=\"Path to token dictionary file. \"\n        \"Eg `wget https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_median_dictionary_gc95M.pkl` \"\n        \"then provide the path to the downloaded file.\",\n        required=True,\n    )\n    parser.add_argument(\"--hf-model-path\", type=str, default=\"ctheodoris/Geneformer\", help=\"HF model path\")\n    parser.add_argument(\"--dataset-path\", type=Path, help=\"Path to dataset directory\", required=True)\n\n    args = parser.parse_args()\n    main(\n        args.model_path,\n        args.hf_model_path,\n        args.dataset_path,\n        args.hf_token_dictionary_path,\n        args.hf_medians_dictionary_path,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/geneformer_mlm_loss_eval/#bionemo.geneformer.scripts.geneformer_mlm_loss_eval.main","title":"<code>main(model_path, hf_model_path, dataset_path, hf_token_dictionary_path, hf_medians_dictionary_path, mask_prob=0.15, batch_size=16, precision='bf16-mixed', config_class=GeneformerConfig, seq_len_nv=2048, seq_len_hf=2048, seed=513)</code>","text":"<p>Inference function (requires DDP and only training data that fits in memory).</p> Source code in <code>bionemo/geneformer/scripts/geneformer_mlm_loss_eval.py</code> <pre><code>def main(\n    model_path: Path | None,\n    hf_model_path: str,\n    dataset_path: Path,\n    hf_token_dictionary_path: Path,\n    hf_medians_dictionary_path: Path,\n    mask_prob: float = 0.15,\n    batch_size: int = 16,\n    precision: str = \"bf16-mixed\",\n    config_class: Type[BioBertConfig] = GeneformerConfig,\n    seq_len_nv: int = 2048,\n    seq_len_hf: int = 2048,\n    seed: int = 513,\n):\n    \"\"\"Inference function (requires DDP and only training data that fits in memory).\"\"\"\n    # This is just used to get the tokenizer :(\n    train_data_path: Path = (\n        load(\"single_cell/testdata-20240506\") / \"cellxgene_2023-12-15_small\" / \"processed_data\" / \"train\"\n    )\n    n_devices: int = torch.cuda.device_count()\n    assert n_devices &gt; 0\n    preprocessor = GeneformerPreprocess(\n        download_directory=train_data_path,\n        medians_file_path=train_data_path / \"medians.json\",\n        tokenizer_vocab_path=train_data_path / \"geneformer.vocab\",\n    )\n    match preprocessor.preprocess():\n        case {\"tokenizer\": tokenizer, \"median_dict\": median_dict}:\n            logging.info(\"*************** Preprocessing Finished ************\")\n        case _:\n            logging.error(\"Failed to download the tokenizer for the NV geneformer model.\")\n            assert False\n    with open(hf_token_dictionary_path, \"rb\") as geneformer_hf_token_file:\n        geneformer_hf_token_dict = pickle.load(geneformer_hf_token_file)\n    with open(hf_medians_dictionary_path, \"rb\") as geneformer_hf_median_file:\n        geneformer_hf_medians_dict = pickle.load(geneformer_hf_median_file)\n    with megatron_parallel_state_utils.distributed_model_parallel_state():\n        geneformer_nv_inferer_cfg = config_class(\n            seq_length=seq_len_nv,\n            params_dtype=get_autocast_dtype(precision),\n            pipeline_dtype=get_autocast_dtype(precision),\n            autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n            # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n            initial_ckpt_path=str(model_path) if model_path is not None else None,\n            initial_ckpt_skip_keys_with_these_prefixes=[],  # load everything from the checkpoint.\n        )\n        geneformer_nv_inferer = Float16Module(\n            geneformer_nv_inferer_cfg, geneformer_nv_inferer_cfg.configure_model(tokenizer).cuda(0 % n_devices)\n        ).eval()\n\n        # TODO only predict with tokens that exist in both models.\n\n        hf_model = GeneformerHFAdapter(hf_model_path, geneformer_hf_token_dict, tokenizer).eval().cuda(1 % n_devices)\n        hf_total_params = sum(p.numel() for p in hf_model.parameters() if p.requires_grad)\n        nv_total_params = sum(p.numel() for p in geneformer_nv_inferer.parameters() if p.requires_grad)\n        print(f\"HF Model Params: {hf_total_params}, NV Model Params: {nv_total_params}\", file=sys.stdout)\n        tokenizer_filt = deepcopy(tokenizer)\n        ori_nv_vocab_size: int = len(tokenizer.vocab)\n        hf_tokenizer = hf_model.get_tokenizer()\n        tokenizer_filt.vocab = {\n            k: v for k, v in tokenizer.vocab.items() if k in hf_tokenizer.vocab or k in tokenizer.special_tokens\n        }\n\n        ds_nv = SingleCellDataset(\n            dataset_path,\n            tokenizer=tokenizer_filt,  # TODO replace with the filtered one.\n            median_dict=median_dict,\n            max_len=seq_len_nv,\n            mask_prob=mask_prob,\n            seed=seed,\n        )\n        ds_hf_nvfilt = SingleCellDataset(\n            dataset_path,\n            hf_tokenizer,\n            geneformer_hf_medians_dict,\n            max_len=seq_len_hf,\n            mask_prob=mask_prob,\n            eos_token=hf_tokenizer.token_to_id(hf_tokenizer.sep_token),  # Stored in the special token\n            seed=seed,\n        )\n        print(f\"Loaded dataset of length (NV): {len(ds_nv)}, (HF): {len(ds_hf_nvfilt)}\")\n\n        dl_hf = DataLoader(\n            ds_hf_nvfilt,\n            batch_size=batch_size,\n            sampler=[EpochIndex(epoch=0, idx=i) for i in range(len(ds_hf_nvfilt))],\n            shuffle=False,\n            num_workers=0,\n            drop_last=False,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=ds_hf_nvfilt.tokenizer.pad_id,\n                min_length=seq_len_hf,\n                max_length=seq_len_hf,\n            ),\n        )\n        dl_nv = DataLoader(\n            ds_nv,\n            batch_size=batch_size,\n            sampler=[EpochIndex(epoch=0, idx=i) for i in range(len(ds_nv))],\n            shuffle=False,\n            num_workers=0,\n            drop_last=False,\n            collate_fn=functools.partial(\n                collate.bert_padding_collate_fn,\n                padding_value=ds_nv.tokenizer.pad_id,\n                min_length=seq_len_nv,\n                max_length=seq_len_nv,\n            ),\n        )\n\n        with torch.no_grad():\n            dl_hf_iter = iter(dl_hf)\n            dl_nv_iter = iter(dl_nv)\n            loss_hf = 0.0\n            n_hf = 0\n            loss_nv = 0.0\n            n_nv = 0\n            nv_device = geneformer_nv_inferer.module.embedding.position_embeddings.weight.device\n            hf_device = hf_model.device\n            for _ in trange(len(dl_hf)):\n                batch_hf = {k: v.to(hf_device) for k, v in next(dl_hf_iter).items()}\n                batch_nv = {k: v.to(nv_device) for k, v in next(dl_nv_iter).items()}\n                logits_hf = hf_model(batch_hf[\"text\"].long(), batch_hf[\"attention_mask\"])\n                loss_hf += (\n                    torch.nn.functional.cross_entropy(\n                        logits_hf[batch_hf[\"loss_mask\"]],\n                        batch_hf[\"labels\"][batch_hf[\"loss_mask\"]],\n                        reduction=\"sum\",\n                    )\n                    .cpu()\n                    .sum()\n                    .item()\n                )\n                n_hf += batch_hf[\"loss_mask\"].sum().cpu().item()\n\n                logits_nv = (\n                    geneformer_nv_inferer(batch_nv[\"text\"], batch_nv[\"attention_mask\"])[\"token_logits\"]\n                    .transpose(0, 1)\n                    .contiguous()\n                )\n                loss_nv += (\n                    torch.nn.functional.cross_entropy(\n                        logits_nv[batch_nv[\"loss_mask\"]][..., :ori_nv_vocab_size],\n                        batch_nv[\"labels\"][batch_nv[\"loss_mask\"]],\n                        reduction=\"sum\",\n                    )\n                    .cpu()\n                    .sum()\n                    .item()\n                )\n                n_nv += batch_nv[\"loss_mask\"].sum().cpu().item()\n        print(f\"NV mean loss: {loss_nv / n_nv}\")\n        print(f\"HF mean loss: {loss_hf / n_hf}\")\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/","title":"Infer geneformer","text":""},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.geneformer_infer_entrypoint","title":"<code>geneformer_infer_entrypoint()</code>","text":"<p>Entrypoint for running inference on a geneformer checkpoint and data.</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def geneformer_infer_entrypoint():\n    \"\"\"Entrypoint for running inference on a geneformer checkpoint and data.\"\"\"\n    # 1. get arguments\n    parser = get_parser()\n    args = parser.parse_args()\n    # 2. Call infer with args\n    infer_model(\n        data_path=args.data_dir,\n        checkpoint_path=args.checkpoint_path,\n        results_path=args.result_path,\n        include_hiddens=args.include_hiddens,\n        micro_batch_size=args.micro_batch_size,\n        include_embeddings=not args.no_embeddings,\n        include_logits=args.include_logits,\n        seq_length=args.seq_length,\n        precision=args.precision,\n        devices=args.num_gpus,\n        num_nodes=args.num_nodes,\n        num_dataset_workers=args.num_dataset_workers,\n        config_class=args.config_class,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Infer sc_memmap processed single cell data with Geneformer from a checkpiont.\"\n    )\n    parser.add_argument(\n        \"--data-dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the data directory, for example this might be \"\n        \"/workspace/bionemo2/data/cellxgene_2023-12-15_small/processed_train\",\n    )\n    parser.add_argument(\n        \"--checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from.\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\"--include-hiddens\", action=\"store_true\", default=False, help=\"Include hiddens in output.\")\n    parser.add_argument(\"--no-embeddings\", action=\"store_true\", default=False, help=\"Do not output embeddings.\")\n    parser.add_argument(\n        \"--include-logits\", action=\"store_true\", default=False, help=\"Include per-token logits in output.\"\n    )\n\n    parser.add_argument(\n        \"--result-path\", type=Path, required=False, default=Path(\"./results.pt\"), help=\"Path to the result file.\"\n    )\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Number of steps to use for training. Default is 0.\",\n    )\n    parser.add_argument(\n        \"--seq-length\",\n        type=int,\n        required=False,\n        default=2048,\n        help=\"Sequence length of cell. Default is 2048.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=32,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n\n    # TODO consider whether nemo.run or some other method can simplify this config class lookup.\n    config_class_options: Dict[str, Type[BioBertConfig]] = {\n        \"GeneformerConfig\": GeneformerConfig,\n        \"FineTuneSeqLenBioBertConfig\": FineTuneSeqLenBioBertConfig,\n    }\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--config-class\",\n        type=config_class_type,\n        default=\"GeneformerConfig\",\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/infer_geneformer/#bionemo.geneformer.scripts.infer_geneformer.infer_model","title":"<code>infer_model(data_path, checkpoint_path, results_path, include_hiddens=False, include_embeddings=False, include_logits=False, seq_length=2048, micro_batch_size=64, precision='bf16-mixed', tensor_model_parallel_size=1, pipeline_model_parallel_size=1, devices=1, num_nodes=1, num_dataset_workers=0, config_class=GeneformerConfig)</code>","text":"<p>Inference function (requires DDP and only training data that fits in memory).</p> Source code in <code>bionemo/geneformer/scripts/infer_geneformer.py</code> <pre><code>def infer_model(\n    data_path: Path,\n    checkpoint_path: Path,\n    results_path: Path,\n    include_hiddens: bool = False,\n    include_embeddings: bool = False,\n    include_logits: bool = False,\n    seq_length: int = 2048,\n    micro_batch_size: int = 64,\n    precision: PrecisionTypes = \"bf16-mixed\",\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    devices: int = 1,\n    num_nodes: int = 1,\n    num_dataset_workers: int = 0,\n    config_class: Type[BioBertConfig] = GeneformerConfig,\n) -&gt; None:\n    \"\"\"Inference function (requires DDP and only training data that fits in memory).\"\"\"\n    # This is just used to get the tokenizer :(\n    train_data_path: Path = (\n        load(\"single_cell/testdata-20240506\") / \"cellxgene_2023-12-15_small\" / \"processed_data\" / \"train\"\n    )\n\n    # Setup the strategy and trainer\n    pipeline_model_parallel_size = 1\n    tensor_model_parallel_size = 1\n    accumulate_grad_batches = 1\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n\n    preprocessor = GeneformerPreprocess(\n        download_directory=train_data_path,\n        medians_file_path=train_data_path / \"medians.json\",\n        tokenizer_vocab_path=train_data_path / \"geneformer.vocab\",\n    )\n    match preprocessor.preprocess():\n        case {\"tokenizer\": tokenizer, \"median_dict\": median_dict}:\n            logging.info(\"*************** Preprocessing Finished ************\")\n        case _:\n            logging.error(\"Preprocessing failed.\")\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        progress_interval=1,\n    )\n    trainer = nl.Trainer(\n        devices=devices,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        num_nodes=num_nodes,\n        callbacks=[],\n        plugins=nl.MegatronMixedPrecision(precision=precision),\n    )\n    # Configure the data module and model\n    data = SingleCellDataModule(\n        seq_length=seq_length,\n        tokenizer=tokenizer,\n        train_dataset_path=None,\n        val_dataset_path=None,\n        test_dataset_path=None,\n        predict_dataset_path=data_path,\n        mask_prob=0,\n        mask_token_prob=0,\n        random_token_prob=0,  # changed to represent the incorrect setting we originally used.\n        median_dict=median_dict,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        # persistent workers is supported when num_dataset_workers &gt; 0\n        persistent_workers=num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=num_dataset_workers,\n    )\n    geneformer_config = config_class(\n        seq_length=seq_length,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(checkpoint_path) if checkpoint_path is not None else None,\n        include_embeddings=include_embeddings,\n        include_hiddens=include_hiddens,\n        skip_logits=not include_logits,\n        initial_ckpt_skip_keys_with_these_prefixes=[],  # load everything from the checkpoint.\n    )\n    # The lightning class owns a copy of the actual model, and a loss function, both of which are configured\n    #  and lazily returned by the `geneformer_config` object defined above.\n    model = biobert_lightning_module(\n        geneformer_config,\n        tokenizer=tokenizer,\n    )\n\n    results_dict = batch_collator(trainer.predict(model, datamodule=data, return_predictions=True))\n    non_none_keys = [key for key, val in results_dict.items() if val is not None]\n    print(f\"Writing output {str(non_none_keys)} into {results_path}\")\n    torch.save(results_dict, results_path)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/sc_memmap/","title":"Sc memmap","text":""},{"location":"API_reference/bionemo/geneformer/scripts/sc_memmap/#bionemo.geneformer.scripts.sc_memmap.create_metadata","title":"<code>create_metadata(file_path, shared_dict)</code>","text":"<p>Extract a series of metadata values from <code>AnnData</code> required to process all files into memmaps.</p> <p>Note: it assumes var.feature_ids contains the gene symbols for each dataset and corresponds to the same order as the data.X columns.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PosixPath</code> <p>Path to <code>AnnData</code> stored as *.h5ad.</p> required <code>shared_dict</code> <code>Dict[str, Dict[str, object]]</code> <p>Dictionary to store the extracted metadata.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>If the file cannot be read or if the <code>data</code> object is None.</p> Source code in <code>bionemo/geneformer/scripts/sc_memmap.py</code> <pre><code>def create_metadata(file_path: Path, shared_dict: Dict[str, Dict[str, object]]) -&gt; None:\n    \"\"\"Extract a series of metadata values from `AnnData` required to process all files into memmaps.\n\n    Note: it assumes var.feature_ids contains the gene symbols for each dataset and corresponds to the same order as the data.X columns.\n\n    Args:\n        file_path (PosixPath):\n            Path to `AnnData` stored as *.h5ad.\n        shared_dict (Dict[str, Dict[str, object]]):\n            Dictionary to store the extracted metadata.\n\n    Returns:\n        None:\n            If the file cannot be read or if the `data` object is None.\n\n    \"\"\"\n    try:\n        data = scanpy.read_h5ad(file_path)\n    except Exception as e:\n        raise ValueError(f\"Could not read {file_path}\") from e\n\n    if data is None:\n        return\n\n    shape = data.shape\n    feature_ids = list(data.var.feature_id)\n\n    if data.raw is not None:\n        X = data.raw.X\n    else:\n        X = data.X\n\n    num_el = X.count_nonzero()  # Count the number of non-zero elements in the sparse array, in total\n    # - metadata associated with each file\n    d = {\"shape\": shape, \"feature_ids\": feature_ids, \"num_el\": num_el, \"file_path\": str(file_path)}\n\n    shared_dict[str(file_path)] = d\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/sc_memmap/#bionemo.geneformer.scripts.sc_memmap.find_ann_data_files","title":"<code>find_ann_data_files(data_path)</code>","text":"<p>Find all AnnData files with the extension '.h5ad' in the given data path and its subdirectories.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The path to the directory containing the AnnData files.</p> required <p>Returns:</p> Type Description <code>List[Path]</code> <p>List[str]: A list of file paths to the AnnData files.</p> Source code in <code>bionemo/geneformer/scripts/sc_memmap.py</code> <pre><code>def find_ann_data_files(data_path: Path) -&gt; List[Path]:\n    \"\"\"Find all AnnData files with the extension '.h5ad' in the given data path and its subdirectories.\n\n    Args:\n        data_path (str): The path to the directory containing the AnnData files.\n\n    Returns:\n        List[str]: A list of file paths to the AnnData files.\n    \"\"\"\n    return sorted(data_path.rglob(\"*.h5ad\"))\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/sc_memmap/#bionemo.geneformer.scripts.sc_memmap.write_data","title":"<code>write_data(file_path, obs_cols, metadata, gene_data, gene_data_indices, gene_data_ptr, strict=False)</code>","text":"<p>Writes <code>AnnData</code> into memmap.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PosixPath</code> <p>The path to the file.</p> required <code>obs_cols</code> <code>List[str]</code> <p>A list of columns to extract from each AnnData <code>obs</code> dataframe.</p> required <code>metadata</code> <code>Dict[str, Dict[str, object]]</code> <p>A dictionary containing metadata information on number of elements, shape, and feature names.</p> required <code>gene_data</code> <code>ndarray</code> <p>The array to store gene data.</p> required <code>gene_data_indices</code> <code>ndarray</code> <p>The array to store gene data indices.</p> required <code>gene_data_ptr</code> <code>ndarray</code> <p>The array to store gene data pointers.</p> required <code>strict</code> <code>bool</code> <p>If True, only extract the columns specified in <code>obs_cols</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>List[pd.DataFrame]: The features extracted from the data.</p> Source code in <code>bionemo/geneformer/scripts/sc_memmap.py</code> <pre><code>def write_data(\n    file_path: Path,\n    obs_cols: list,\n    metadata: Dict[str, Dict[str, object]],\n    gene_data: np.ndarray,\n    gene_data_indices: np.ndarray,\n    gene_data_ptr: np.ndarray,\n    strict: bool = False,\n) -&gt; List[pd.DataFrame]:\n    \"\"\"Writes `AnnData` into memmap.\n\n    Args:\n        file_path (PosixPath): The path to the file.\n        obs_cols (List[str]): A list of columns to extract from each AnnData `obs` dataframe.\n        metadata (Dict[str, Dict[str, object]]): A dictionary containing metadata information\n            on number of elements, shape, and feature names.\n        gene_data (np.ndarray): The array to store gene data.\n        gene_data_indices (np.ndarray): The array to store gene data indices.\n        gene_data_ptr (np.ndarray): The array to store gene data pointers.\n        strict (bool): If True, only extract the columns specified in `obs_cols`.\n\n    Returns:\n        List[pd.DataFrame]: The features extracted from the data.\n    \"\"\"\n    # - check if the file name exists in the metadata dictionary\n    if str(file_path) not in metadata:\n        return []\n\n    # Get the metadata for the file\n    meta = metadata[str(file_path)]\n    num_el = meta[\"num_el\"]\n    running_el = meta[\"running_el\"]\n    num_obs = meta[\"shape\"][0]\n    cur_count = meta[\"cur_count\"]\n\n    try:\n        # - read the data from the file using scanpy\n        data = scanpy.read_h5ad(file_path)\n    except Exception:\n        print(f\"couldn't read {file_path}\")\n        return []\n\n    # - get the gene data from the data object\n    X = data.X if data.raw is None else data.raw.X  # Use X if raw is not None, otherwise use raw\n\n    # - store the gene data, indices, and pointers in the respective arrays\n    gene_data[running_el : running_el + num_el] = X.data  # This is a flattened array with everything in it.\n    gene_data_indices[running_el : running_el + num_el] = X.indices.astype(\n        int\n    )  # these are flattened column indices eg [0, 1, 2, 0, 1, 3] for a 2x4 sparse matrix\n    gene_data_ptr[cur_count : cur_count + num_obs + 1] = X.indptr.astype(int) + int(\n        running_el\n    )  # These are mappings between row indices and ranges. eg [0, 3, 6] for a 2x4 sparse matrix\n\n    # - extract the features from the data\n    # TODO: this doesnt work if obs_column doesnt have the right things in it.\n    if not strict:\n        new_obs_cols = list(set(data.obs.columns.tolist()) &amp; set(obs_cols))\n        features = data.obs[new_obs_cols]\n    else:\n        features = data.obs[obs_cols]\n\n    # - flush the data arrays to disk\n    GLOBAL_LOCK.acquire()\n    gene_data.flush()\n    gene_data_ptr.flush()\n    gene_data_indices.flush()\n    GLOBAL_LOCK.release()\n\n    return features\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/","title":"Train geneformer","text":""},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/#bionemo.geneformer.scripts.train_geneformer.get_parser","title":"<code>get_parser()</code>","text":"<p>Return the cli parser for this tool.</p> Source code in <code>bionemo/geneformer/scripts/train_geneformer.py</code> <pre><code>def get_parser():\n    \"\"\"Return the cli parser for this tool.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Pretrain Geneformer with single cell data.\")\n    parser.add_argument(\n        \"--data-dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the data base directory, for example this might be \"\n        \"/workspace/bionemo2/data/cellxgene_2023-12-15_small\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        choices=get_args(PrecisionTypes),\n        required=False,\n        default=\"bf16-mixed\",\n        help=\"Precision type to use for training.\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        required=False,\n        default=1e-4,\n        help=\"Learning rate for training. Default is 1e-4. With bigger global batches try 1e-3\",\n    )\n    parser.add_argument(\n        \"--create-tensorboard-logger\", action=\"store_true\", default=False, help=\"Create a tensorboard logger.\"\n    )\n    # FIXME (@skothenhill) figure out how checkpointing and resumption should work with the new nemo trainer\n    parser.add_argument(\n        \"--resume-if-exists\", action=\"store_true\", default=False, help=\"Resume training if a checkpoint exists.\"\n    )\n    parser.add_argument(\n        \"--result-dir\", type=Path, required=False, default=Path(\"./results\"), help=\"Path to the result directory.\"\n    )\n    parser.add_argument(\n        \"--experiment-name\", type=str, required=False, default=\"geneformer\", help=\"Name of the experiment.\"\n    )\n    parser.add_argument(\"--wandb-entity\", type=str, default=None, help=\"The team posting this run\")\n    parser.add_argument(\"--wandb-project\", type=str, default=None, help=\"Wandb project name \")\n    parser.add_argument(\"--wandb-tags\", nargs=\"+\", type=str, default=None, help=\"Tags associated with this run\")\n    parser.add_argument(\n        \"--wandb-group\", type=str, default=None, help=\"A unique string shared by all runs in a given group\"\n    )\n    parser.add_argument(\n        \"--wandb-id\", type=str, default=None, help=\"Sets the version, mainly used to resume a previous run\"\n    )\n    parser.add_argument(\n        \"--wandb-anonymous\", action=\"store_true\", help=\"Enable or explicitly disable anonymous logging\"\n    )\n    parser.add_argument(\n        \"--wandb-log-model\", action=\"store_true\", help=\"Save checkpoints in wandb dir to upload on W&amp;B servers\"\n    )\n    parser.add_argument(\"--wandb-offline\", action=\"store_true\", help=\"Use wandb in offline mode\")\n    parser.add_argument(\n        \"--cosine-rampup-frac\",\n        type=float,\n        required=False,\n        default=0.01,\n        help=\"Fraction of steps in which to ramp up the learning rate. Default is 0.01.\",\n    )\n    parser.add_argument(\n        \"--cosine-hold-frac\",\n        type=float,\n        required=False,\n        default=0.05,\n        help=\"Fraction of final steps in which to hold the minimum LR. Default is 0.05.\",\n    )\n\n    parser.add_argument(\n        \"--num-gpus\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of GPUs to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-nodes\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Number of nodes to use for training. Default is 1.\",\n    )\n    parser.add_argument(\n        \"--num-steps\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps to use for training. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--num-dataset-workers\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Number of steps to use for training. Default is 0.\",\n    )\n    parser.add_argument(\n        \"--val-check-interval\",\n        type=int,\n        required=False,\n        default=10000,\n        help=\"Number of steps to use for training. Default is 10000.\",\n    )\n    parser.add_argument(\n        \"--log-every-n-steps\",\n        type=int,\n        required=False,\n        default=50,\n        help=\"Number of steps between logging. Default is 50.\",\n    )\n    parser.add_argument(\n        \"--seq-length\",\n        type=int,\n        required=False,\n        default=2048,\n        help=\"Sequence length of cell. Default is 2048.\",\n    )\n    parser.add_argument(\n        \"--limit-val-batches\",\n        type=float_or_int_or_none,\n        required=False,\n        default=2,\n        help=\"Number of global batches used for validation if int. Fraction of validation dataset if float. Default is 2.\",\n    )\n    parser.add_argument(\n        \"--micro-batch-size\",\n        type=int,\n        required=False,\n        default=64,\n        help=\"Micro-batch size. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--accumulate-grad-batches\",\n        type=int,\n        required=False,\n        default=1,\n        help=\"Gradient accumulation steps. Global batch size is inferred from this.\",\n    )\n    parser.add_argument(\n        \"--biobert-spec-option\",\n        type=BiobertSpecOption,\n        choices=[e.value for e in BiobertSpecOption],\n        required=False,\n        default=BiobertSpecOption.bert_layer_with_transformer_engine_spec.value,\n        help=\"Biobert spec option to use for the model. Default is 'bert_layer_with_transformer_engine_spec'.\",\n    )\n    parser.add_argument(\n        \"--nemo1-init-path\",\n        type=Path,\n        required=False,\n        help=\"Path to nemo1 file, if desired to load at init time.\",\n    )\n    parser.add_argument(\n        \"--save-best-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the best checkpoint based on the metric to monitor.\",\n    )\n    parser.add_argument(\n        \"--save-last-checkpoint\",\n        action=\"store_true\",\n        default=True,\n        help=\"Save the last checkpoint.\",\n    )\n    parser.add_argument(\n        \"--metric-to-monitor-for-checkpoints\",\n        type=str,\n        required=False,\n        default=\"val_loss\",\n        help=\"The metric to monitor for checkpointing.\",\n    )\n    parser.add_argument(\n        \"--save-top-k\",\n        type=int,\n        required=False,\n        default=2,\n        help=\"Save the top k checkpoints.\",\n    )\n    parser.add_argument(\n        \"--restore-from-checkpoint-path\",\n        type=Path,\n        required=False,\n        default=None,\n        help=\"Path to the checkpoint directory to restore from. Will override `--resume-if-exists` when set.\",\n    )\n\n    # TODO consider whether nemo.run or some other method can simplify this config class lookup.\n    config_class_options: Dict[str, Type[BioBertConfig]] = {\n        \"GeneformerConfig\": GeneformerConfig,\n        \"FineTuneSeqLenBioBertConfig\": FineTuneSeqLenBioBertConfig,\n    }\n\n    def config_class_type(desc: str) -&gt; Type[BioBertConfig]:\n        try:\n            return config_class_options[desc]\n        except KeyError:\n            raise argparse.ArgumentTypeError(\n                f\"Do not recognize key {desc}, valid options are: {config_class_options.keys()}\"\n            )\n\n    parser.add_argument(\n        \"--training-model-config-class\",\n        type=config_class_type,\n        default=\"GeneformerConfig\",\n        help=\"Model configs link model classes with losses, and handle model initialization (including from a prior \"\n        \"checkpoint). This is how you can fine-tune a model. First train with one config class that points to one model \"\n        \"class and loss, then implement and provide an alternative config class that points to a variant of that model \"\n        \"and alternative loss. In the future this script should also provide similar support for picking different data \"\n        f\"modules for fine-tuning with different data types. Choices: {config_class_options.keys()}\",\n    )\n\n    parser.add_argument(\n        \"--nsys-profiling\",\n        action=\"store_true\",\n        default=False,\n        help=\"Enable targeted `nsys` profiling on the training loop for a defined step range. To actually get profiling output you must run the whole program with `nsys`. For example: \"\n        \" `nsys profile -s none -o output_report_name -t cuda,nvtx --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop  [regular python command here]`\",\n    )\n    # start, end, rank\n    parser.add_argument(\n        \"--nsys-start-step\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Start nsys profiling after this step.\",\n    )\n    parser.add_argument(\n        \"--nsys-end-step\",\n        type=int,\n        required=False,\n        help=\"End nsys profiling after this step.\",\n    )\n    # rank as list of integers\n    parser.add_argument(\n        \"--nsys-ranks\",\n        type=int,\n        nargs=\"+\",\n        required=False,\n        default=[0],\n        help=\"Enable nsys profiling for these ranks.\",\n    )\n\n    parser.add_argument(\n        \"--gc-interval\",\n        type=int,\n        required=False,\n        default=0,\n        help=\"Run garbage collection on the cluster every --gc-interval steps, 0 to disable (default). Keeping gc interval\"\n        \" in sync this way on large cluster runs is important for training performance.\",\n    )\n\n    parser.add_argument(\n        \"--aligned-megatron-ddp\",\n        action=\"store_true\",\n        default=False,\n        help=\"By default param overlap/etc is disabled in megatron, this enables all of those settings. This is probably \"\n        \"good for cluster performance.\",\n    )\n\n    return parser\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/scripts/train_geneformer/#bionemo.geneformer.scripts.train_geneformer.main","title":"<code>main(data_dir, num_nodes, devices, seq_length, result_dir, num_steps, limit_val_batches, val_check_interval, num_dataset_workers, biobert_spec_option, lr, micro_batch_size, accumulate_grad_batches, cosine_rampup_frac, cosine_hold_frac, experiment_name, resume_if_exists, precision, wandb_entity=None, wandb_project=None, wandb_offline=False, wandb_tags=None, wandb_group=None, wandb_id=None, wandb_anonymous=False, wandb_log_model=False, create_tensorboard_logger=False, nemo1_init_path=None, restore_from_checkpoint_path=None, save_last_checkpoint=True, metric_to_monitor_for_checkpoints='val_loss', save_top_k=2, nsys_profiling=False, nsys_start_step=0, nsys_end_step=None, nsys_ranks=[0], config_class=GeneformerConfig, log_every_n_steps=50, gc_interval=0, aligned_megatron_ddp=False)</code>","text":"<p>Train a Geneformer model on single cell data.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Base directory for the data.</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes to run on</p> required <code>devices</code> <code>int</code> <p>number of devices</p> required <code>seq_length</code> <code>int</code> <p>sequence length</p> required <code>result_dir</code> <code>Path</code> <p>directory to store results, logs and checkpoints</p> required <code>num_steps</code> <code>int</code> <p>number of steps to train the model for</p> required <code>limit_val_batches</code> <code>int</code> <p>limit the number of validation global batches to this many</p> required <code>val_check_interval</code> <code>int</code> <p>number of steps to periodically check the validation loss and save</p> required <code>num_dataset_workers</code> <code>int</code> <p>num dataset workers</p> required <code>biobert_spec_option</code> <code>BiobertSpecOption</code> <p>the biobert spec option (architecture) to use for this run</p> required <code>lr</code> <code>float</code> <p>learning rate</p> required <code>micro_batch_size</code> <code>int</code> <p>micro batch size, from this and parallelism settings we infer the global batch size</p> required <code>cosine_rampup_frac</code> <code>float</code> <p>fraction of steps at the beginning of the run to ramp up the learning rate</p> required <code>cosine_hold_frac</code> <code>float</code> <p>fraction of steps to hold the minimum learning rate at the end of the run</p> required <code>experiment_name</code> <code>str</code> <p>experiment name, this is the name used for the wandb run, and the sub-directory of the result_dir that stores the logs and checkpoints.</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>if requested, gradients are only updated every <code>accumulate_grad_batches</code> steps.</p> required <code>config_class</code> <code>Type[BioBertConfig]</code> <p>which model config do you want to train?</p> <code>GeneformerConfig</code> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>which metric do you want to monitor for checkpoints?</p> <code>'val_loss'</code> <code>nemo1_init_path</code> <code>str</code> <p>if you have a nemo1 checkpoint you want to initialize the model weights from, you can provide that. Note that settings are not pulled from the model.</p> <code>None</code> <code>precision</code> <code>str</code> <p>desired training precision</p> required <code>save_last_checkpoint</code> <code>bool</code> <p>if you want the last checkpoint saved</p> <code>True</code> <code>save_top_k</code> <code>int</code> <p>if you want the top k checkpoints all saved.</p> <code>2</code> <code>resume_if_exists</code> <code>bool</code> <p>attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]</p> required <code>wandb_entity</code> <code>str</code> <p>The team posting this run (default: your username or your default team)</p> <code>None</code> <code>wandb_project</code> <code>str</code> <p>The name of the project to which this run will belong.</p> <code>None</code> <code>wandb_tags</code> <code>List[str]</code> <p>Tags associated with this run.</p> <code>None</code> <code>wandb_group</code> <code>str</code> <p>A unique string shared by all runs in a given group</p> <code>None</code> <code>wandb_offline</code> <code>bool</code> <p>Run offline (data can be streamed later to wandb servers).</p> <code>False</code> <code>wandb_id</code> <code>str</code> <p>Sets the version, mainly used to resume a previous run.</p> <code>None</code> <code>wandb_anonymous</code> <code>bool</code> <p>Enables or explicitly disables anonymous logging.</p> <code>False</code> <code>wandb_log_model</code> <code>bool</code> <p>Save checkpoints in wandb dir to upload on W&amp;B servers.</p> <code>False</code> <code>create_tensorboard_logger</code> <code>bool</code> <p>create the tensorboard logger</p> <code>False</code> <code>restore_from_checkpoint_path</code> <code>path</code> <p>If set, restores the model from the directory passed in. Expects the checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.</p> <code>None</code> <code>log_every_n_steps</code> <code>int</code> <p>log at this interval.</p> <code>50</code> <code>nsys_profiling</code> <code>bool</code> <p>Whether to enable the nsys profiling callback hooks. You still need to execute the function with nsys on the command line, but this enables more useful outputs in your nsys profiles, as well as control over which step ranges are captured.</p> <code>False</code> <code>nsys_start_step</code> <code>int</code> <p>Step to start profiling.</p> <code>0</code> <code>nsys_ranks</code> <code>list[int]</code> <p>GPU/node ranks to profile. Defaults to [0] (only main gpu.)</p> <code>[0]</code> <code>nsys_end_step</code> <code>int</code> <p>Step to stop profiling.</p> <code>None</code> <code>gc_interval</code> <code>int</code> <p>if a value &gt; 0 is provided, this will turn off automatic garbage collection and only run at this requested interval of train/val steps. This will likely slow down single GPU runs.</p> <code>0</code> <code>aligned_megatron_ddp</code> <code>bool</code> <p>if activated, this will activate a number of communication optimizations that are good for clusters. This will likely slow down single node runs though.</p> <code>False</code> Source code in <code>bionemo/geneformer/scripts/train_geneformer.py</code> <pre><code>def main(\n    data_dir: Path,\n    num_nodes: int,\n    devices: int,\n    seq_length: int,\n    result_dir: Path,\n    num_steps: int,\n    limit_val_batches: int,\n    val_check_interval: int,\n    num_dataset_workers: int,\n    biobert_spec_option: BiobertSpecOption,\n    lr: float,\n    micro_batch_size: int,\n    accumulate_grad_batches: int,\n    cosine_rampup_frac: float,\n    cosine_hold_frac: float,\n    experiment_name: str,\n    resume_if_exists: bool,\n    precision: PrecisionTypes,\n    wandb_entity: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_offline: bool = False,\n    wandb_tags: Optional[List[str]] = None,\n    wandb_group: Optional[str] = None,\n    wandb_id: Optional[str] = None,\n    wandb_anonymous: Optional[bool] = False,\n    wandb_log_model: bool = False,\n    create_tensorboard_logger: bool = False,\n    nemo1_init_path: Path | None = None,\n    restore_from_checkpoint_path: Path | None = None,\n    save_last_checkpoint: bool = True,\n    metric_to_monitor_for_checkpoints: str = \"val_loss\",\n    save_top_k: int = 2,\n    nsys_profiling: bool = False,\n    nsys_start_step: int = 0,\n    nsys_end_step: Optional[int] = None,\n    nsys_ranks: List[int] = [0],\n    config_class: Type[BioBertConfig] = GeneformerConfig,\n    log_every_n_steps: int = 50,\n    gc_interval: int = 0,\n    aligned_megatron_ddp: bool = False,\n    # TODO add datamodule class, and ability to change data step to get full support for pretraining workflows\n) -&gt; None:\n    \"\"\"Train a Geneformer model on single cell data.\n\n    Args:\n        data_dir (Path): Base directory for the data.\n        num_nodes (int): Number of nodes to run on\n        devices (int): number of devices\n        seq_length (int): sequence length\n        result_dir (Path): directory to store results, logs and checkpoints\n        num_steps (int): number of steps to train the model for\n        limit_val_batches (int): limit the number of validation global batches to this many\n        val_check_interval (int): number of steps to periodically check the validation loss and save\n        num_dataset_workers (int): num dataset workers\n        biobert_spec_option (BiobertSpecOption): the biobert spec option (architecture) to use for this run\n        lr (float): learning rate\n        micro_batch_size (int): micro batch size, from this and parallelism settings we infer the global batch size\n        cosine_rampup_frac (float): fraction of steps at the beginning of the run to ramp up the learning rate\n        cosine_hold_frac (float): fraction of steps to hold the minimum learning rate at the end of the run\n        experiment_name (str): experiment name, this is the name used for the wandb run, and the sub-directory of the\n            result_dir that stores the logs and checkpoints.\n        accumulate_grad_batches (int): if requested, gradients are only updated every `accumulate_grad_batches` steps.\n        config_class (Type[BioBertConfig]): which model config do you want to train?\n        metric_to_monitor_for_checkpoints (str): which metric do you want to monitor for checkpoints?\n        nemo1_init_path (str): if you have a nemo1 checkpoint you want to initialize the model weights from, you can\n            provide that. Note that settings are not pulled from the model.\n        precision (str): desired training precision\n        save_last_checkpoint (bool): if you want the last checkpoint saved\n        save_top_k (int): if you want the top k checkpoints all saved.\n        resume_if_exists (bool): attempt to resume if the checkpoint exists [FIXME @skothenhill this doesn't work yet]\n        wandb_entity (str): The team posting this run (default: your username or your default team)\n        wandb_project (str): The name of the project to which this run will belong.\n        wandb_tags (List[str]): Tags associated with this run.\n        wandb_group (str): A unique string shared by all runs in a given group\n        wandb_offline (bool): Run offline (data can be streamed later to wandb servers).\n        wandb_id (str): Sets the version, mainly used to resume a previous run.\n        wandb_anonymous (bool): Enables or explicitly disables anonymous logging.\n        wandb_log_model (bool): Save checkpoints in wandb dir to upload on W&amp;B servers.\n        create_tensorboard_logger (bool): create the tensorboard logger\n        restore_from_checkpoint_path (path): If set, restores the model from the directory passed in. Expects the\n            checkpoint to be created by using the ModelCheckpoint class and always_save_context=True.\n        log_every_n_steps (int): log at this interval.\n        nsys_profiling (bool): Whether to enable the nsys profiling callback hooks. You still need to execute the\n            function with nsys on the command line, but this enables more useful outputs in your nsys profiles, as\n            well as control over which step ranges are captured.\n        nsys_start_step (int): Step to start profiling.\n        nsys_ranks (list[int]): GPU/node ranks to profile. Defaults to [0] (only main gpu.)\n        nsys_end_step (int): Step to stop profiling.\n        gc_interval (int): if a value &gt; 0 is provided, this will turn off automatic garbage collection and only run\n            at this requested interval of train/val steps. This will likely slow down single GPU runs.\n        aligned_megatron_ddp (bool): if activated, this will activate a number of communication optimizations that are\n            good for clusters. This will likely slow down single node runs though.\n    \"\"\"\n    # Create the result directory if it does not exist.\n    result_dir.mkdir(parents=True, exist_ok=True)\n    val_check_interval = min(val_check_interval, num_steps)  # Training will fail if val_check_interval &gt; num_steps\n\n    # Setup train/test/val data paths\n    train_data_path = data_dir / \"train\"\n    val_data_path = data_dir / \"val\"\n    test_data_path = data_dir / \"test\"\n\n    # Setup the strategy and trainer\n    pipeline_model_parallel_size = 1\n    tensor_model_parallel_size = 1\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=micro_batch_size,\n        num_nodes=num_nodes,\n        devices=devices,\n        accumulate_grad_batches=accumulate_grad_batches,\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n    )\n    if aligned_megatron_ddp:\n        ddp: str | DistributedDataParallelConfig = DistributedDataParallelConfig(\n            check_for_nan_in_grad=True,\n            grad_reduce_in_fp32=False,\n            overlap_grad_reduce=True,\n            overlap_param_gather=True,\n            average_in_collective=True,\n            use_distributed_optimizer=True,  # this should inherit from the optimizer config, but just in case...\n        )\n    else:\n        ddp = \"megatron\"  # this will launch DistributedDataParallelConfig(check_for_nan_in_grad=True).\n\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=tensor_model_parallel_size,\n        pipeline_model_parallel_size=pipeline_model_parallel_size,\n        ddp=ddp,\n        progress_interval=log_every_n_steps,\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n        gradient_as_bucket_view=True,\n        ckpt_async_save=True,\n        ckpt_parallel_load=True,\n    )\n\n    # for wandb integration\n    # Please refer to https://pytorch-lightning.readthedocs.io/en/0.7.6/api/pytorch_lightning.loggers.html\"\n    wandb_options: Optional[WandbConfig] = (\n        None\n        if wandb_project is None\n        else WandbConfig(\n            offline=wandb_offline,\n            project=wandb_project,\n            entity=wandb_entity,\n            tags=wandb_tags,\n            group=wandb_group,\n            id=wandb_id,\n            anonymous=wandb_anonymous,\n            log_model=wandb_log_model,\n        )\n    )\n    callbacks = [\n        # Skip perplexity and disable forward output in the loss for speed\n        RichModelSummary(max_depth=4),\n        TimingCallback(),\n        LearningRateMonitor(),\n    ]\n\n    if gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(gc_interval_train=gc_interval, gc_interval_val=gc_interval)\n        )\n\n    if nsys_profiling:\n        if nsys_end_step is None:\n            nsys_end_step = num_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_start_step, end_step=nsys_end_step, ranks=nsys_ranks, gen_shape=True\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=devices,\n        max_steps=num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=limit_val_batches,  # This controls upsampling and downsampling\n        val_check_interval=val_check_interval,  # TODO(@jstjohn) Checkpoint saving is currently broken, fix and change this.\n        log_every_n_steps=log_every_n_steps,\n        num_nodes=num_nodes,\n        callbacks=callbacks,\n        use_distributed_sampler=False,\n        plugins=nl.MegatronMixedPrecision(precision=precision),\n    )\n\n    preprocessor = GeneformerPreprocess(\n        download_directory=train_data_path,\n        medians_file_path=train_data_path / \"medians.json\",\n        tokenizer_vocab_path=train_data_path / \"geneformer.vocab\",\n    )\n    match preprocessor.preprocess():\n        case {\"tokenizer\": tokenizer, \"median_dict\": median_dict}:\n            logging.info(\"*************** Preprocessing Finished ************\")\n        case _:\n            logging.error(\"Preprocessing failed.\")\n\n    # Configure the data module and model\n    data = SingleCellDataModule(\n        seq_length=seq_length,\n        tokenizer=tokenizer,\n        train_dataset_path=str(train_data_path),\n        val_dataset_path=str(val_data_path),\n        test_dataset_path=str(test_data_path),\n        random_token_prob=0.02,  # changed to represent the incorrect setting we originally used.\n        median_dict=median_dict,\n        micro_batch_size=micro_batch_size,\n        global_batch_size=global_batch_size,\n        # persistent workers is supported when num_dataset_workers &gt; 0\n        persistent_workers=num_dataset_workers &gt; 0,\n        pin_memory=False,\n        num_workers=num_dataset_workers,\n    )\n    geneformer_config = config_class(\n        # TODO let users set different num layers/model shapes here to support bigger/smaller architectures\n        num_layers=6,\n        hidden_size=256,\n        ffn_hidden_size=512,\n        num_attention_heads=4,\n        seq_length=seq_length,\n        params_dtype=get_autocast_dtype(precision),\n        pipeline_dtype=get_autocast_dtype(precision),\n        autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n        biobert_spec_option=biobert_spec_option,\n        nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n        # handle checkpoint resumption here rather than auto-resume so this supports fine-tuning capabilities\n        initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n    )\n\n    # The lightning class owns a copy of the actual model, and a loss function, both of which are configured\n    #  and lazily returned by the `geneformer_config` object defined above.\n    model = biobert_lightning_module(\n        geneformer_config,\n        tokenizer=tokenizer,\n        optimizer=MegatronOptimizerModule(\n            config=OptimizerConfig(\n                lr=lr,\n                # TODO(@jstjohn) try decoupled_lr\n                optimizer=\"adam\",\n                use_distributed_optimizer=True,\n                # Pass through fp16/bf16 settings to avoid errors around model having bf16 enabled but optimizer not.\n                fp16=geneformer_config.fp16,\n                bf16=geneformer_config.bf16,\n            ),\n            lr_scheduler=CosineAnnealingScheduler(\n                max_steps=num_steps,\n                # minimum learning rate is 1/100th of the initial learning rate, so eg lr=1e-3 -&gt; min_lr=1e-5\n                min_lr=lr / 100,\n                warmup_steps=int(math.ceil(num_steps * cosine_rampup_frac)),\n                interval=\"step\",\n                monitor=\"val_loss\",\n                constant_steps=int(math.ceil(num_steps * cosine_hold_frac)),\n            ),\n        ),\n    )\n    # Configure our custom Checkpointer\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=save_last_checkpoint,\n        monitor=metric_to_monitor_for_checkpoints,  # \"val_loss\",\n        save_top_k=save_top_k,\n        every_n_train_steps=val_check_interval,\n        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n    )\n\n    # Setup the logger and train the model\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=result_dir,\n        name=experiment_name,\n        initialize_tensorboard_logger=create_tensorboard_logger,\n        wandb_config=wandb_options,\n        ckpt_callback=checkpoint_callback,\n    )\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            # TODO: uncomment this once nemo2 supports our fine-tuning workflow\n            #  for now this happens inside of our config file in the configure_model step.\n            # path=restore_from_checkpoint_path,\n            resume_if_exists=resume_if_exists,  # Looks for the -last checkpoint to continue training.\n            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n        ),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/","title":"Gene tokenizer","text":""},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer","title":"<code>GeneTokenizer</code>","text":"<p>               Bases: <code>Label2IDTokenizer</code>, <code>IOMixin</code></p> <p>Initializes the GeneTokenizer object.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>class GeneTokenizer(Label2IDTokenizer, io.IOMixin):\n    \"\"\"Initializes the GeneTokenizer object.\"\"\"\n\n    cls_token: str = \"[CLS]\"\n    mask_token: str = \"[MASK]\"\n    pad_token: str = \"[PAD]\"\n    sep_token: str = \"[SEP]\"\n    ukw_token: str = \"[UKW]\"\n    special_tokens: Tuple[str, str, str, str, str] = (cls_token, mask_token, pad_token, sep_token, ukw_token)\n\n    def __init__(self, vocab: Dict[str, int], gene_to_ens: Dict[str, str]):  # noqa: D107\n        # Sets up vocab/decode_vocab dictionaries, parent class is sateful.\n        super().__init__()\n        assert set(self.special_tokens).issubset(\n            set(vocab.keys())\n        ), f\"Vocab must contain all of {self.special_tokens}, missing {set(self.special_tokens) - set(vocab.keys())}\"\n        self.gene_to_ens = deepcopy(gene_to_ens)\n        self.ens_to_gene = {v: k for k, v in self.gene_to_ens.items()}\n        self.vocab = deepcopy(vocab)\n        self.decode_vocab = {v: k for k, v in self.vocab.items()}\n\n    @classmethod\n    def from_medians_and_genes_dicts(cls, median_dict: Dict[str, float], gene_to_ens: Dict[str, str]) -&gt; T:\n        \"\"\"Creates a tokenizer from a median dictionary.\"\"\"\n        tokens = list(cls.special_tokens) + list(median_dict.keys())\n        vocab = cls._build_vocab(tokens)\n        return cls(vocab, gene_to_ens)\n\n    @staticmethod\n    def _build_vocab(strings: Union[List[str], str]) -&gt; Dict[str, int]:\n        \"\"\"We override the parent because complete strings are tokens. Otherwise, has the same behavior.\"\"\"\n        vocab: Dict[str, int] = {}\n        if isinstance(strings, str):\n            strings = [strings]\n\n        for token in strings:\n            if token not in vocab:\n                vocab[token] = len(vocab)\n        return vocab\n\n    def token_to_id(self, token: str) -&gt; int:\n        \"\"\"Converts a token to its corresponding ID.\n\n        Args:\n            token: The token to be converted.\n\n        Returns:\n            The ID corresponding to the token.\n        \"\"\"\n        return self.vocab.get(token)\n\n    @property\n    def pad_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.pad_token)\n\n    @property\n    def mask_token_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.mask_token)\n\n    @property\n    def all_special_ids(self) -&gt; list[int]:  # noqa: D102\n        return [self.token_to_id(tok) for tok in self.special_tokens]\n\n    @property\n    def class_id(self) -&gt; int:  # noqa: D102\n        return self.token_to_id(self.cls_token)\n\n    def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:  # noqa: D102\n        return super().tokens_to_ids(tokens)\n\n    def save_vocab(self, vocab_file: str) -&gt; None:\n        \"\"\"Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.\"\"\"\n        vocab_dir = os.path.dirname(vocab_file)\n        if not os.path.exists(vocab_dir):\n            os.makedirs(vocab_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n\n        to_serialize = {}\n        to_serialize[\"vocab\"] = self.vocab\n        to_serialize[\"gene_to_ens\"] = self.gene_to_ens\n\n        with open(vocab_file, \"w\") as f:\n            json.dump(to_serialize, f)\n\n    @classmethod\n    def from_vocab_file(cls, vocab_file: str) -&gt; None:\n        \"\"\"This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.\"\"\"\n        if not os.path.exists(vocab_file):\n            raise FileNotFoundError(f\"Vocab file {vocab_file} not found, run preprocessing to create it.\")\n\n        with open(vocab_file) as f:\n            to_deserialize = json.load(f)\n            vocab = to_deserialize[\"vocab\"]\n            gene_to_ens = to_deserialize[\"gene_to_ens\"]\n\n        tokenizer = GeneTokenizer(vocab, gene_to_ens)\n        return tokenizer\n\n    def gene_tok_to_ens(self, gene: str) -&gt; str:\n        \"\"\"Converts a gene token to its corresponding Ensembl ID.\n\n        Args:\n            gene (str): The gene token to be converted.\n\n        Returns:\n            str: The Ensembl ID corresponding to the gene token.\n        \"\"\"\n        return self.gene_to_ens[gene]\n\n    def ens_tok_to_gene(self, ens: str) -&gt; str:\n        \"\"\"Converts an Ensembl token to a gene name.\n\n        Args:\n            ens (str): The Ensembl token to be converted.\n\n        Returns:\n            str: The corresponding gene name.\n        \"\"\"\n        return self.ens_to_gene[ens]\n\n    def genes_to_enss(self, genes: List[str]) -&gt; List[str]:\n        \"\"\"Converts a list of gene names to Ensembl IDs.\n\n        Args:\n            genes (List[str]): A list of gene names.\n\n        Returns:\n            List[str]: A list of corresponding Ensembl IDs.\n\n        Raises:\n            ValueError: If a gene name is not found in the gene_to_ens dictionary.\n        \"\"\"\n        ens_ids = []\n        for gene in genes:\n            if gene in self.gene_to_ens:\n                ens_ids.append(self.gene_to_ens[gene])\n            else:\n                raise ValueError(f\"{gene} not found\")\n        return ens_ids\n\n    def enss_to_genes(self, ensemble_ids: List[str]) -&gt; List[str]:\n        \"\"\"Converts a list of ensemble IDs to gene names.\n\n        Args:\n            ensemble_ids (List[str]): A list of ensemble IDs.\n\n        Returns:\n            List[str]: A list of gene names corresponding to the ensemble IDs.\n\n        Raises:\n            ValueError: If an ensemble ID is not found in the mapping.\n        \"\"\"\n        genes = []\n        for ens_id in ensemble_ids:\n            if ens_id in self.ens_to_gene:\n                genes.append(self.ens_to_gene[ens_id])\n            else:\n                raise ValueError(f\"{ens_id} not found\")\n        return genes\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.ens_tok_to_gene","title":"<code>ens_tok_to_gene(ens)</code>","text":"<p>Converts an Ensembl token to a gene name.</p> <p>Parameters:</p> Name Type Description Default <code>ens</code> <code>str</code> <p>The Ensembl token to be converted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The corresponding gene name.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def ens_tok_to_gene(self, ens: str) -&gt; str:\n    \"\"\"Converts an Ensembl token to a gene name.\n\n    Args:\n        ens (str): The Ensembl token to be converted.\n\n    Returns:\n        str: The corresponding gene name.\n    \"\"\"\n    return self.ens_to_gene[ens]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.enss_to_genes","title":"<code>enss_to_genes(ensemble_ids)</code>","text":"<p>Converts a list of ensemble IDs to gene names.</p> <p>Parameters:</p> Name Type Description Default <code>ensemble_ids</code> <code>List[str]</code> <p>A list of ensemble IDs.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of gene names corresponding to the ensemble IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an ensemble ID is not found in the mapping.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def enss_to_genes(self, ensemble_ids: List[str]) -&gt; List[str]:\n    \"\"\"Converts a list of ensemble IDs to gene names.\n\n    Args:\n        ensemble_ids (List[str]): A list of ensemble IDs.\n\n    Returns:\n        List[str]: A list of gene names corresponding to the ensemble IDs.\n\n    Raises:\n        ValueError: If an ensemble ID is not found in the mapping.\n    \"\"\"\n    genes = []\n    for ens_id in ensemble_ids:\n        if ens_id in self.ens_to_gene:\n            genes.append(self.ens_to_gene[ens_id])\n        else:\n            raise ValueError(f\"{ens_id} not found\")\n    return genes\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.from_medians_and_genes_dicts","title":"<code>from_medians_and_genes_dicts(median_dict, gene_to_ens)</code>  <code>classmethod</code>","text":"<p>Creates a tokenizer from a median dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>@classmethod\ndef from_medians_and_genes_dicts(cls, median_dict: Dict[str, float], gene_to_ens: Dict[str, str]) -&gt; T:\n    \"\"\"Creates a tokenizer from a median dictionary.\"\"\"\n    tokens = list(cls.special_tokens) + list(median_dict.keys())\n    vocab = cls._build_vocab(tokens)\n    return cls(vocab, gene_to_ens)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.from_vocab_file","title":"<code>from_vocab_file(vocab_file)</code>  <code>classmethod</code>","text":"<p>This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>@classmethod\ndef from_vocab_file(cls, vocab_file: str) -&gt; None:\n    \"\"\"This method adds a layer on the constructor in the case we are working from a filename instead of a dictionary.\"\"\"\n    if not os.path.exists(vocab_file):\n        raise FileNotFoundError(f\"Vocab file {vocab_file} not found, run preprocessing to create it.\")\n\n    with open(vocab_file) as f:\n        to_deserialize = json.load(f)\n        vocab = to_deserialize[\"vocab\"]\n        gene_to_ens = to_deserialize[\"gene_to_ens\"]\n\n    tokenizer = GeneTokenizer(vocab, gene_to_ens)\n    return tokenizer\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.gene_tok_to_ens","title":"<code>gene_tok_to_ens(gene)</code>","text":"<p>Converts a gene token to its corresponding Ensembl ID.</p> <p>Parameters:</p> Name Type Description Default <code>gene</code> <code>str</code> <p>The gene token to be converted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Ensembl ID corresponding to the gene token.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def gene_tok_to_ens(self, gene: str) -&gt; str:\n    \"\"\"Converts a gene token to its corresponding Ensembl ID.\n\n    Args:\n        gene (str): The gene token to be converted.\n\n    Returns:\n        str: The Ensembl ID corresponding to the gene token.\n    \"\"\"\n    return self.gene_to_ens[gene]\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.genes_to_enss","title":"<code>genes_to_enss(genes)</code>","text":"<p>Converts a list of gene names to Ensembl IDs.</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <code>List[str]</code> <p>A list of gene names.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of corresponding Ensembl IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a gene name is not found in the gene_to_ens dictionary.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def genes_to_enss(self, genes: List[str]) -&gt; List[str]:\n    \"\"\"Converts a list of gene names to Ensembl IDs.\n\n    Args:\n        genes (List[str]): A list of gene names.\n\n    Returns:\n        List[str]: A list of corresponding Ensembl IDs.\n\n    Raises:\n        ValueError: If a gene name is not found in the gene_to_ens dictionary.\n    \"\"\"\n    ens_ids = []\n    for gene in genes:\n        if gene in self.gene_to_ens:\n            ens_ids.append(self.gene_to_ens[gene])\n        else:\n            raise ValueError(f\"{gene} not found\")\n    return ens_ids\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.save_vocab","title":"<code>save_vocab(vocab_file)</code>","text":"<p>Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def save_vocab(self, vocab_file: str) -&gt; None:\n    \"\"\"Saves the vocabulary as a newline delimieted vocabulary file, each line represents an int -&gt; token mapping. line number is assumed to be the integer.\"\"\"\n    vocab_dir = os.path.dirname(vocab_file)\n    if not os.path.exists(vocab_dir):\n        os.makedirs(vocab_dir, exist_ok=True)  # ensure the dir exists but be ok with race conditions.\n\n    to_serialize = {}\n    to_serialize[\"vocab\"] = self.vocab\n    to_serialize[\"gene_to_ens\"] = self.gene_to_ens\n\n    with open(vocab_file, \"w\") as f:\n        json.dump(to_serialize, f)\n</code></pre>"},{"location":"API_reference/bionemo/geneformer/tokenizer/gene_tokenizer/#bionemo.geneformer.tokenizer.gene_tokenizer.GeneTokenizer.token_to_id","title":"<code>token_to_id(token)</code>","text":"<p>Converts a token to its corresponding ID.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The token to be converted.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The ID corresponding to the token.</p> Source code in <code>bionemo/geneformer/tokenizer/gene_tokenizer.py</code> <pre><code>def token_to_id(self, token: str) -&gt; int:\n    \"\"\"Converts a token to its corresponding ID.\n\n    Args:\n        token: The token to be converted.\n\n    Returns:\n        The ID corresponding to the token.\n    \"\"\"\n    return self.vocab.get(token)\n</code></pre>"},{"location":"API_reference/bionemo/llm/api/","title":"Api","text":""},{"location":"API_reference/bionemo/llm/api/#bionemo.llm.api.BionemoMegatronModel","title":"<code>BionemoMegatronModel</code>","text":"<p>               Bases: <code>MegatronModule</code>, <code>Generic[DataT]</code>, <code>ABC</code></p> <p>Models that use Megatron must be a MegatronModule type.</p> <p>The only major difference is the explicit <code>forward</code> pass method signature that makes this class compatible with bionemo-core's <code>Model</code> structural type.</p> Source code in <code>bionemo/llm/api.py</code> <pre><code>class BionemoMegatronModel(MegatronModule, Generic[DataT], ABC):\n    \"\"\"Models that use Megatron must be a MegatronModule type.\n\n    The only major difference is the explicit `forward` pass method signature that makes this class compatible\n    with bionemo-core's `Model` structural type.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; DataT:  # noqa: D102\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.DataStep","title":"<code>DataStep = Callable[[Iterator[DataT]], DataT]</code>  <code>module-attribute</code>","text":"<p>Batches together an iterator of individual examples.</p> <p>Necessary for compatability with Megatron. This function type is similiar to the collate function of PyTorch.</p> <p>A <code>DataStep</code> function takes an iterator over individual examples. Each example may be a tensor, sequence of tensors, or a set of named tensors (provided as a <code>dict</code> mapping <code>str</code> names to each <code>Tensor</code>). Each iteration must yield the same type.</p> <p>The output of this function will mirror the same structure of each yielded example. It will be a concatenation of all of the examples in the iterator.</p>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.ForwardStep","title":"<code>ForwardStep = Callable[[MegatronModelType, DataT], DataT]</code>  <code>module-attribute</code>","text":"<p>Megatron-compatible forward pass function.</p>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule","title":"<code>BionemoLightningModule</code>","text":"<p>               Bases: <code>Generic[MegatronModelType, MegatronLossType]</code>, <code>LightningModule</code>, <code>IOMixin</code>, <code>ConnectorMixin</code>, <code>LightningPassthroughPredictionMixin</code></p> <p>Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class BionemoLightningModule(\n    Generic[MegatronModelType, MegatronLossType],\n    pl.LightningModule,\n    nlio.IOMixin,\n    nlio.ConnectorMixin,\n    LightningPassthroughPredictionMixin,\n):\n    \"\"\"Reusable PyTorch Lightning module for Megatron models that is compatible with NeMo's conventions.\"\"\"\n\n    def __init__(\n        self,\n        config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n        forward_step: ForwardStep,\n        data_step: DataStep,\n        # TODO: Add transformer_layer_spec when we update mcore\n        optimizer: MegatronOptimizerModule,\n        model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n        **model_construct_args,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            config: Serializable configuration object that allows one to construct a new model instance and loss\n                function. Necessary for Megatron-based training as the model itself cannot be serialized and\n                distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n            forward_step: Performs forward pass using the model and a batch of data.\n            data_step: Custom batch-creating function for the model.\n            optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n                rate.\n            model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n                `configure_model` method.\n            model_transform: Optional. The model transform function.\n            **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n                `configure_model` method, which will make an instance of the model.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n        # ***must** be set up in configure_model() -- megatron constraint\n        # also, must be called `module`: nemo expects the actual model to be stored this way\n        self.module: Optional[MegatronModelType] = None\n        self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n        self.optim = optimizer\n        self.optim.connect(self)  # This will bind the `configure_optimizers` method\n        self._data_step = data_step\n        self._forward_step = forward_step\n        self.model_transform = model_transform\n\n    def configure_model(self) -&gt; None:\n        \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n        NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n        Raises:\n            ValueError iff the internal config's configure_model method returns None.\n        \"\"\"\n        if self.module is None:\n            model: MegatronModelType = (\n                self.config.configure_model(**self.module_construct_args)\n                if self.module_construct_args is not None\n                else self.config.configure_model()\n            )\n            self.module = model\n        if self.module is None:\n            raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n\n    def forward(self, *args, **kwargs) -&gt; DataT:\n        \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n        prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n        return prediction\n\n    def data_step(self, dataloader_iter: Iterator[DataT]) -&gt; DataT:  # noqa: D102\n        return self._data_step(dataloader_iter)\n\n    def forward_step(self, batch) -&gt; Tensor:\n        \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n        Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n        from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n        and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n        https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n        To get actual predictions, use the :func:`forward` method instead.\n        \"\"\"\n        # safe to do because configure_model is idempotent\n        self.configure_model()\n        assert self.module is not None\n        return self._forward_step(self.module, batch)\n\n    def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        return self.forward_step(batch)\n\n    def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n        return self.forward_step(batch)\n\n    def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n        \"\"\"Alias for forward_step.\"\"\"\n        return self.forward_step(batch)\n\n    def training_loss_reduction(self) -&gt; MegatronLossType:\n        \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n        return self.loss_reduction_class()\n\n    def validation_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n\n    def test_loss_reduction(self) -&gt; MegatronLossType:  # noqa: D102\n        return self.loss_reduction_class(validation_step=True)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.__init__","title":"<code>__init__(config, forward_step, data_step, optimizer, model_transform=None, **model_construct_args)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code> <p>Serializable configuration object that allows one to construct a new model instance and loss function. Necessary for Megatron-based training as the model itself cannot be serialized and distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.</p> required <code>forward_step</code> <code>ForwardStep</code> <p>Performs forward pass using the model and a batch of data.</p> required <code>data_step</code> <code>DataStep</code> <p>Custom batch-creating function for the model.</p> required <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning rate.</p> required <code>model_construct_args</code> <p>Optional. Any arguments necessary to construct the model in the <code>config</code>'s <code>configure_model</code> method.</p> <code>{}</code> <code>model_transform</code> <code>Optional[Callable[[MegatronModelType], MegatronModelType]]</code> <p>Optional. The model transform function.</p> <code>None</code> <code>**model_construct_args</code> <p>Optional. Arguments necessary for the supplied model configuration's <code>configure_model</code> method, which will make an instance of the model.</p> <code>{}</code> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def __init__(\n    self,\n    config: BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    forward_step: ForwardStep,\n    data_step: DataStep,\n    # TODO: Add transformer_layer_spec when we update mcore\n    optimizer: MegatronOptimizerModule,\n    model_transform: Optional[Callable[[MegatronModelType], MegatronModelType]] = None,\n    **model_construct_args,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        config: Serializable configuration object that allows one to construct a new model instance and loss\n            function. Necessary for Megatron-based training as the model itself cannot be serialized and\n            distributed to nodes. Instead, we serialize the procedure for making the model and distribute that.\n        forward_step: Performs forward pass using the model and a batch of data.\n        data_step: Custom batch-creating function for the model.\n        optimizer: Megatron-compatible distributed optimizer instance. Defaults to using ADAM with a 1e-4 learning\n            rate.\n        model_construct_args: Optional. Any arguments necessary to construct the model in the `config`'s\n            `configure_model` method.\n        model_transform: Optional. The model transform function.\n        **model_construct_args: Optional. Arguments necessary for the supplied model configuration's\n            `configure_model` method, which will make an instance of the model.\n    \"\"\"\n    super().__init__()\n    self.config = config\n    self.module_construct_args: Optional[dict[str, Any]] = model_construct_args\n    # ***must** be set up in configure_model() -- megatron constraint\n    # also, must be called `module`: nemo expects the actual model to be stored this way\n    self.module: Optional[MegatronModelType] = None\n    self.loss_reduction_class: type[MegatronLossType] = config.get_loss_reduction_class()\n    self.optim = optimizer\n    self.optim.connect(self)  # This will bind the `configure_optimizers` method\n    self._data_step = data_step\n    self._forward_step = forward_step\n    self.model_transform = model_transform\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.configure_model","title":"<code>configure_model()</code>","text":"<p>Updates internal state: instantiates the model from the object's config, assigns to <code>model</code> attribute.</p> <p>NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def configure_model(self) -&gt; None:\n    \"\"\"Updates internal state: instantiates the model from the object's config, assigns to `model` attribute.\n\n    NOTE: this method is idempotent; successive calls have no effect. The model is only initialized once.\n\n    Raises:\n        ValueError iff the internal config's configure_model method returns None.\n    \"\"\"\n    if self.module is None:\n        model: MegatronModelType = (\n            self.config.configure_model(**self.module_construct_args)\n            if self.module_construct_args is not None\n            else self.config.configure_model()\n        )\n        self.module = model\n    if self.module is None:\n        raise ValueError(\"Invalid semantics: configure_model method **MUST** initialize the model.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Call the forward method of the underlying model, and return whatever it outputs.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; DataT:\n    \"\"\"Call the forward method of the underlying model, and return whatever it outputs.\"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None, \"ERROR: configure_model() method has been incorrectly overridden!\"\n    prediction = self.module(*args, **kwargs)  # for now just pass through to the underlying model\n    return prediction\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.forward_step","title":"<code>forward_step(batch)</code>","text":"<p>Megatron-required: the training forward step for the model, which is required to produce the loss.</p> <p>Normally, the forward pass of a model means its inference. Loss is computed using the predictions from the forward pass against labels. Megatron unfortunately conflates these two different concepts and instead has models \"forward\" method produce the loss. See the Megatron docs for details: https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170</p> <p>To get actual predictions, use the :func:<code>forward</code> method instead.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward_step(self, batch) -&gt; Tensor:\n    \"\"\"Megatron-required: the training forward step for the model, which is required to produce the loss.\n\n    Normally, the forward pass of a model means its inference. Loss is computed using the predictions\n    from the forward pass against labels. Megatron unfortunately conflates these two different concepts\n    and instead has models \"forward\" method produce the loss. See the Megatron docs for details:\n    https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/pipeline_parallel/schedules.py#L170\n\n    To get actual predictions, use the :func:`forward` method instead.\n    \"\"\"\n    # safe to do because configure_model is idempotent\n    self.configure_model()\n    assert self.module is not None\n    return self._forward_step(self.module, batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.predict_step","title":"<code>predict_step(batch, batch_idx=None)</code>","text":"<p>Alias for forward_step.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"Alias for forward_step.\"\"\"\n    return self.forward_step(batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_loss_reduction","title":"<code>training_loss_reduction()</code>","text":"<p>This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_loss_reduction(self) -&gt; MegatronLossType:\n    \"\"\"This is the function that takes batch['loss_mask'] and the logits output by the model and reduces the loss.\"\"\"\n    return self.loss_reduction_class()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.training_step","title":"<code>training_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def training_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    return self.forward_step(batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.BionemoLightningModule.validation_step","title":"<code>validation_step(batch, batch_idx=None)</code>","text":"<p>In mcore the loss-function is part of the forward-pass when labels are provided.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def validation_step(self, batch, batch_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"In mcore the loss-function is part of the forward-pass when labels are provided.\"\"\"\n    return self.forward_step(batch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin","title":"<code>LightningPassthroughPredictionMixin</code>","text":"<p>A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class LightningPassthroughPredictionMixin:\n    \"\"\"A mixin that allows your model to do inference on the predict step by hijacking nemo's loss reduction mechanism.\"\"\"\n\n    def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n        \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n        return PassthroughLossReduction()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.LightningPassthroughPredictionMixin.predict_loss_reduction","title":"<code>predict_loss_reduction()</code>","text":"<p>For the predict step, pass through the forward pass output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def predict_loss_reduction(self) -&gt; PassthroughLossReduction:\n    \"\"\"For the predict step, pass through the forward pass output.\"\"\"\n    return PassthroughLossReduction()\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction","title":"<code>PassthroughLossReduction</code>","text":"<p>               Bases: <code>MegatronLossReduction</code>, <code>Generic[DataT]</code></p> <p>A workaround for nemo/megatron to perform inference.</p> <p>Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of tensors. The inner type must always be a Tensor.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class PassthroughLossReduction(MegatronLossReduction, Generic[DataT]):\n    \"\"\"A workaround for nemo/megatron to perform inference.\n\n    Internally in NeMo2.0 the forward step is always expected to return a loss reduction class, and forward is\n    expected to return a loss. This class hijacks that mechanism to instead pass through the forward output unperturbed\n    as the loss (to enable inference in the predict step), and then the reduce method is used to collate the batch of\n    forward outputs into a single batch. This supports the model forward output being a tensor, dict, tuple, or list of\n    tensors. The inner type _must always be a Tensor_.\n    \"\"\"\n\n    def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n        \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n        Args:\n            batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n            forward_out: The output from your model's forward pass.\n\n        Returns:\n            A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n        \"\"\"\n        dtype, device = get_dtype_device(forward_out)\n        return torch.zeros(1, device=device, dtype=dtype), forward_out\n\n    def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n        \"\"\"Collates list of model's outputs into a single output.\"\"\"\n        return batch_collator(forward_out)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Passes through the <code>forward_out</code> value as the 2nd tuple element.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>DataT</code> <p>The batch of data that was passed through the model to generate output. NOTE: this value is ignored.</p> required <code>forward_out</code> <code>DataT</code> <p>The output from your model's forward pass.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, DataT]</code> <p>A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def forward(self, batch: DataT, forward_out: DataT) -&gt; Tuple[Tensor, DataT]:\n    \"\"\"Passes through the `forward_out` value as the 2nd tuple element.\n\n    Args:\n        batch: The batch of data that was passed through the model to generate output. NOTE: this value is ignored.\n        forward_out: The output from your model's forward pass.\n\n    Returns:\n        A tuple containing the loss tensor (dummy in this case) and the forward output (unmodified).\n    \"\"\"\n    dtype, device = get_dtype_device(forward_out)\n    return torch.zeros(1, device=device, dtype=dtype), forward_out\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PassthroughLossReduction.reduce","title":"<code>reduce(forward_out)</code>","text":"<p>Collates list of model's outputs into a single output.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def reduce(self, forward_out: List[DataT]) -&gt; DataT:\n    \"\"\"Collates list of model's outputs into a single output.\"\"\"\n    return batch_collator(forward_out)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PerplexityLoggingCallback","title":"<code>PerplexityLoggingCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code></p> <p>Megatron Callback to log perplexity in validation and optionally training.</p> <p>NeMo2.0 checks whether a callback is an instance of {LightningModule,LightningDataModule,Callback} but only megatron_hooks are useful.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>class PerplexityLoggingCallback(pl.Callback, CallbackMethods):\n    \"\"\"Megatron Callback to log perplexity in validation and optionally training.\n\n    NeMo2.0 checks whether a callback is an instance of {LightningModule,LightningDataModule,Callback} but only megatron_hooks are useful.\n    \"\"\"\n\n    def __init__(self, log_train: bool = False, log_val: bool = True):\n        \"\"\"Initialize PerplexityLoggingCallback.\n\n        Args:\n            log_train: whether to log train perplexity. Defaults to False.\n            log_val: whether to log validation perplexity. Defaults to True.\n        \"\"\"\n        super().__init__()\n        self.log_train = log_train\n        self.log_val = log_val\n\n    def _pad_to_max_length(\n        self,\n        microbatch_outputs: List[Dict[str, Dict[str, Tensor]]],\n        key1: str,\n        key2: str,\n        pad_value: int = 0,\n        seq_dim: int = 1,\n        batch_dim: int = 0,\n    ) -&gt; Tensor:\n        \"\"\"Pad tensors to max length in microbatch_outputs.\"\"\"\n        assert seq_dim != batch_dim, \"Forgot to set one of seq_dim, batch_dim, they are equal!\"\n        max_sequence_length: int = max(output[key1][key2].shape[seq_dim] for output in microbatch_outputs)\n\n        tensors: List[Tensor] = []\n        for microbatch_output in microbatch_outputs:\n            tensor = microbatch_output[key1][key2]\n            assert (\n                tensor.dim() &gt;= 2\n            ), f\"Tensor in microbatch_outputs must have at least 2 dimensions, but got {tensor.dim()} dimensions\"\n            pad_size = [(0, 0)] * tensor.dim()\n            pad_size[seq_dim] = (0, max_sequence_length - tensor.shape[seq_dim])\n            # Flatten pad size list for F.pad\n            pad_size_flat = [item for sublist in reversed(pad_size) for item in sublist]\n            tensors.append(\n                torch.nn.functional.pad(  # padding reverse in order\n                    tensor,\n                    pad_size_flat,\n                    mode=\"constant\",\n                    value=pad_value,\n                )\n            )\n\n        return torch.cat(tensors, dim=batch_dim)  # concat on batch dim\n\n    @override\n    def on_megatron_reduce_microbatches_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        loss_reduction: MegatronLossReduction,\n        reduced: Tensor | dict[str, Tensor],\n    ) -&gt; None:\n        \"\"\"Log after MegatronReductionLoss.reduce is called.\n\n        Expected microbatch_outputs to be a list of dicts with the following keys:\n            - batch: dict of tensors with the following keys:\n                - labels: [b s]\n                - loss_mask: [b s]; 1 means included 0 means ignored\n            - forward_out: dict of tensors with the following keys:\n                - token_logits: [b s vocab]\n        \"\"\"\n        if step.trainer.sanity_checking:  # skip sanity check\n            return\n\n        if step.trainer.training and not self.log_train:\n            return\n\n        if not parallel_state.is_pipeline_last_stage():\n            return\n\n        assert step.num_microbatches is not None, \"num_microbatches must be initialized to non-None\"\n        assert step.num_microbatches &gt; 0, \"num_microbatches must be greater than 0\"\n        assert (\n            len(microbatch_outputs) == step.num_microbatches\n        ), \"microbatch_outputs length does not match num_microbatches\"\n        labels = self._pad_to_max_length(microbatch_outputs, \"batch\", \"labels\", pad_value=-100)\n        loss_mask = self._pad_to_max_length(microbatch_outputs, \"batch\", \"loss_mask\")\n        token_logits = self._pad_to_max_length(\n            microbatch_outputs, \"forward_out\", \"token_logits\", seq_dim=0, batch_dim=1\n        )\n\n        unreduced_token_loss = unreduced_token_loss_fn(\n            token_logits.clone(),  # [s,b] as expected unreduced_token_loss_fn has inplace operation on token_logits\n            labels.clone(),  # [b,s] as expected\n        )  # [b s] is the return\n\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            ppl = torch.exp((unreduced_token_loss * loss_mask).sum() / loss_mask.sum())\n        else:\n            raise NotImplementedError(\"Context parallel perplexity logging is not supported yet\")\n\n        if self.log_val and not step.trainer.training:\n            step.pl_module.log(\"val_ppl\", ppl, prog_bar=True, on_epoch=True)\n        elif self.log_train and step.trainer.training:\n            step.pl_module.log(\"train_ppl\", ppl, prog_bar=True, batch_size=1, sync_dist=False)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PerplexityLoggingCallback.__init__","title":"<code>__init__(log_train=False, log_val=True)</code>","text":"<p>Initialize PerplexityLoggingCallback.</p> <p>Parameters:</p> Name Type Description Default <code>log_train</code> <code>bool</code> <p>whether to log train perplexity. Defaults to False.</p> <code>False</code> <code>log_val</code> <code>bool</code> <p>whether to log validation perplexity. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def __init__(self, log_train: bool = False, log_val: bool = True):\n    \"\"\"Initialize PerplexityLoggingCallback.\n\n    Args:\n        log_train: whether to log train perplexity. Defaults to False.\n        log_val: whether to log validation perplexity. Defaults to True.\n    \"\"\"\n    super().__init__()\n    self.log_train = log_train\n    self.log_val = log_val\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.PerplexityLoggingCallback.on_megatron_reduce_microbatches_end","title":"<code>on_megatron_reduce_microbatches_end(step, microbatch_outputs, loss_reduction, reduced)</code>","text":"<p>Log after MegatronReductionLoss.reduce is called.</p> Expected microbatch_outputs to be a list of dicts with the following keys <ul> <li>batch: dict of tensors with the following keys:<ul> <li>labels: [b s]</li> <li>loss_mask: [b s]; 1 means included 0 means ignored</li> </ul> </li> <li>forward_out: dict of tensors with the following keys:<ul> <li>token_logits: [b s vocab]</li> </ul> </li> </ul> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>@override\ndef on_megatron_reduce_microbatches_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    loss_reduction: MegatronLossReduction,\n    reduced: Tensor | dict[str, Tensor],\n) -&gt; None:\n    \"\"\"Log after MegatronReductionLoss.reduce is called.\n\n    Expected microbatch_outputs to be a list of dicts with the following keys:\n        - batch: dict of tensors with the following keys:\n            - labels: [b s]\n            - loss_mask: [b s]; 1 means included 0 means ignored\n        - forward_out: dict of tensors with the following keys:\n            - token_logits: [b s vocab]\n    \"\"\"\n    if step.trainer.sanity_checking:  # skip sanity check\n        return\n\n    if step.trainer.training and not self.log_train:\n        return\n\n    if not parallel_state.is_pipeline_last_stage():\n        return\n\n    assert step.num_microbatches is not None, \"num_microbatches must be initialized to non-None\"\n    assert step.num_microbatches &gt; 0, \"num_microbatches must be greater than 0\"\n    assert (\n        len(microbatch_outputs) == step.num_microbatches\n    ), \"microbatch_outputs length does not match num_microbatches\"\n    labels = self._pad_to_max_length(microbatch_outputs, \"batch\", \"labels\", pad_value=-100)\n    loss_mask = self._pad_to_max_length(microbatch_outputs, \"batch\", \"loss_mask\")\n    token_logits = self._pad_to_max_length(\n        microbatch_outputs, \"forward_out\", \"token_logits\", seq_dim=0, batch_dim=1\n    )\n\n    unreduced_token_loss = unreduced_token_loss_fn(\n        token_logits.clone(),  # [s,b] as expected unreduced_token_loss_fn has inplace operation on token_logits\n        labels.clone(),  # [b,s] as expected\n    )  # [b s] is the return\n\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        ppl = torch.exp((unreduced_token_loss * loss_mask).sum() / loss_mask.sum())\n    else:\n        raise NotImplementedError(\"Context parallel perplexity logging is not supported yet\")\n\n    if self.log_val and not step.trainer.training:\n        step.pl_module.log(\"val_ppl\", ppl, prog_bar=True, on_epoch=True)\n    elif self.log_train and step.trainer.training:\n        step.pl_module.log(\"train_ppl\", ppl, prog_bar=True, batch_size=1, sync_dist=False)\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.batch_collator","title":"<code>batch_collator(batches, batch_dim=0, batch_dim_key_defaults={'token_logits': 1})</code>","text":"<p>Takes a sequence of batches and collates them into a single batch.</p> <pre><code>This is distinct from the standard pytorch default_collator since it does\nnot add the batch dimension, it's assumed the batch\ndimension is already present in the input, as would be the case when\nparallelizing across minibatches.\n</code></pre> <p>IMPORTANT: The underlying data primitive must be a torch Tensor. The input to this function is a recurisve type, there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.</p> <p>Examples:</p> <p>Outer container = Dict:     [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])} Outer container = List:     [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])] Outer container = Tuple:     ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Optional[Sequence[ReductionT]]</code> <p>sequence of batches to collate into a single batch.</p> required <code>batch_dim</code> <code>int</code> <p>If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for example it is sequence first) then supply that dimension.</p> <code>0</code> <code>batch_dim_key_defaults</code> <code>dictionary of keys to integers</code> <p>If your batch is a dictionary and you know that some keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1 and otherwise all keys are assumed to have batch dim 0.</p> <code>{'token_logits': 1}</code> <p>Returns:</p> Type Description <code>Optional[ReductionT]</code> <p>A single batch of the same type as the elements of your input sequence.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def batch_collator(\n    batches: Optional[Union[Tuple[ReductionT], List[ReductionT]]],\n    batch_dim: int = 0,\n    batch_dim_key_defaults: dict[str, int] = {\"token_logits\": 1},\n) -&gt; Optional[ReductionT]:\n    \"\"\"Takes a sequence of batches and collates them into a single batch.\n\n        This is distinct from the standard pytorch default_collator since it does\n        not add the batch dimension, it's assumed the batch\n        dimension is already present in the input, as would be the case when\n        parallelizing across minibatches.\n\n    IMPORTANT: The underlying data primitive _must_ be a torch Tensor. The input to this function is a recurisve type,\n    there can be any amount of nesting between dictionaries, tuples, and lists, as long as the inner type is a n-d Tensor.\n\n    Examples:\n        Outer container = Dict:\n            [{'a': Tensor([1]), 'b': Tensor([2])}, {'a': Tensor([2]), 'b': Tensor([3])}] -&gt; {'a': Tensor([1, 2]), 'b': Tensor([2, 3])}\n        Outer container = List:\n            [[Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]] -&gt; [Tensor([1, 2]), Tensor([2, 3])]\n        Outer container = Tuple:\n            ([Tensor([1]), Tensor([2])], [Tensor([2]), Tensor([3])]) -&gt; (Tensor([1, 2]), Tensor([2, 3]))\n\n    Args:\n        batches (Optional[Sequence[ReductionT]]): sequence of batches to collate into a single batch.\n        batch_dim: If you know that the batch dim for the batch you are concatenating is not the 0th dimension (for\n            example it is sequence first) then supply that dimension.\n        batch_dim_key_defaults (dictionary of keys to integers): If your batch is a dictionary and you know that some\n            keys have non-standard (0) batch dimensions, supply those here. By default \"token_logits\" has batch dim 1\n            and otherwise all keys are assumed to have batch dim 0.\n\n    Returns:\n        A single batch of the same type as the elements of your input sequence.\n    \"\"\"\n    match batches:\n        # Handle base-cases for batch concatenation, either a list of None or a list of tensors\n        case [None, *_]:\n            return None\n        case [Tensor(), *_]:\n            return torch.cat(batches, dim=batch_dim)\n        # Next 3 calls are the recursive calls into the sub-structures of the batch. We handle dictionaries, tuples, and lists\n        case [dict(), *_]:\n            return {\n                key: batch_collator(\n                    [batch[key] for batch in batches],\n                    batch_dim=batch_dim_key_defaults.get(key, 0),\n                    batch_dim_key_defaults=batch_dim_key_defaults,\n                )\n                for key in batches[0]\n            }\n        case [tuple(), *_]:\n            return tuple(\n                batch_collator(\n                    [batch[i] for batch in batches], batch_dim=batch_dim, batch_dim_key_defaults=batch_dim_key_defaults\n                )\n                for i in range(len(batches[0]))\n            )\n        case [list(), *_]:\n            return [\n                batch_collator(\n                    [batch[i] for batch in batches], batch_dim=batch_dim, batch_dim_key_defaults=batch_dim_key_defaults\n                )\n                for i in range(len(batches[0]))\n            ]\n        # Final cases shouldn't happen, an empty sequence (no batches), or \"other\".\n        case []:\n            raise ValueError(\"Cannot process an empty sequence\")\n        case _:\n            raise ValueError(\"Unsupported input structure in batch_collator\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.default_megatron_optimizer","title":"<code>default_megatron_optimizer()</code>","text":"<p>Default distributed optimizer uses Adam with a 1e-4 learning rate.</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def default_megatron_optimizer() -&gt; MegatronOptimizerModule:\n    \"\"\"Default distributed optimizer uses Adam with a 1e-4 learning rate.\"\"\"\n    return MegatronOptimizerModule(\n        config=OptimizerConfig(lr=1e-4, optimizer=\"adam\", use_distributed_optimizer=True),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/lightning/#bionemo.llm.lightning.some_first","title":"<code>some_first(seq)</code>","text":"<p>Returns the first non-None value from the sequence or fails</p> Source code in <code>bionemo/llm/lightning.py</code> <pre><code>def some_first(seq: Iterable[Optional[T]]) -&gt; T:\n    \"\"\"Returns the first non-None value from the sequence or fails\"\"\"  # noqa: D415\n    for s in seq:\n        if s is not None:\n            return s\n    raise ValueError(\"non-None value not found\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/","title":"Train","text":""},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.NsysConfig","title":"<code>NsysConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for nsys profiling.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>class NsysConfig(BaseModel):\n    \"\"\"Configuration for nsys profiling.\"\"\"\n\n    start_step: int = 0\n    end_step: Optional[int] = None\n    ranks: list[int] = field(default_factory=lambda: [0])\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.nemo_logger_factory","title":"<code>nemo_logger_factory(experiment_config, wandb_config)</code>","text":"<p>Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration object containing experiment settings such as result directory, experiment name, checkpoint settings, and logger preferences.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Optional configuration object for Weights and Biases logging.</p> required <p>Returns:</p> Type Description <code>NeMoLogger</code> <p>nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def nemo_logger_factory(experiment_config: ExperimentConfig, wandb_config: Optional[WandbConfig]) -&gt; nl.NeMoLogger:\n    \"\"\"Creates and returns a NeMoLogger instance configured based on the provided experiment and wandb configurations.\n\n    Args:\n        experiment_config (ExperimentConfig): Configuration object containing experiment settings such as\n            result directory, experiment name, checkpoint settings, and logger preferences.\n        wandb_config (Optional[WandbConfig]): Optional configuration object for Weights and Biases logging.\n\n    Returns:\n        nl.NeMoLogger: An instance of NeMoLogger configured with the specified settings.\n    \"\"\"\n    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n        save_last=experiment_config.save_last_checkpoint,\n        monitor=experiment_config.metric_to_monitor_for_checkpoints,\n        save_top_k=experiment_config.save_top_k,\n        every_n_train_steps=experiment_config.save_every_n_steps,\n        always_save_context=True,\n    )\n\n    nemo_logger = setup_nemo_lightning_logger(\n        root_dir=experiment_config.result_dir,\n        name=experiment_config.experiment_name,\n        initialize_tensorboard_logger=experiment_config.create_tensorboard_logger,\n        wandb_config=wandb_config,\n        ckpt_callback=checkpoint_callback,\n    )\n    return nemo_logger\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.setup_trainer","title":"<code>setup_trainer(parallel_config, training_config, callbacks=None, nsys_config=None)</code>","text":"<p>Set up the trainer for model training using the specified parallel and training configurations.</p> <p>Parameters:</p> Name Type Description Default <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallelism, including tensor and pipeline model parallel sizes,                               number of devices, and number of nodes.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training, including maximum steps, accelerator type,                               validation batch limit, validation check interval, and precision.</p> required <code>callbacks</code> <code>list</code> <p>List of callback functions to be used during training. Defaults to None,                         in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.</p> <code>None</code> <code>nsys_config</code> <code>NsysConfig</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <p>Returns:</p> Type Description <code>Trainer</code> <p>nl.Trainer: Configured trainer object ready for model training.</p> Source code in <code>bionemo/llm/train.py</code> <pre><code>def setup_trainer(\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    callbacks=None,\n    nsys_config: NsysConfig | None = None,\n) -&gt; nl.Trainer:\n    \"\"\"Set up the trainer for model training using the specified parallel and training configurations.\n\n    Args:\n        parallel_config (ParallelConfig): Configuration for parallelism, including tensor and pipeline model parallel sizes,\n                                          number of devices, and number of nodes.\n        training_config (TrainingConfig): Configuration for training, including maximum steps, accelerator type,\n                                          validation batch limit, validation check interval, and precision.\n        callbacks (list, optional): List of callback functions to be used during training. Defaults to None,\n                                    in which case default callbacks (RichModelSummary and LearningRateMonitor) are used.\n        nsys_config (NsysConfig, optional): Configuration for nsys profiling. If None, is disabled.\n\n    Returns:\n        nl.Trainer: Configured trainer object ready for model training.\n    \"\"\"\n    strategy = nl.MegatronStrategy(\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n    )\n    if callbacks is None:\n        callbacks = [\n            RichModelSummary(max_depth=4),\n            LearningRateMonitor(),\n        ]\n\n    if training_config.include_perplexity:\n        callbacks.append(PerplexityLoggingCallback())\n\n    if training_config.gc_interval &gt; 0:\n        callbacks.append(\n            nl_callbacks.GarbageCollectionCallback(\n                gc_interval_train=training_config.gc_interval, gc_interval_val=training_config.gc_interval\n            )\n        )\n\n    if nsys_config:\n        if nsys_config.end_step is None:\n            nsys_config.end_step = training_config.max_steps\n        callbacks.append(\n            nl_callbacks.NsysCallback(\n                start_step=nsys_config.start_step,\n                end_step=nsys_config.end_step,\n                ranks=nsys_config.ranks,\n                gen_shape=True,\n            )\n        )\n\n    trainer = nl.Trainer(\n        devices=parallel_config.num_devices,\n        max_steps=training_config.max_steps,\n        accelerator=training_config.accelerator,\n        strategy=strategy,\n        limit_val_batches=training_config.limit_val_batches,\n        val_check_interval=training_config.val_check_interval,\n        num_nodes=parallel_config.num_nodes,\n        callbacks=callbacks,\n        plugins=nl.MegatronMixedPrecision(precision=training_config.precision),\n    )\n    return trainer\n</code></pre>"},{"location":"API_reference/bionemo/llm/train/#bionemo.llm.train.train","title":"<code>train(bionemo_exposed_model_config, data_config, parallel_config, training_config, optim_config, experiment_config, wandb_config, nsys_config=None, resume_if_exists=True)</code>","text":"<p>Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.</p> <p>Parameters:</p> Name Type Description Default <code>bionemo_exposed_model_config</code> <code>ExposedModelConfig</code> <p>Configuration for the exposed BioNemo model.</p> required <code>data_config</code> <code>DataConfig[DataModuleT]</code> <p>Configuration for the data module.</p> required <code>parallel_config</code> <code>ParallelConfig</code> <p>Configuration for parallel training.</p> required <code>training_config</code> <code>TrainingConfig</code> <p>Configuration for training parameters.</p> required <code>optim_config</code> <code>OptimizerSchedulerConfig</code> <p>Configuration for the optimizer and scheduler.</p> required <code>experiment_config</code> <code>ExperimentConfig</code> <p>Configuration for the experiment.</p> required <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>Configuration for Weights and Biases logging.n</p> required <code>nsys_config</code> <code>Optional[NsysConfig]</code> <p>Configuration for nsys profiling. If None, is disabled.</p> <code>None</code> <code>resume_if_exists</code> <code>bool</code> <p>Flag to resume training if a checkpoint exists. Defaults to True.</p> <code>True</code> Source code in <code>bionemo/llm/train.py</code> <pre><code>def train(\n    bionemo_exposed_model_config: ExposedModelConfig,\n    data_config: DataConfig[DataModuleT],\n    parallel_config: ParallelConfig,\n    training_config: TrainingConfig,\n    optim_config: OptimizerSchedulerConfig,\n    experiment_config: ExperimentConfig,\n    wandb_config: Optional[WandbConfig],\n    nsys_config: Optional[NsysConfig] = None,\n    resume_if_exists: bool = True,\n):\n    \"\"\"Train a BioNemo model using the provided configurations. Uses the ExposedModelConfig and DataConfig as the primary variants for this method.\n\n    Args:\n        bionemo_exposed_model_config (ExposedModelConfig): Configuration for the exposed BioNemo model.\n        data_config (DataConfig[DataModuleT]): Configuration for the data module.\n        parallel_config (ParallelConfig): Configuration for parallel training.\n        training_config (TrainingConfig): Configuration for training parameters.\n        optim_config (OptimizerSchedulerConfig): Configuration for the optimizer and scheduler.\n        experiment_config (ExperimentConfig): Configuration for the experiment.\n        wandb_config (Optional[WandbConfig]): Configuration for Weights and Biases logging.n\n        nsys_config (Optional[NsysConfig], optional): Configuration for nsys profiling. If None, is disabled.\n        resume_if_exists (bool, optional): Flag to resume training if a checkpoint exists. Defaults to True.\n    \"\"\"\n    bionemo_model_config = bionemo_exposed_model_config.exposed_to_internal_bionemo_model_config()\n    pathlib.Path(data_config.result_dir).mkdir(parents=True, exist_ok=True)\n\n    if experiment_config.save_every_n_steps != training_config.val_check_interval:\n        logging.warning(\"Mutating training_config.save_every_n_steps to be equal to val_check_interval.\")\n        experiment_config.save_every_n_steps = training_config.val_check_interval\n\n    global_batch_size = infer_global_batch_size(\n        micro_batch_size=data_config.micro_batch_size,\n        num_nodes=parallel_config.num_nodes,\n        devices=parallel_config.num_devices,\n        accumulate_grad_batches=parallel_config.accumulate_grad_batches,\n        tensor_model_parallel_size=parallel_config.tensor_model_parallel_size,\n        pipeline_model_parallel_size=parallel_config.pipeline_model_parallel_size,\n    )\n\n    data: DataModuleT = data_config.construct_data_module(global_batch_size)\n    # TODO BioBertDataModule or BioBertTokenizer abstractions. We know all DataModuleT in this case has data.tokenizer,\n    # although this constraint is not documented.\n\n    # TODO: need an abstraction for LrSchedulerConfig\n    if optim_config.lr_scheduler == \"cosine\":\n        lr_scheduler = CosineAnnealingScheduler(\n            max_steps=training_config.max_steps,\n            min_lr=optim_config.lr / 100,\n            warmup_steps=int(math.ceil(training_config.max_steps * optim_config.cosine_rampup_frac)),\n            interval=optim_config.interval,\n            monitor=optim_config.monitor,\n            constant_steps=int(math.ceil(training_config.max_steps * optim_config.cosine_hold_frac)),\n        )\n    elif optim_config.lr_scheduler == \"warmup_anneal\":\n        lr_scheduler = WarmupAnnealDecayHoldScheduler(\n            warmup_steps=optim_config.warmup_steps,\n            max_steps=training_config.max_steps,\n            max_lr=optim_config.lr,\n            min_lr=optim_config.lr / 10.0,\n            anneal_percentage=0.10,\n        )\n    else:\n        raise NotImplementedError(f\"Scheduler {optim_config.lr_scheduler} not implemented.\")\n\n    optimizer = MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=optim_config.lr,\n            optimizer=optim_config.optimizer,\n            use_distributed_optimizer=True,\n            fp16=bionemo_model_config.fp16,\n            bf16=bionemo_model_config.bf16,\n        ),\n        lr_scheduler=lr_scheduler,\n    )\n\n    model: BionemoLightningModule = biobert_lightning_module(\n        config=bionemo_model_config, tokenizer=data.tokenizer, optimizer=optimizer\n    )\n    trainer: nl.Trainer = setup_trainer(parallel_config, training_config, nsys_config=nsys_config)\n    nemo_logger: nl.NeMoLogger = nemo_logger_factory(experiment_config, wandb_config=wandb_config)\n\n    llm.train(\n        model=model,\n        data=data,\n        trainer=trainer,\n        log=nemo_logger,\n        resume=resume.AutoResume(\n            resume_if_exists=resume_if_exists,\n            resume_ignore_no_checkpoint=True,\n        ),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/collate/","title":"Collate","text":""},{"location":"API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.bert_padding_collate_fn","title":"<code>bert_padding_collate_fn(batch, padding_value, min_length=None, max_length=None)</code>","text":"<p>Padding collate function for BERT dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>List of samples.</p> required <code>padding_value</code> <code>int</code> <p>The tokenizer's pad token ID.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def bert_padding_collate_fn(\n    batch: Sequence[types.BertSample],\n    padding_value: int,\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; types.BertSample:\n    \"\"\"Padding collate function for BERT dataloaders.\n\n    Args:\n        batch (list): List of samples.\n        padding_value (int, optional): The tokenizer's pad token ID.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n    \"\"\"\n    padding_values = {\n        \"text\": padding_value,\n        \"types\": 0,\n        \"attention_mask\": False,\n        \"labels\": -1,\n        \"loss_mask\": False,\n        \"is_random\": 0,\n    }\n    return padding_collate_fn(\n        batch=batch,  # type: ignore[assignment]\n        padding_values=padding_values,\n        min_length=min_length,\n        max_length=max_length,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/collate/#bionemo.llm.data.collate.padding_collate_fn","title":"<code>padding_collate_fn(batch, padding_values, min_length=None, max_length=None)</code>","text":"<p>Collate function with padding.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Sequence[_T]</code> <p>List of samples, each of which is a dictionary of tensors.</p> required <code>padding_values</code> <code>dict[str, int]</code> <p>A dictionary of padding values for each tensor key.</p> required <code>min_length</code> <code>int | None</code> <p>Minimum length of the output batch; tensors will be padded to this length. If not provided, no extra padding beyond the max_length will be added.</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum length of the sequence. If not provided, tensors will be padded to the longest sequence in the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>_T</code> <p>A collated batch with the same dictionary input structure.</p> Source code in <code>bionemo/llm/data/collate.py</code> <pre><code>def padding_collate_fn(\n    batch: Sequence[_T],\n    padding_values: dict[str, int],\n    min_length: int | None = None,\n    max_length: int | None = None,\n) -&gt; _T:\n    \"\"\"Collate function with padding.\n\n    Args:\n        batch: List of samples, each of which is a dictionary of tensors.\n        padding_values: A dictionary of padding values for each tensor key.\n        min_length: Minimum length of the output batch; tensors will be padded to this length. If not\n            provided, no extra padding beyond the max_length will be added.\n        max_length: Maximum length of the sequence. If not provided, tensors will be padded to the\n            longest sequence in the batch.\n\n    Returns:\n        A collated batch with the same dictionary input structure.\n    \"\"\"\n    global _warned_once\n    keys: set[str] | None = None\n    for entry in batch:\n        # First check that we have sane batches where keys align with each other.\n        if keys is None:\n            keys = set(entry.keys())\n        else:\n            if set(entry.keys()) != keys:\n                raise ValueError(f\"All keys in inputs must match each other. Got: {[sorted(e.keys()) for e in batch]}\")\n        if entry.keys() != padding_values.keys():\n            if not _warned_once:\n                extra_keys = {k for k in entry.keys() if k not in padding_values}\n                missing_keys = {k for k in padding_values.keys() if k not in entry}\n                logger.warning(\n                    f\"Extra keys in batch that will not be padded: {extra_keys}. Missing keys in batch: {missing_keys}\"\n                )\n                _warned_once = True\n\n    def _pad(tensors, padding_value):\n        if max_length is not None:\n            tensors = [t[:max_length] for t in tensors]\n        batched_tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True, padding_value=padding_value)\n        if min_length is None:\n            return batched_tensors\n        return torch.nn.functional.pad(batched_tensors, (0, min_length - batched_tensors.size(1)), value=padding_value)\n\n    return {\n        k: _pad([s[k] for s in batch], padding_values[k])\n        if k in padding_values\n        else torch.stack([s[k] for s in batch])\n        for k in batch[0].keys()\n    }  # type: ignore[return-value]\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule","title":"<code>MegatronDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A mixin that adds a <code>state_dict</code> and <code>load_state_dict</code> method for datamodule training resumption in NeMo.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>class MegatronDataModule(pl.LightningDataModule):\n    \"\"\"A mixin that adds a `state_dict` and `load_state_dict` method for datamodule training resumption in NeMo.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.init_global_step = 0\n\n    def update_init_global_step(self):\n        \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n        self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n        self.data_sampler.init_global_step = (\n            self.init_global_step\n        )  # Update the init_global_step whenever we re-init training\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n        Returns:\n            A dictionary containing datamodule state.\n\n        \"\"\"\n        consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n        return {\"consumed_samples\": consumed_samples}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n        Args:\n            state_dict: the datamodule state returned by ``state_dict``.\n\n        \"\"\"\n        try:\n            from megatron.core.num_microbatches_calculator import update_num_microbatches\n\n        except (ImportError, ModuleNotFoundError):\n            logging.warning(\"Megatron num_microbatches_calculator not found, using Apex version.\")\n            from apex.transformer.pipeline_parallel.utils import update_num_microbatches\n\n        consumed_samples = state_dict[\"consumed_samples\"]\n        self.data_sampler.init_consumed_samples = consumed_samples\n        self.data_sampler.prev_consumed_samples = consumed_samples\n\n        update_num_microbatches(\n            consumed_samples=consumed_samples,\n            consistency_check=False,\n        )\n        self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Set init_global_step to 0 for datamodule resumption.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Set init_global_step to 0 for datamodule resumption.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.init_global_step = 0\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.load_state_dict","title":"<code>load_state_dict(state_dict)</code>","text":"<p>Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>the datamodule state returned by <code>state_dict</code>.</p> required Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Called when loading a checkpoint, implement to reload datamodule state given datamodule stat.\n\n    Args:\n        state_dict: the datamodule state returned by ``state_dict``.\n\n    \"\"\"\n    try:\n        from megatron.core.num_microbatches_calculator import update_num_microbatches\n\n    except (ImportError, ModuleNotFoundError):\n        logging.warning(\"Megatron num_microbatches_calculator not found, using Apex version.\")\n        from apex.transformer.pipeline_parallel.utils import update_num_microbatches\n\n    consumed_samples = state_dict[\"consumed_samples\"]\n    self.data_sampler.init_consumed_samples = consumed_samples\n    self.data_sampler.prev_consumed_samples = consumed_samples\n\n    update_num_microbatches(\n        consumed_samples=consumed_samples,\n        consistency_check=False,\n    )\n    self.data_sampler.if_first_step = 1\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.state_dict","title":"<code>state_dict()</code>","text":"<p>Called when saving a checkpoint, implement to generate and save datamodule state.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing datamodule state.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Called when saving a checkpoint, implement to generate and save datamodule state.\n\n    Returns:\n        A dictionary containing datamodule state.\n\n    \"\"\"\n    consumed_samples = self.data_sampler.compute_consumed_samples(self.trainer.global_step - self.init_global_step)\n    return {\"consumed_samples\": consumed_samples}\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/datamodule/#bionemo.llm.data.datamodule.MegatronDataModule.update_init_global_step","title":"<code>update_init_global_step()</code>","text":"<p>Please always call this when you get a new dataloader... if you forget, your resumption will not work.</p> Source code in <code>bionemo/llm/data/datamodule.py</code> <pre><code>def update_init_global_step(self):\n    \"\"\"Please always call this when you get a new dataloader... if you forget, your resumption will not work.\"\"\"\n    self.init_global_step = self.trainer.global_step  # Update the init_global_step whenever we re-init training\n    self.data_sampler.init_global_step = (\n        self.init_global_step\n    )  # Update the init_global_step whenever we re-init training\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/","title":"Label2id tokenizer","text":""},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer","title":"<code>Label2IDTokenizer</code>","text":"<p>               Bases: <code>TokenizerSpec</code></p> <p>Initializes simple Char Tokenizer.</p> <p>Intended to be used for extracting class labels for classification models such as secondary structure prediction model, where each class is encoded with a character (ex. \"C\", \"H\", \"E\")</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n&gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n&gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n</code></pre> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>class Label2IDTokenizer(TokenizerSpec):\n    \"\"\"Initializes simple Char Tokenizer.\n\n    Intended to be used for extracting class labels\n    for classification models such as secondary\n    structure prediction model, where each class is\n    encoded with a character (ex. \"C\", \"H\", \"E\")\n\n    Examples:\n            &gt;&gt;&gt; tokenizer = Label2IDTokenizer()\n            &gt;&gt;&gt; seqs = ['CHE', 'CCC', 'EHH']\n            &gt;&gt;&gt; tokenizer = tokenizer.build_vocab(s)\n\n    \"\"\"\n\n    def __init__(self) -&gt; None:  # noqa: D107\n        super().__init__()\n        self.vocab: Dict[str, int] = {}\n        self.decode_vocab: Dict[int, str] = {id_: token for token, id_ in self.vocab.items()}\n\n    @property\n    def vocab_size(self) -&gt; int:\n        \"\"\"Return the size of the vocab being used.\"\"\"\n        return len(self.vocab)\n\n    def text_to_tokens(self, text: str) -&gt; List[str]:  # noqa: D102\n        return list(text)\n\n    def tokens_to_text(self, tokens: List[str]) -&gt; str:  # noqa: D102\n        return \"\".join(tokens)\n\n    def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n        \"\"\"Convert tokens to indexes/ids.\n\n        Args:\n            tokens: Containing tokens\n        Returns:\n            Containing ID's for each token\n        \"\"\"\n        ids = []\n        for token in tokens:\n            id_ = self.vocab.get(token)\n            if id_ is None:\n                raise ValueError(f\"Do not recognize token: {token}\")\n            else:\n                ids.append(id_)\n        return ids\n\n    def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n        \"\"\"Convert Ids to tokens.\n\n        Args:\n            ids: Containg ids for each token\n        Returns:\n            Containing tokens\n        \"\"\"\n        tokens = []\n        for id_ in ids:\n            token = self.decode_vocab.get(id_)\n            if token is None:\n                raise ValueError(f\"Do not recognize ID: {id_}\")\n            tokens.append(token)\n        return tokens\n\n    def text_to_ids(self, text: str) -&gt; List[int]:\n        \"\"\"Converts text to ids.\n\n        Args:\n            text (str): String containing text to convert\n        Returns:\n            (List[int]): Id's corresponding to the tokenization\n            of the text\n        \"\"\"\n        tokens = self.text_to_tokens(text)\n        return self.tokens_to_ids(tokens)\n\n    def ids_to_text(self, ids: List[int]) -&gt; str:  # noqa: D102\n        tokens = self.ids_to_tokens(ids)\n        return self.tokens_to_text(tokens)\n\n    def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n        \"\"\"Builds the vocabulary of the tokenizer from strings\n        Args:\n            strings: (Union[str, Iterable[str]]): Strings to\n                build the vocabulary with. If a string is supplied,\n                then the vocabulary is built from the single string.\n                Otherwise, the vocabulary is progressively built\n                from all the strings in `strings`.\n        \"\"\"  # noqa: D205\n        if isinstance(strings, str):\n            strings = [strings]\n\n        for string in strings:\n            for token in string:\n                if token not in self.vocab:\n                    self.vocab[token] = len(self.vocab)\n                    self.decode_vocab[self.vocab[token]] = token\n\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.vocab_size","title":"<code>vocab_size: int</code>  <code>property</code>","text":"<p>Return the size of the vocab being used.</p>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.build_vocab","title":"<code>build_vocab(strings)</code>","text":"<p>Builds the vocabulary of the tokenizer from strings Args:     strings: (Union[str, Iterable[str]]): Strings to         build the vocabulary with. If a string is supplied,         then the vocabulary is built from the single string.         Otherwise, the vocabulary is progressively built         from all the strings in <code>strings</code>.</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def build_vocab(self, strings: Union[str, Iterable[str]]) -&gt; \"Label2IDTokenizer\":\n    \"\"\"Builds the vocabulary of the tokenizer from strings\n    Args:\n        strings: (Union[str, Iterable[str]]): Strings to\n            build the vocabulary with. If a string is supplied,\n            then the vocabulary is built from the single string.\n            Otherwise, the vocabulary is progressively built\n            from all the strings in `strings`.\n    \"\"\"  # noqa: D205\n    if isinstance(strings, str):\n        strings = [strings]\n\n    for string in strings:\n        for token in string:\n            if token not in self.vocab:\n                self.vocab[token] = len(self.vocab)\n                self.decode_vocab[self.vocab[token]] = token\n\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.ids_to_tokens","title":"<code>ids_to_tokens(ids)</code>","text":"<p>Convert Ids to tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[int]</code> <p>Containg ids for each token</p> required <p>Returns:     Containing tokens</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def ids_to_tokens(self, ids: List[int]) -&gt; List[str]:\n    \"\"\"Convert Ids to tokens.\n\n    Args:\n        ids: Containg ids for each token\n    Returns:\n        Containing tokens\n    \"\"\"\n    tokens = []\n    for id_ in ids:\n        token = self.decode_vocab.get(id_)\n        if token is None:\n            raise ValueError(f\"Do not recognize ID: {id_}\")\n        tokens.append(token)\n    return tokens\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.text_to_ids","title":"<code>text_to_ids(text)</code>","text":"<p>Converts text to ids.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>String containing text to convert</p> required <p>Returns:     (List[int]): Id's corresponding to the tokenization     of the text</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def text_to_ids(self, text: str) -&gt; List[int]:\n    \"\"\"Converts text to ids.\n\n    Args:\n        text (str): String containing text to convert\n    Returns:\n        (List[int]): Id's corresponding to the tokenization\n        of the text\n    \"\"\"\n    tokens = self.text_to_tokens(text)\n    return self.tokens_to_ids(tokens)\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/label2id_tokenizer/#bionemo.llm.data.label2id_tokenizer.Label2IDTokenizer.tokens_to_ids","title":"<code>tokens_to_ids(tokens)</code>","text":"<p>Convert tokens to indexes/ids.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>Containing tokens</p> required <p>Returns:     Containing ID's for each token</p> Source code in <code>bionemo/llm/data/label2id_tokenizer.py</code> <pre><code>def tokens_to_ids(self, tokens: List[str]) -&gt; List[int]:\n    \"\"\"Convert tokens to indexes/ids.\n\n    Args:\n        tokens: Containing tokens\n    Returns:\n        Containing ID's for each token\n    \"\"\"\n    ids = []\n    for token in tokens:\n        id_ = self.vocab.get(token)\n        if id_ is None:\n            raise ValueError(f\"Do not recognize token: {token}\")\n        else:\n            ids.append(id_)\n    return ids\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/","title":"Masking","text":""},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig","title":"<code>BertMaskConfig</code>  <code>dataclass</code>","text":"<p>Configuration for masking tokens in a BERT-style model.</p> <p>Attributes:</p> Name Type Description <code>mask_prob</code> <code>float</code> <p>Probability of masking a token.</p> <code>mask_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with the mask token.</p> <code>random_token_prob</code> <code>float</code> <p>Probability of replacing a masked token with a random token.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>@dataclass(frozen=True)\nclass BertMaskConfig:\n    \"\"\"Configuration for masking tokens in a BERT-style model.\n\n    Attributes:\n        mask_prob: Probability of masking a token.\n        mask_token_prob: Probability of replacing a masked token with the mask token.\n        random_token_prob: Probability of replacing a masked token with a random token.\n    \"\"\"\n\n    tokenizer: Tokenizer\n    random_tokens: range\n    mask_prob: float = 0.15\n    mask_token_prob: float = 0.8\n    random_token_prob: float = 0.1\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n        Raises:\n            ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n        \"\"\"\n        if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n            raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.BertMaskConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Check that the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is less than or equal to 1.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of <code>mask_token_prob</code> and <code>random_token_prob</code> is greater than 1.0.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Check that the sum of `mask_token_prob` and `random_token_prob` is less than or equal to 1.0.\n\n    Raises:\n        ValueError: If the sum of `mask_token_prob` and `random_token_prob` is greater than 1.0.\n    \"\"\"\n    if self.random_token_prob + self.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.add_cls_and_eos_tokens","title":"<code>add_cls_and_eos_tokens(sequence, labels, loss_mask, cls_token=None, eos_token=None)</code>","text":"<p>Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.</p> <p>These labels should never be masked, so this is done after the masking step.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Tensor</code> <p>The input (likely masked) sequence.</p> required <code>labels</code> <code>Tensor</code> <p>The true values of the input sequence at the mask positions.</p> required <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor indicating which tokens should be included in the loss.</p> required <code>cls_token</code> <code>int | None</code> <p>The token to use for the CLS token. If None, no CLS token is added.</p> <code>None</code> <code>eos_token</code> <code>int | None</code> <p>The token to use for the EOS token. If None, no EOS token is added.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def add_cls_and_eos_tokens(\n    sequence: torch.Tensor,\n    labels: torch.Tensor,\n    loss_mask: torch.Tensor,\n    cls_token: int | None = None,\n    eos_token: int | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Prepends the CLS token and appends the EOS token to the masked sequence, updating the loss mask and labels.\n\n    These labels should never be masked, so this is done after the masking step.\n\n    Args:\n        sequence: The input (likely masked) sequence.\n        labels: The true values of the input sequence at the mask positions.\n        loss_mask: A boolean tensor indicating which tokens should be included in the loss.\n        cls_token: The token to use for the CLS token. If None, no CLS token is added.\n        eos_token: The token to use for the EOS token. If None, no EOS token is added.\n\n    Returns:\n        The same input tensors with the CLS and EOS tokens added, and the labels and loss_mask updated accordingly.\n    \"\"\"\n    # Prepend the CLS token and append the EOS token, and update the loss mask and labels accordingly.\n    sequence = torch.cat(\n        [\n            torch.tensor([cls_token], dtype=sequence.dtype)\n            if cls_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n            sequence,\n            torch.tensor([eos_token], dtype=sequence.dtype)\n            if eos_token is not None\n            else torch.tensor([], dtype=sequence.dtype),\n        ]\n    )\n\n    labels = torch.cat(\n        [\n            torch.tensor([-1], dtype=labels.dtype) if cls_token is not None else torch.tensor([], dtype=labels.dtype),\n            labels,\n            torch.tensor([-1], dtype=labels.dtype) if eos_token is not None else torch.tensor([], dtype=labels.dtype),\n        ]\n    )\n\n    loss_mask = torch.cat(\n        [\n            torch.tensor([False]) if cls_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n            loss_mask,\n            torch.tensor([False]) if eos_token is not None else torch.tensor([], dtype=loss_mask.dtype),\n        ]\n    )\n\n    return sequence, labels, loss_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/masking/#bionemo.llm.data.masking.apply_bert_pretraining_mask","title":"<code>apply_bert_pretraining_mask(tokenized_sequence, random_seed, mask_config)</code>","text":"<p>Applies the pretraining mask to a tokenized sequence.</p> <p>Parameters:</p> Name Type Description Default <code>tokenized_sequence</code> <code>Tensor</code> <p>Tokenized protein sequence.</p> required <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility.</p> required <code>mask_config</code> <code>BertMaskConfig</code> <p>Configuration for masking tokens in a BERT-style model.</p> required <p>Returns:</p> Name Type Description <code>masked_sequence</code> <code>Tensor</code> <p>The tokenized sequence with some tokens masked.</p> <code>labels</code> <code>Tensor</code> <p>A tensor the same shape as <code>masked_sequence</code> containing labels for the masked tokens, with -1 for non-masked tokens.</p> <code>loss_mask</code> <code>Tensor</code> <p>A boolean tensor the same shape as <code>masked_sequence</code>, where 'True' indicates which tokens should be included in the loss.</p> Source code in <code>bionemo/llm/data/masking.py</code> <pre><code>def apply_bert_pretraining_mask(\n    tokenized_sequence: torch.Tensor, random_seed: int, mask_config: BertMaskConfig\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Applies the pretraining mask to a tokenized sequence.\n\n    Args:\n        tokenized_sequence: Tokenized protein sequence.\n        random_seed: Random seed for reproducibility.\n        mask_config: Configuration for masking tokens in a BERT-style model.\n\n    Returns:\n        masked_sequence:\n            The tokenized sequence with some tokens masked.\n        labels:\n            A tensor the same shape as `masked_sequence` containing labels for the masked tokens, with -1 for non-masked\n            tokens.\n        loss_mask:\n            A boolean tensor the same shape as `masked_sequence`, where 'True' indicates which tokens should be included\n            in the loss.\n    \"\"\"\n    if mask_config.tokenizer.mask_token_id is None:\n        raise ValueError(\"Tokenizer must have a mask token.\")\n\n    if mask_config.random_token_prob + mask_config.mask_token_prob &gt; 1.0:\n        raise ValueError(\"Sum of random_token_prob and mask_token_prob must be less than or equal to 1.0.\")\n\n    # Set the seed so that __getitem__(idx) is always deterministic.\n    # This is required by Megatron-LM's parallel strategies.\n    generator = torch.Generator().manual_seed(random_seed)\n\n    mask_stop_1 = mask_config.mask_prob * mask_config.mask_token_prob\n    mask_stop_2 = mask_config.mask_prob * (mask_config.mask_token_prob + mask_config.random_token_prob)\n\n    random_draws = torch.rand(tokenized_sequence.shape, generator=generator)  # Random draws for each token in [0, 1).\n\n    # Overall mask for a token being masked in some capacity - either mask token, random token, or left as-is\n    # (identity). We don't want to mask special tokens.\n    loss_mask = ~torch.isin(tokenized_sequence, torch.tensor(mask_config.tokenizer.all_special_ids))\n    loss_mask &amp;= random_draws &lt; mask_config.mask_prob\n\n    # The first `mask_token_prob` fraction of the `mask_prob` tokens are replaced with the mask token.\n    mask_token_mask = (random_draws &lt; mask_stop_1) &amp; loss_mask\n\n    # The next `random_token_prob` fraction of the `mask_prob` tokens are replaced with a random token.\n    random_token_mask = ((random_draws &gt;= mask_stop_1) &amp; (random_draws &lt; mask_stop_2)) &amp; loss_mask\n\n    # The remaining tokens are implicitly left as-is, representing an identity mask.\n\n    # Mask the tokens.\n    masked_sequence = tokenized_sequence.clone()\n    masked_sequence[mask_token_mask] = mask_config.tokenizer.mask_token_id\n    num_random_tokens: int = random_token_mask.sum().item()  # type: ignore[assignment]\n    masked_sequence[random_token_mask] = torch.randint(\n        low=mask_config.random_tokens.start,\n        high=mask_config.random_tokens.stop,\n        size=(num_random_tokens,),\n        dtype=masked_sequence.dtype,\n        generator=generator,\n    )\n\n    # Create the labels for the masked tokens.\n    labels = tokenized_sequence.clone()\n    labels[~loss_mask] = -100  # Ignore loss for non-masked tokens.\n\n    return masked_sequence, labels, loss_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/types/","title":"Types","text":""},{"location":"API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.BertSample","title":"<code>BertSample</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The type expected by NeMo/Megatron for a single dataset item.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Tensor</code> <p>The tokenized, masked input text.</p> <code>types</code> <code>Tensor</code> <p>The token type ids, if applicable.</p> <code>attention_mask</code> <code>Tensor</code> <p>A mask over all valid tokens, excluding padding.</p> <code>labels</code> <code>Tensor</code> <p>The true values of the masked tokens at each position covered by loss_mask.</p> <code>loss_mask</code> <code>Tensor</code> <p>The mask over the text indicating which tokens are masked and should be predicted.</p> <code>is_random</code> <code>Tensor</code> <p>??</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class BertSample(TypedDict):\n    \"\"\"The type expected by NeMo/Megatron for a single dataset item.\n\n    Attributes:\n        text: The tokenized, masked input text.\n        types: The token type ids, if applicable.\n        attention_mask: A mask over all valid tokens, excluding padding.\n        labels: The true values of the masked tokens at each position covered by loss_mask.\n        loss_mask: The mask over the text indicating which tokens are masked and should be predicted.\n        is_random: ??\n    \"\"\"\n\n    text: Tensor\n    types: Tensor\n    attention_mask: Tensor\n    labels: Tensor\n    loss_mask: Tensor\n    is_random: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/data/types/#bionemo.llm.data.types.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Required attributes for a tokenizers provided to apply_bert_pretraining_mask.</p> Source code in <code>bionemo/llm/data/types.py</code> <pre><code>class Tokenizer(Protocol):\n    \"\"\"Required attributes for a tokenizers provided to apply_bert_pretraining_mask.\"\"\"\n\n    @property\n    def mask_token_id(self) -&gt; int | None:  # noqa: D102\n        ...\n\n    @property\n    def all_special_ids(self) -&gt; list[int]:  # noqa: D102\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/","title":"Config","text":""},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto","title":"<code>IOMixinProto</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol for the get/set hparam functions of the IOMixin class from NeMo.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class IOMixinProto(Protocol):\n    \"\"\"A Protocol for the get/set hparam functions of the IOMixin class from NeMo.\"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Get the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Get the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.IOMixinProto.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Set the value of an attribute in the config attached to the class by the IOMixin.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Set the value of an attribute in the config attached to the class by the IOMixin.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoModelConfig","title":"<code>MegatronBioNeMoModelConfig</code>","text":"<p>               Bases: <code>BionemoModelConfig[MegatronModelType]</code>, <code>TransformerConfig</code>, <code>WillHaveGetSetHparam</code></p> <p>A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>class MegatronBioNeMoModelConfig(BionemoModelConfig[MegatronModelType], TransformerConfig, iom.WillHaveGetSetHparam):\n    \"\"\"A ModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    model_cls: Type[MegatronModelType]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig","title":"<code>MegatronBioNeMoTrainableModelConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoModelConfig[MegatronModelType]</code>, <code>BionemoTrainableModelConfig[MegatronModelType, MegatronLossType]</code>, <code>Generic[MegatronModelType, MegatronLossType]</code></p> <p>A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>@dataclass\nclass MegatronBioNeMoTrainableModelConfig(\n    MegatronBioNeMoModelConfig[MegatronModelType],\n    BionemoTrainableModelConfig[MegatronModelType, MegatronLossType],\n    Generic[MegatronModelType, MegatronLossType],\n):\n    \"\"\"A TrainableModelConfig class for bionemo that supports usage with Megatron models, for example as NeMo2 requires.\"\"\"\n\n    initial_ckpt_path: str | None = None\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIONEMO_CONFIG_DEFAULTS)\n\n    def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Load settings into self from the checkpoint saved in self.\n\n        Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n        parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n        Args:\n            initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n                other than the settings in self.override_parent_fields.\n\n        Returns:\n            None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n                a checkpoint are updated.\n        \"\"\"\n        logger.warn(f\"Loading {self.initial_ckpt_path}\")\n        # 1. get the config\n        # TODO type(self) is probably not correct, maybe make the class name of the config to load an argument?\n        cfg_trainer_ctx: TrainerContext = io.load_context(Path(initial_ckpt_path) / \"context\")\n        initial_config: MegatronBioNeMoTrainableModelConfig = cfg_trainer_ctx.model.config\n        initial_fields = {f.name for f in fields(initial_config)}\n        my_fields = [f.name for f in fields(self)]\n        skip_fields = set(self.override_parent_fields)\n        override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n        override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n\n    def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n        \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n        Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n            self.initial_ckpt_skip_keys_with_these_prefixes.\n\n        Args:\n            model: The Megatron model to update.\n            initial_ckpt_path: The path to the megatron checkpoint to load.\n\n        Returns:\n            None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n                any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n        \"\"\"\n        load_weights_sharded_inplace_nemo2_to_mcore(\n            model=model,  # type: ignore\n            distributed_checkpoint_dir=initial_ckpt_path,\n            skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n        )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.load_settings_from_checkpoint","title":"<code>load_settings_from_checkpoint(initial_ckpt_path)</code>","text":"<p>Load settings into self from the checkpoint saved in self.</p> <p>Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper parameters in this config, as well as the associated attributes in self in case they were modified post-init.</p> <p>Parameters:</p> Name Type Description Default <code>initial_ckpt_path</code> <code>str</code> <p>The path to the checkpoint to load, note that everything is loaded from this checkpoint other than the settings in self.override_parent_fields.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into a checkpoint are updated.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def load_settings_from_checkpoint(self, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Load settings into self from the checkpoint saved in self.\n\n    Any setting in self.override_parent_fields is not overriden. Note that this function will also update the hyper\n    parameters in this config, as well as the associated attributes in self in case they were modified post-init.\n\n    Args:\n        initial_ckpt_path: The path to the checkpoint to load, note that everything is loaded from this checkpoint\n            other than the settings in self.override_parent_fields.\n\n    Returns:\n        None, the settings are loaded into self in place, and the hyper-parameters that will later be saved into\n            a checkpoint are updated.\n    \"\"\"\n    logger.warn(f\"Loading {self.initial_ckpt_path}\")\n    # 1. get the config\n    # TODO type(self) is probably not correct, maybe make the class name of the config to load an argument?\n    cfg_trainer_ctx: TrainerContext = io.load_context(Path(initial_ckpt_path) / \"context\")\n    initial_config: MegatronBioNeMoTrainableModelConfig = cfg_trainer_ctx.model.config\n    initial_fields = {f.name for f in fields(initial_config)}\n    my_fields = [f.name for f in fields(self)]\n    skip_fields = set(self.override_parent_fields)\n    override_fields = [f for f in my_fields if f in initial_fields and f not in skip_fields]\n    override_mutate_possibly_extra_mutated_fiddle(self, initial_config, override_fields)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig.update_model_from_checkpoint","title":"<code>update_model_from_checkpoint(model, initial_ckpt_path)</code>","text":"<p>Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.</p> <p>Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in     self.initial_ckpt_skip_keys_with_these_prefixes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>The Megatron model to update.</p> required <code>initial_ckpt_path</code> <code>str</code> <p>The path to the megatron checkpoint to load.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def update_model_from_checkpoint(self, model: MegatronModelType, initial_ckpt_path: str) -&gt; None:\n    \"\"\"Utility function to standardize how to load a megatron model from a checkpoint ignoring user-specified keys.\n\n    Update the model with the weights from the provided checkpoint path, skipping the keys with the prefixes in\n        self.initial_ckpt_skip_keys_with_these_prefixes.\n\n    Args:\n        model: The Megatron model to update.\n        initial_ckpt_path: The path to the megatron checkpoint to load.\n\n    Returns:\n        None, the model is updated in place, supporting megatron model parallelism abstractions, and ignoring\n            any extra keys that are provided in self.initial_ckpt_skip_keys_with_these_prefixes.\n    \"\"\"\n    load_weights_sharded_inplace_nemo2_to_mcore(\n        model=model,  # type: ignore\n        distributed_checkpoint_dir=initial_ckpt_path,\n        skip_keys_with_these_prefixes=set(self.initial_ckpt_skip_keys_with_these_prefixes),\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/config/#bionemo.llm.model.config.override_mutate_possibly_extra_mutated_fiddle","title":"<code>override_mutate_possibly_extra_mutated_fiddle(target_cfg, source_cfg, maybe_mutated_elements_to_clone)</code>","text":"<p>Override the values of the target config with the values of the source config for the given elements.</p> <p>This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in     self incase they were modified later by post_init code.</p> <p>Parameters:</p> Name Type Description Default <code>target_cfg</code> <code>IOMixinProto</code> <p>The config to update.</p> required <code>source_cfg</code> <code>IOMixinProto</code> <p>The config to copy values from.</p> required <code>maybe_mutated_elements_to_clone</code> <code>List[str]</code> <p>The list of elements to copy from the source config to the target config.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, the target config is updated in place.</p> Source code in <code>bionemo/llm/model/config.py</code> <pre><code>def override_mutate_possibly_extra_mutated_fiddle(\n    target_cfg: IOMixinProto, source_cfg: IOMixinProto, maybe_mutated_elements_to_clone: List[str]\n) -&gt; None:\n    \"\"\"Override the values of the target config with the values of the source config for the given elements.\n\n    This will modify the tracked init hyper-parameter values, as well as modifying the associated attributes in\n        self incase they were modified later by post_init code.\n\n    Args:\n        target_cfg: The config to update.\n        source_cfg: The config to copy values from.\n        maybe_mutated_elements_to_clone: The list of elements to copy from the source config to the target config.\n\n    Returns:\n        None, the target config is updated in place.\n    \"\"\"\n    for f in maybe_mutated_elements_to_clone:\n        # 1. Update the tracked config values. Note that the associated attribute in self may have been modified\n        #  post-init, so we don't want to change the value in self here. We do that separately next.\n        target_cfg.set_hparam(f, source_cfg.get_hparam(f), also_change_value=False)\n        # 2. Update the lazily untracked values (if the same variable name is used post-init)\n        setattr(target_cfg, f, getattr(source_cfg, f))\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/","title":"Layers","text":""},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling","title":"<code>ESM2QueryScaling</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class ESM2QueryScaling(torch.nn.Module):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A custom layer that scales quary values.\n\n        This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n        which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for computing projection_size\n        \"\"\"\n        super().__init__()\n        projection_size = config.kv_channels * config.num_attention_heads\n        self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n        self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n\n    @torch.compile\n    def forward(self, query, *args, **kwargs):  # noqa: D102\n        return query / self.sqrt_val\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.ESM2QueryScaling.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A custom layer that scales quary values.</p> <p>This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2 which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for computing projection_size</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A custom layer that scales quary values.\n\n    This layer should replace the q_layernorm=IdentityOp in ESM2 ModuleSpec to reproduce ESM2\n    which apply 1/sqrt(hidden_size_per_attention_head) scaling prior to apply_rotary_pos_emb()\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for computing projection_size\n    \"\"\"\n    super().__init__()\n    projection_size = config.kv_channels * config.num_attention_heads\n    self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)\n    self.sqrt_val = math.sqrt(self.hidden_size_per_attention_head)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm","title":"<code>TELayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>class TELayerNorm(te.pytorch.LayerNorm):  # noqa: D101\n    def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n        \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n            This allows this method to be used in a megatron layerspec.\n\n        Args:\n            config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n                The rest of the config is not used.\n        \"\"\"  # noqa: D205\n        # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n        super().__init__(\n            *args,\n            zero_centered_gamma=config.layernorm_zero_centered_gamma,\n            sequence_parallel=config.sequence_parallel,\n            **kwargs,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/layers/#bionemo.llm.model.layers.TELayerNorm.__init__","title":"<code>__init__(config, *args, **kwargs)</code>","text":"<p>A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.     This allows this method to be used in a megatron layerspec.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma. The rest of the config is not used.</p> required Source code in <code>bionemo/llm/model/layers.py</code> <pre><code>def __init__(self, config: TransformerConfig, *args, **kwargs) -&gt; None:  # noqa: D417\n    \"\"\"A wrapper around transformer engine layernorm that allows it to be initialized with a TransformerConfig.\n        This allows this method to be used in a megatron layerspec.\n\n    Args:\n        config (TransformerConfig): The megatron config. This is used for extracing sequence_parallel and zero_centered_gamma.\n            The rest of the config is not used.\n    \"\"\"  # noqa: D205\n    # Eps tends to get passed through properly, as does hidden_size, but not other params from the config.\n    super().__init__(\n        *args,\n        zero_centered_gamma=config.layernorm_zero_centered_gamma,\n        sequence_parallel=config.sequence_parallel,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/","title":"Loss","text":""},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction","title":"<code>BERTMLMLossWithReduction</code>","text":"<p>               Bases: <code>_Nemo2CompatibleLossReduceMixin</code>, <code>MegatronLossReduction</code></p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class BERTMLMLossWithReduction(_Nemo2CompatibleLossReduceMixin, MegatronLossReduction):  # noqa: D101\n    def __init__(\n        self,\n        validation_step: bool = False,\n        val_drop_last: bool = True,\n        send_train_output: bool = False,\n        send_val_output: bool = True,\n    ) -&gt; None:\n        \"\"\"Initializes the Model class.\n\n        Args:\n            validation_step (bool, optional): Whether this object is being applied to the validation step. Defaults to False.\n            val_drop_last (bool, optional): Whether the last batch is configured to be dropped during validation. Defaults to True.\n            send_train_output (bool): Whether to return the model output in training. Defaults to False.\n            send_val_output (bool, optional): Whether to return the model output in validation. Defaults to True.\n            include_forward_output_for_metrics (bool): Some downstream metrics such as perplexity require this. It can be\n                expensive to return however, so disable this if performance is a top consideration.\n        \"\"\"\n        # TODO(@jomitchell): Track down how we handle test. This is a common pattern in NeMo2, but these parameters seem likely\n        #  to change in the future.\n        super().__init__()\n        self.validation_step = validation_step\n        self.val_drop_last = val_drop_last\n        self.send_train_output = send_train_output\n        self.send_val_output = send_val_output\n\n    def forward(\n        self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n    ) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n        \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently. In the future this will be extended\n            to handle other loss types like sequence loss if it is present in the forward_out and batch.\n\n        Args:\n            batch (Dict[str, Tensor]): The batch of data. Each tensor should be of shape [batch_size, *, *],\n                and match the corresponding dimension for that particular key in the batch output.\n                For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n            forward_out (Dict[str, Tensor]): The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n        Taken from:\n        https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .\n        \"\"\"  # noqa: D205\n        if \"labels\" not in batch:\n            raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n        train_step: bool = not self.validation_step\n        # Determine if we need to capture/send forward output for downstream metrics, such as perplexity logging\n        #  this is expensive so only do if necessary.\n        send_forward_output: bool = (self.validation_step and self.send_val_output) or (\n            train_step and self.send_train_output\n        )\n\n        if send_forward_output:\n            forward_out_report = {\n                k: v.detach().clone() if torch.is_tensor(v) else v for k, v in forward_out.items()\n            }  # avoid impact from inplace operation on token_logits in unreduced_token_loss_fn\n        else:\n            forward_out_report = {}\n\n        # NOTE: token_logits is [sequence, batch] but labels and other fiels, including the loss are [batch, sequence]\n        unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n        # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n        # compute loss\n        cp_size = parallel_state.get_context_parallel_world_size()\n        if cp_size == 1:\n            # reduce the loss across the micro batch per valid token\n            loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n        else:\n            # reduce the loss across the micro batch per valid token.\n            # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n            #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n            #  other necessary keys to the batch. Thanks!\n            loss_for_microbatch = masked_token_loss_context_parallel(\n                unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n            )\n\n        # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n        #  reducing the loss across the data parallel group.\n        if self.validation_step and not self.val_drop_last:\n            num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n            if loss_for_microbatch.isnan():\n                # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n                #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n                #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n                if batch[\"loss_mask\"].count_nonzero() != 0:\n                    raise ValueError(\"Got NaN loss with non-empty input\")\n                loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n            else:\n                loss_sum_for_microbatch = (\n                    num_valid_tokens_in_microbatch * loss_for_microbatch\n                )  # sum over all valid tokens\n\n            # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n            loss_sum_and_microbatch_size_all_gpu = torch.cat(\n                [\n                    loss_sum_for_microbatch.clone().detach().view(1),\n                    Tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n                ]\n            )\n            torch.distributed.all_reduce(\n                loss_sum_and_microbatch_size_all_gpu,\n                group=parallel_state.get_data_parallel_group(),\n                op=torch.distributed.ReduceOp.SUM,\n            )\n            return loss_for_microbatch * cp_size, {\n                \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n            }\n\n        # average the losses across the data parallel group, but also return the unreduced loss\n        reduced_loss = average_losses_across_data_parallel_group([loss_for_microbatch])\n        if send_forward_output:\n            return loss_for_microbatch * cp_size, {\n                \"avg\": reduced_loss,\n                \"batch\": batch,\n                \"forward_out\": forward_out_report,\n            }\n        else:\n            return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.__init__","title":"<code>__init__(validation_step=False, val_drop_last=True, send_train_output=False, send_val_output=True)</code>","text":"<p>Initializes the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>validation_step</code> <code>bool</code> <p>Whether this object is being applied to the validation step. Defaults to False.</p> <code>False</code> <code>val_drop_last</code> <code>bool</code> <p>Whether the last batch is configured to be dropped during validation. Defaults to True.</p> <code>True</code> <code>send_train_output</code> <code>bool</code> <p>Whether to return the model output in training. Defaults to False.</p> <code>False</code> <code>send_val_output</code> <code>bool</code> <p>Whether to return the model output in validation. Defaults to True.</p> <code>True</code> <code>include_forward_output_for_metrics</code> <code>bool</code> <p>Some downstream metrics such as perplexity require this. It can be expensive to return however, so disable this if performance is a top consideration.</p> required Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def __init__(\n    self,\n    validation_step: bool = False,\n    val_drop_last: bool = True,\n    send_train_output: bool = False,\n    send_val_output: bool = True,\n) -&gt; None:\n    \"\"\"Initializes the Model class.\n\n    Args:\n        validation_step (bool, optional): Whether this object is being applied to the validation step. Defaults to False.\n        val_drop_last (bool, optional): Whether the last batch is configured to be dropped during validation. Defaults to True.\n        send_train_output (bool): Whether to return the model output in training. Defaults to False.\n        send_val_output (bool, optional): Whether to return the model output in validation. Defaults to True.\n        include_forward_output_for_metrics (bool): Some downstream metrics such as perplexity require this. It can be\n            expensive to return however, so disable this if performance is a top consideration.\n    \"\"\"\n    # TODO(@jomitchell): Track down how we handle test. This is a common pattern in NeMo2, but these parameters seem likely\n    #  to change in the future.\n    super().__init__()\n    self.validation_step = validation_step\n    self.val_drop_last = val_drop_last\n    self.send_train_output = send_train_output\n    self.send_val_output = send_val_output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.BERTMLMLossWithReduction.forward","title":"<code>forward(batch, forward_out)</code>","text":"<p>Computes loss of <code>labels</code> in the batch vs <code>token_logits</code> in the forward output currently. In the future this will be extended     to handle other loss types like sequence loss if it is present in the forward_out and batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The batch of data. Each tensor should be of shape [batch_size, , ], and match the corresponding dimension for that particular key in the batch output. For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].</p> required <code>forward_out</code> <code>Dict[str, Tensor]</code> <p>The forward output from the model. Each tensor should be of shape [batch_size, , ]</p> required <p>Taken from: https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def forward(\n    self, batch: Dict[str, Tensor], forward_out: Dict[str, Tensor]\n) -&gt; Tuple[Tensor, PerTokenLossDict | SameSizeLossDict | DataParallelGroupLossAndIO]:\n    \"\"\"Computes loss of `labels` in the batch vs `token_logits` in the forward output currently. In the future this will be extended\n        to handle other loss types like sequence loss if it is present in the forward_out and batch.\n\n    Args:\n        batch (Dict[str, Tensor]): The batch of data. Each tensor should be of shape [batch_size, *, *],\n            and match the corresponding dimension for that particular key in the batch output.\n            For example, the \"labels\" and \"token_logits\" key should have a tensor of shape [batch_size, sequence_length].\n        forward_out (Dict[str, Tensor]): The forward output from the model. Each tensor should be of shape [batch_size, *, *]\n\n    Taken from:\n    https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L951-L976 .\n    \"\"\"  # noqa: D205\n    if \"labels\" not in batch:\n        raise ValueError(\"Labels not provided in the batch. These are required for this loss computation.\")\n\n    train_step: bool = not self.validation_step\n    # Determine if we need to capture/send forward output for downstream metrics, such as perplexity logging\n    #  this is expensive so only do if necessary.\n    send_forward_output: bool = (self.validation_step and self.send_val_output) or (\n        train_step and self.send_train_output\n    )\n\n    if send_forward_output:\n        forward_out_report = {\n            k: v.detach().clone() if torch.is_tensor(v) else v for k, v in forward_out.items()\n        }  # avoid impact from inplace operation on token_logits in unreduced_token_loss_fn\n    else:\n        forward_out_report = {}\n\n    # NOTE: token_logits is [sequence, batch] but labels and other fiels, including the loss are [batch, sequence]\n    unreduced_token_loss = unreduced_token_loss_fn(forward_out[\"token_logits\"], batch[\"labels\"])  # [b s]\n\n    # TODO(@jstjohn) also handle different output keys, like the sequence loss.\n\n    # compute loss\n    cp_size = parallel_state.get_context_parallel_world_size()\n    if cp_size == 1:\n        # reduce the loss across the micro batch per valid token\n        loss_for_microbatch = masked_token_loss(unreduced_token_loss, batch[\"loss_mask\"])\n    else:\n        # reduce the loss across the micro batch per valid token.\n        # TODO(@jomitchell): Figure out who defines \"num_valid_tokens_in_ub\" in the batch and document/understand this.\n        #  This has something to do with context parallel, and there is probably a megatron or nemo function that adds this and\n        #  other necessary keys to the batch. Thanks!\n        loss_for_microbatch = masked_token_loss_context_parallel(\n            unreduced_token_loss, batch[\"loss_mask\"], batch[\"num_valid_tokens_in_ub\"]\n        )\n\n    # If we do not drop the last partial batch of validation, we need to do fancy reduction handling to support\n    #  reducing the loss across the data parallel group.\n    if self.validation_step and not self.val_drop_last:\n        num_valid_tokens_in_microbatch = batch[\"loss_mask\"].sum()\n        if loss_for_microbatch.isnan():\n            # TODO(@jomitchell): Add a unit test for this. This is the case where there are no valid tokens in the microbatch for the loss\n            #  to be computed over, so we expect a NaN loss (divide by zero for a mean) but we make this an expected and non-breaking case,\n            #  re-defining it as a 0 loss. This is standard in NeMo/NeMo2.\n            if batch[\"loss_mask\"].count_nonzero() != 0:\n                raise ValueError(\"Got NaN loss with non-empty input\")\n            loss_sum_for_microbatch = torch.zeros_like(num_valid_tokens_in_microbatch)\n        else:\n            loss_sum_for_microbatch = (\n                num_valid_tokens_in_microbatch * loss_for_microbatch\n            )  # sum over all valid tokens\n\n        # In this case we need to store the loss sum as well as the number of valid tokens in the microbatch.\n        loss_sum_and_microbatch_size_all_gpu = torch.cat(\n            [\n                loss_sum_for_microbatch.clone().detach().view(1),\n                Tensor([num_valid_tokens_in_microbatch]).cuda().clone().detach(),\n            ]\n        )\n        torch.distributed.all_reduce(\n            loss_sum_and_microbatch_size_all_gpu,\n            group=parallel_state.get_data_parallel_group(),\n            op=torch.distributed.ReduceOp.SUM,\n        )\n        return loss_for_microbatch * cp_size, {\n            \"loss_sum_and_microbatch_size\": loss_sum_and_microbatch_size_all_gpu\n        }\n\n    # average the losses across the data parallel group, but also return the unreduced loss\n    reduced_loss = average_losses_across_data_parallel_group([loss_for_microbatch])\n    if send_forward_output:\n        return loss_for_microbatch * cp_size, {\n            \"avg\": reduced_loss,\n            \"batch\": batch,\n            \"forward_out\": forward_out_report,\n        }\n    else:\n        return loss_for_microbatch * cp_size, {\"avg\": reduced_loss}\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.DataParallelGroupLossAndIO","title":"<code>DataParallelGroupLossAndIO</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Average losses across the data parallel group + the original batch and inference output.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class DataParallelGroupLossAndIO(TypedDict):\n    \"\"\"Average losses across the data parallel group + the original batch and inference output.\"\"\"\n\n    avg: Tensor\n    batch: dict[str, Tensor]\n    forward_out: dict[str, Tensor]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.PerTokenLossDict","title":"<code>PerTokenLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class PerTokenLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed per token in the batch, supporting microbatches of varying sizes.\n    \"\"\"\n\n    loss_sum_and_microbatch_size: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.SameSizeLossDict","title":"<code>SameSizeLossDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Tensor dictionary for loss.</p> <p>This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>class SameSizeLossDict(TypedDict):\n    \"\"\"Tensor dictionary for loss.\n\n    This is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.\n    \"\"\"\n\n    avg: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/loss/#bionemo.llm.model.loss.unreduced_token_loss_fn","title":"<code>unreduced_token_loss_fn(logits, labels, cross_entropy_loss_fusion=True)</code>","text":"<p>Computes the unreduced token loss given the logits and labels without regard to the loss mask.</p> <p>WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>The predicted logits of shape [sequence_length, batch_size, num_classes].</p> required <code>labels</code> <code>Tensor</code> <p>The true labels of shape [batch_size, sequence_length].</p> required <code>cross_entropy_loss_fusion</code> <code>bool</code> <p>If True, use the fused kernel version of vocab parallel cross entropy. This should generally be preferred as it packs more operations into a single kernel on the GPU.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The unreduced token loss of shape [batch_size, sequence_length].</p> Source code in <code>bionemo/llm/model/loss.py</code> <pre><code>def unreduced_token_loss_fn(logits: Tensor, labels: Tensor, cross_entropy_loss_fusion: bool = True) -&gt; Tensor:\n    \"\"\"Computes the unreduced token loss given the logits and labels without regard to the loss mask.\n\n    WARNING: This function does not apply a loss mask. Also, it does inplace operation on the inputs.\n\n    Args:\n        logits (Tensor): The predicted logits of shape [sequence_length, batch_size, num_classes].\n        labels (Tensor): The true labels of shape [batch_size, sequence_length].\n        cross_entropy_loss_fusion (bool): If True, use the fused kernel version of vocab parallel cross entropy. This\n            should generally be preferred as it packs more operations into a single kernel on the GPU.\n\n    Returns:\n        Tensor: The unreduced token loss of shape [batch_size, sequence_length].\n    \"\"\"\n    labels = labels.transpose(0, 1).contiguous()  # [b, s] -&gt; [s, b]\n    if cross_entropy_loss_fusion:\n        loss = fused_vocab_parallel_cross_entropy(logits, labels)\n    else:\n        loss = tensor_parallel.vocab_parallel_cross_entropy(logits, labels)\n    # [s b] =&gt; [b, s]\n    loss = loss.transpose(0, 1).contiguous()\n    return loss\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/","title":"Lr scheduler","text":""},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.SchedulerOutput","title":"<code>SchedulerOutput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Output of the scheduler method.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class SchedulerOutput(TypedDict):\n    \"\"\"Output of the scheduler method.\"\"\"\n\n    optimizer: MegatronOptimizerModule\n    lr_scheduler: dict\n    monitor: str\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold","title":"<code>WarmupAnnealDecayHold</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> <p>Warmup Anneal Decay Hold learning rate scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHold(_LRScheduler):\n    \"\"\"Warmup Anneal Decay Hold learning rate scheduler.\"\"\"\n\n    def __init__(\n        self,\n        optimizer: MegatronOptimizerModule,\n        *,\n        warmup_steps: Optional[int] = None,\n        max_steps: Optional[int] = None,\n        max_lr: Optional[float] = None,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        last_epoch: int = -1,\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n        Args:\n            optimizer: Optimizer to apply the learning rate scheduler.\n            warmup_steps (int): Number of steps for the linear warm-up.\n            max_steps (int): Total number of training steps.\n            max_lr (float): Peak learning rate to be achieved after warm-up.\n            min_lr (float): Minimum learning rate.\n            anneal_percentage (float): Percentage of the max_lr to hold after decay.\n            last_epoch (int): The index of the last epoch.\n        \"\"\"\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.last_epoch = last_epoch\n\n        for group in optimizer.param_groups:\n            group.setdefault(\"initial_lr\", max_lr)\n\n        super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self) -&gt; List[float]:\n        \"\"\"Get the learning rate at the current step.\"\"\"\n        step_num = self.last_epoch\n        if step_num &lt; self.warmup_steps:\n            lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n        else:\n            decay_steps = self.max_steps - self.warmup_steps\n            lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n            lr = max(lr, self.max_lr * self.anneal_percentage)\n\n        return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.__init__","title":"<code>__init__(optimizer, *, warmup_steps=None, max_steps=None, max_lr=None, min_lr=4e-05, anneal_percentage=0.1, last_epoch=-1)</code>","text":"<p>Initializes the WarmupAnnealDecayHold learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>MegatronOptimizerModule</code> <p>Optimizer to apply the learning rate scheduler.</p> required <code>warmup_steps</code> <code>int</code> <p>Number of steps for the linear warm-up.</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>Total number of training steps.</p> <code>None</code> <code>max_lr</code> <code>float</code> <p>Peak learning rate to be achieved after warm-up.</p> <code>None</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>4e-05</code> <code>anneal_percentage</code> <code>float</code> <p>Percentage of the max_lr to hold after decay.</p> <code>0.1</code> <code>last_epoch</code> <code>int</code> <p>The index of the last epoch.</p> <code>-1</code> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    optimizer: MegatronOptimizerModule,\n    *,\n    warmup_steps: Optional[int] = None,\n    max_steps: Optional[int] = None,\n    max_lr: Optional[float] = None,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    last_epoch: int = -1,\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHold learning rate scheduler.\n\n    Args:\n        optimizer: Optimizer to apply the learning rate scheduler.\n        warmup_steps (int): Number of steps for the linear warm-up.\n        max_steps (int): Total number of training steps.\n        max_lr (float): Peak learning rate to be achieved after warm-up.\n        min_lr (float): Minimum learning rate.\n        anneal_percentage (float): Percentage of the max_lr to hold after decay.\n        last_epoch (int): The index of the last epoch.\n    \"\"\"\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.last_epoch = last_epoch\n\n    for group in optimizer.param_groups:\n        group.setdefault(\"initial_lr\", max_lr)\n\n    super(WarmupAnnealDecayHold, self).__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHold.get_lr","title":"<code>get_lr()</code>","text":"<p>Get the learning rate at the current step.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def get_lr(self) -&gt; List[float]:\n    \"\"\"Get the learning rate at the current step.\"\"\"\n    step_num = self.last_epoch\n    if step_num &lt; self.warmup_steps:\n        lr = self.min_lr + (self.max_lr - self.min_lr) * step_num / self.warmup_steps\n    else:\n        decay_steps = self.max_steps - self.warmup_steps\n        lr = self.max_lr * (1 - (step_num - self.warmup_steps) / decay_steps)\n        lr = max(lr, self.max_lr * self.anneal_percentage)\n\n    return [lr for _ in self.optimizer.param_groups]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler","title":"<code>WarmupAnnealDecayHoldScheduler</code>","text":"<p>               Bases: <code>LRSchedulerModule</code></p> <p>Warmup Policy Learning Rate Scheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>class WarmupAnnealDecayHoldScheduler(LRSchedulerModule):\n    \"\"\"Warmup Policy Learning Rate Scheduler.\"\"\"\n\n    def __init__(\n        self,\n        warmup_steps: int = 2000,\n        max_steps: int = 500_000,\n        max_lr: float = 4e-4,\n        min_lr: float = 4e-5,\n        anneal_percentage: float = 0.10,\n        interval: str = \"step\",\n        frequency: int = 1,\n        monitor: str = \"val_loss\",\n    ) -&gt; None:\n        \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n        super().__init__()\n        self.warmup_steps = warmup_steps\n        self.max_steps = max_steps\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n        self.anneal_percentage = anneal_percentage\n        self.interval = interval\n        self.frequency = frequency\n        self.monitor = monitor\n\n    def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n        \"\"\"Returns the scheduler output.\"\"\"\n        lr_scheduler = WarmupAnnealDecayHold(\n            optimizer,\n            warmup_steps=self.warmup_steps,\n            max_steps=self.max_steps,\n            max_lr=self.max_lr,\n            min_lr=self.min_lr,\n            anneal_percentage=self.anneal_percentage,\n        )\n        return {\n            \"optimizer\": optimizer,\n            # REQUIRED: The scheduler instance\n            \"lr_scheduler\": {\n                \"scheduler\": lr_scheduler,\n                # `interval` is the unit of the scheduler's step size, could also be 'step'.\n                # 'epoch' updates the scheduler on epoch end whereas 'step'\n                # updates it after a optimizer update.\n                \"interval\": self.interval,\n                # How many epochs/steps should pass between calls to\n                # `scheduler.step()`. 1 corresponds to updating the learning\n                # rate after every epoch/step.\n                \"frequency\": self.frequency,\n            },\n            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n            \"monitor\": self.monitor,\n        }\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.__init__","title":"<code>__init__(warmup_steps=2000, max_steps=500000, max_lr=0.0004, min_lr=4e-05, anneal_percentage=0.1, interval='step', frequency=1, monitor='val_loss')</code>","text":"<p>Initializes the WarmupAnnealDecayHoldScheduler.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def __init__(\n    self,\n    warmup_steps: int = 2000,\n    max_steps: int = 500_000,\n    max_lr: float = 4e-4,\n    min_lr: float = 4e-5,\n    anneal_percentage: float = 0.10,\n    interval: str = \"step\",\n    frequency: int = 1,\n    monitor: str = \"val_loss\",\n) -&gt; None:\n    \"\"\"Initializes the WarmupAnnealDecayHoldScheduler.\"\"\"\n    super().__init__()\n    self.warmup_steps = warmup_steps\n    self.max_steps = max_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.anneal_percentage = anneal_percentage\n    self.interval = interval\n    self.frequency = frequency\n    self.monitor = monitor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/lr_scheduler/#bionemo.llm.model.lr_scheduler.WarmupAnnealDecayHoldScheduler.scheduler","title":"<code>scheduler(model, optimizer)</code>","text":"<p>Returns the scheduler output.</p> Source code in <code>bionemo/llm/model/lr_scheduler.py</code> <pre><code>def scheduler(self, model: MegatronBioBertModel, optimizer: MegatronOptimizerModule) -&gt; SchedulerOutput:\n    \"\"\"Returns the scheduler output.\"\"\"\n    lr_scheduler = WarmupAnnealDecayHold(\n        optimizer,\n        warmup_steps=self.warmup_steps,\n        max_steps=self.max_steps,\n        max_lr=self.max_lr,\n        min_lr=self.min_lr,\n        anneal_percentage=self.anneal_percentage,\n    )\n    return {\n        \"optimizer\": optimizer,\n        # REQUIRED: The scheduler instance\n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            # `interval` is the unit of the scheduler's step size, could also be 'step'.\n            # 'epoch' updates the scheduler on epoch end whereas 'step'\n            # updates it after a optimizer update.\n            \"interval\": self.interval,\n            # How many epochs/steps should pass between calls to\n            # `scheduler.step()`. 1 corresponds to updating the learning\n            # rate after every epoch/step.\n            \"frequency\": self.frequency,\n        },\n        # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n        \"monitor\": self.monitor,\n    }\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatch","title":"<code>BertBatch</code>","text":"<p>               Bases: <code>BertBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatch(BertBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertBatchCore","title":"<code>BertBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    text: Tensor\n    attention_mask: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel","title":"<code>BertModel</code>","text":"<p>               Bases: <code>Protocol[DataT]</code></p> <p>Interface for BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BertModel(Protocol[DataT]):\n    \"\"\"Interface for BERT-like models.\"\"\"\n\n    def forward(\n        self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n    ) -&gt; DataT:\n        \"\"\"Inference for BERT-like models.\n\n        Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n        and the original sequence lengths if the sequences are packed into a dense batch.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BertModel.forward","title":"<code>forward(input_ids, attention_mask, packed_seq_params=None)</code>","text":"<p>Inference for BERT-like models.</p> <p>Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input, and the original sequence lengths if the sequences are packed into a dense batch.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def forward(\n    self, input_ids: Tensor, attention_mask: Tensor, packed_seq_params: Optional[PackedSeqParams] = None\n) -&gt; DataT:\n    \"\"\"Inference for BERT-like models.\n\n    Inference for BERT-like models require their tokenized inputs by IDs, an attention mask over the input,\n    and the original sequence lengths if the sequences are packed into a dense batch.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule","title":"<code>BioBertLightningModule</code>","text":"<p>               Bases: <code>BionemoLightningModule</code></p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class BioBertLightningModule(BionemoLightningModule):\n    def __init__(\n        self,\n        *args,\n        data_step_function: DataStepFunction = biobert_data_step,\n        forward_step_function: ForwardStepFunction = bert_forward_step,\n        **kwargs,\n    ):\n        \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n        This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n        `data_step`.\n\n        Args:\n            *args: all args are passed through to BionemoLightningModule\n            data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n            forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n            **kwargs: all other kwargs are passed through to BionemoLightningModule.\n        \"\"\"  # noqa: D205\n        super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.BioBertLightningModule.__init__","title":"<code>__init__(*args, data_step_function=biobert_data_step, forward_step_function=bert_forward_step, **kwargs)</code>","text":"<p>DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints. This maps the old name <code>forward_step_function</code> to the new name <code>forward_step</code> and <code>data_step_function</code> to <code>data_step</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>all args are passed through to BionemoLightningModule</p> <code>()</code> <code>data_step_function</code> <code>DataStepFunction</code> <p>The data step function. Defaults to biobert_data_step.</p> <code>biobert_data_step</code> <code>forward_step_function</code> <code>ForwardStepFunction</code> <p>The forward step function. Defaults to bert_forward_step.</p> <code>bert_forward_step</code> <code>**kwargs</code> <p>all other kwargs are passed through to BionemoLightningModule.</p> <code>{}</code> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    data_step_function: DataStepFunction = biobert_data_step,\n    forward_step_function: ForwardStepFunction = bert_forward_step,\n    **kwargs,\n):\n    \"\"\"DEPRECATED! Please use BionemoLightningModule. This is here so we can load older checkpoints.\n    This maps the old name `forward_step_function` to the new name `forward_step` and `data_step_function` to\n    `data_step`.\n\n    Args:\n        *args: all args are passed through to BionemoLightningModule\n        data_step_function (DataStepFunction, optional): The data step function. Defaults to biobert_data_step.\n        forward_step_function (ForwardStepFunction, optional): The forward step function. Defaults to bert_forward_step.\n        **kwargs: all other kwargs are passed through to BionemoLightningModule.\n    \"\"\"  # noqa: D205\n    super().__init__(*args, forward_step=forward_step_function, data_step=data_step_function, **kwargs)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatch","title":"<code>SequenceBatch</code>","text":"<p>               Bases: <code>SequenceBatchCore</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatch(SequenceBatchCore, total=False):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens_argmin: Tensor\n    max_seqlen: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.SequenceBatchCore","title":"<code>SequenceBatchCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Input datatype for inference with BERT-like models.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>class SequenceBatchCore(TypedDict):\n    \"\"\"Input datatype for inference with BERT-like models.\"\"\"\n\n    cu_seqlens: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.bert_default_optimizer","title":"<code>bert_default_optimizer(model)</code>","text":"<p>Returns the default optimizer for the BERT model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The BERT model.</p> required <p>Returns:</p> Type Description <code>FusedAdam</code> <p>The default optimizer initialized for this BERT module's parameters.</p> <code>FusedAdam</code> <p>Uses a learning rate of 1e-4 and weight decay of 1e-2.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def bert_default_optimizer(model: torch.nn.Module) -&gt; FusedAdam:\n    \"\"\"Returns the default optimizer for the BERT model.\n\n    Args:\n        model: The BERT model.\n\n    Returns:\n        The default optimizer initialized for this BERT module's parameters.\n        Uses a learning rate of 1e-4 and weight decay of 1e-2.\n    \"\"\"\n    return FusedAdam(model.parameters(), lr=1e-4, weight_decay=0.01)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.bert_forward_step","title":"<code>bert_forward_step(model, batch)</code>","text":"<p>Performs the model's forward pass using the batch, for Megatron compatibility.</p> <p>This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the model for forward pass efficiency.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def bert_forward_step(model: BertModel[DataT], batch: BertBatch) -&gt; DataT:\n    \"\"\"Performs the model's forward pass using the batch, for Megatron compatibility.\n\n    This subsets the batch keys to the ones actually used by forward pass of the model, and then calls the model's\n    forward pass. if \"cu_seqsens\" are defined in the batch, then the packed sequence parameters are also passed to the\n    model for forward pass efficiency.\n    \"\"\"\n    if \"cu_seqlens\" in batch:\n        forward_results = model.forward(\n            input_ids=batch[\"text\"],\n            attention_mask=batch[\"attention_mask\"],\n            packed_seq_params=get_packed_seq_params(cast(SequenceBatch, batch)),\n        )\n    else:\n        forward_results = model.forward(input_ids=batch[\"text\"], attention_mask=batch[\"attention_mask\"])\n    # TODO support losses that also include the binary head, this means doing something more fancy than the one\n    #      default GPT reduction function above MaskedTokenLossReduction()\n    return forward_results\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_data_step","title":"<code>biobert_data_step(dataloader_iter)</code>","text":"<p>Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.     only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.     TODO document how parallel_state pipeline stages work.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader_iter</code> <p>An iterator over the dataloader.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Dict[str, Tensor]</code> <p>A dictionary of this batch limiting to relevant keys.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_data_step(dataloader_iter) -&gt; Dict[str, Tensor]:\n    \"\"\"Preprocesses a batch of data for the GeneFormer model, and ingest a single batch of data from the dataloader iterator.\n        only necessary batch keys are subsetted and passed to the model's forward pass, and the loss forward pass, depending on stage.\n        TODO document how parallel_state pipeline stages work.\n\n    Args:\n        dataloader_iter: An iterator over the dataloader.\n\n    Returns:\n        output: A dictionary of this batch limiting to relevant keys.\n\n    \"\"\"  # noqa: D205\n    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87\n    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842\n\n    batch = next(dataloader_iter)\n\n    if isinstance(batch, tuple) and len(batch) == 3:\n        _batch: dict = batch[0]\n    else:\n        _batch = batch\n\n    required_keys = set()\n    required_keys.add(\"attention_mask\")\n    if parallel_state.is_pipeline_first_stage():\n        required_keys.add(\"text\")\n    if parallel_state.is_pipeline_last_stage():\n        required_keys.update((\"labels\", \"loss_mask\", \"types\", \"is_random\"))\n    # if self.get_attention_mask_from_fusion:\n    #     required_keys.remove('attention_mask')\n\n    _batch = {key: val.cuda(non_blocking=True) if key in required_keys else None for key, val in _batch.items()}\n    # slice batch along sequence dimension for context parallelism\n    output = get_batch_on_this_context_parallel_rank(_batch)\n\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.biobert_lightning_module","title":"<code>biobert_lightning_module(config, optimizer=None, tokenizer=None, data_step=biobert_data_step, forward_step=bert_forward_step, model_transform=None, **model_construct_args)</code>","text":"<p>A pytorch lightning module for BioBert-derived models.</p> <p>This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions. To change your loss, pass in a different config object that returns a different loss reduction class. To change your model and what it outputs, pass in a different config object that returns a different model. Do not modify this function unless you need to change higher level logic. You may need to modify the various step and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some of those functions may need to be refactored out into the config object or a different place so that they live closer to the model definition.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def biobert_lightning_module(\n    config: BioBertConfig[MegatronBioBertModel, MegatronLossReduction],\n    optimizer: Optional[MegatronOptimizerModule] = None,\n    tokenizer: Optional[TokenizerSpec | PreTrainedTokenizerBase] = None,\n    data_step: DataStep = biobert_data_step,\n    forward_step: ForwardStep = bert_forward_step,\n    model_transform: Optional[Callable] = None,\n    **model_construct_args,\n) -&gt; BionemoLightningModule[MegatronBioBertModel, MegatronLossReduction]:\n    \"\"\"A pytorch lightning module for BioBert-derived models.\n\n    This module is designed to be used with the Megatron-LM strategy and nemo 2.0 conventions.\n    To change your loss, pass in a different config object that returns a different loss reduction class.\n    To change your model and what it outputs, pass in a different config object that returns a different model.\n    Do not modify this function unless you need to change higher level logic. You may need to modify the various step\n    and forward functions towards the bottom of this file to handle new/different keys in the batch. In the future some\n    of those functions may need to be refactored out into the config object or a different place so that they live\n    closer to the model definition.\n    \"\"\"\n    return BionemoLightningModule(\n        config=config,\n        optimizer=optimizer if optimizer is not None else default_megatron_optimizer(),\n        data_step=data_step,\n        forward_step=forward_step,\n        tokenizer=tokenizer,\n        model_transform=model_transform,\n        **model_construct_args,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_batch_on_this_context_parallel_rank","title":"<code>get_batch_on_this_context_parallel_rank(batch, in_place=True)</code>","text":"<p>Ensures that the input batch is in the right format for context parallel rank.</p> <p>Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1. Otherwise, the batch is returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Tensor]</code> <p>The input batch data.</p> required <code>in_place</code> <code>bool</code> <p>If true, then the input is mutated. The returned dict is a reference to the input.       Otherwise, the input data is always shallow-copied and this copy is modified and returned.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Tensor]</code> <p>The modified batch data based on the context parallel rank.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_batch_on_this_context_parallel_rank(batch: Dict[str, Tensor], in_place: bool = True) -&gt; Dict[str, Tensor]:\n    \"\"\"Ensures that the input batch is in the right format for context parallel rank.\n\n    Modifies the batch data based on the context parallel rank, if the context parallel world size is greater than 1.\n    Otherwise, the batch is returned as-is.\n\n\n    Args:\n        batch: The input batch data.\n        in_place: If true, then the input is mutated. The returned dict is a reference to the input.\n                  Otherwise, the input data is always shallow-copied and this copy is modified and returned.\n\n    Returns:\n        dict: The modified batch data based on the context parallel rank.\n    \"\"\"\n    if not in_place:\n        batch: dict[str, Tensor] = dict(**batch)\n\n    if cp_size := parallel_state.get_context_parallel_world_size() &gt; 1:\n        num_valid_tokens_in_ub: Tensor | None = None\n        if \"loss_mask\" in batch and batch[\"loss_mask\"] is not None:\n            num_valid_tokens_in_ub = batch[\"loss_mask\"].sum()\n\n        cp_rank = parallel_state.get_context_parallel_rank()\n        for key, val in batch.items():\n            if val is not None:\n                seq_dim = 1 if key != \"attention_mask\" else 2\n                _val = val.view(\n                    *val.shape[0:seq_dim],\n                    2 * cp_size,\n                    val.shape[seq_dim] // (2 * cp_size),\n                    *val.shape[(seq_dim + 1) :],\n                )\n                index = torch.tensor([cp_rank, (2 * cp_size - cp_rank - 1)], device=\"cpu\", pin_memory=True).cuda(\n                    non_blocking=True\n                )\n                _val = _val.index_select(seq_dim, index)\n                _val = _val.view(*val.shape[0:seq_dim], -1, *_val.shape[(seq_dim + 2) :])\n                batch[key] = _val\n        batch[\"num_valid_tokens_in_ub\"] = num_valid_tokens_in_ub  # type: ignore\n\n    return batch\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/lightning/#bionemo.llm.model.biobert.lightning.get_packed_seq_params","title":"<code>get_packed_seq_params(batch)</code>","text":"<p>Get the packed sequence parameters for the given batch.</p> <p>This function should only be called if <code>cu_seqlens</code> is defined in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SequenceBatch</code> <p>The input batch to pack.</p> required <p>Returns:</p> Name Type Description <code>PackedSeqParams</code> <code>PackedSeqParams</code> <p>The packed sequence parameters containing the following attributes: - cu_seqlens_q (Tensor): The sequence lengths for query. - cu_seqlens_kv (Tensor): The sequence lengths for key and value. - max_seqlen_q (Tensor, optional): The maximum sequence length for query. - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value. - qkv_format (str): The format of query, key, and value tensors.</p> Source code in <code>bionemo/llm/model/biobert/lightning.py</code> <pre><code>def get_packed_seq_params(batch: SequenceBatch) -&gt; PackedSeqParams:\n    \"\"\"Get the packed sequence parameters for the given batch.\n\n    This function should only be called if `cu_seqlens` is defined in the batch.\n\n    Args:\n        batch: The input batch to pack.\n\n    Returns:\n        PackedSeqParams: The packed sequence parameters containing the following attributes:\n            - cu_seqlens_q (Tensor): The sequence lengths for query.\n            - cu_seqlens_kv (Tensor): The sequence lengths for key and value.\n            - max_seqlen_q (Tensor, optional): The maximum sequence length for query.\n            - max_seqlen_kv (Tensor, optional): The maximum sequence length for key and value.\n            - qkv_format (str): The format of query, key, and value tensors.\n\n    \"\"\"\n    cu_seqlens = batch[\"cu_seqlens\"].squeeze()  # remove batch size dimension (mbs=1)\n    # remove -1 \"paddings\" added in collate_fn\n    if cu_seqlens_argmin := batch.get(\"cu_seqlens_argmin\", None) is not None:\n        # pre-compute cu_seqlens_argmin in dataset class for perf\n        cu_seqlens = cu_seqlens[: cu_seqlens_argmin.item()]\n    else:\n        cu_seqlens = cu_seqlens[: torch.argmin(cu_seqlens)]\n\n    # pre-compute max_seqlens in dataset class for perf\n    max_seqlen = batch[\"max_seqlen\"].squeeze() if \"max_seqlen\" in batch else None\n\n    # these args are passed eventually into TEDotProductAttention.forward()\n    return PackedSeqParams(\n        cu_seqlens_q=cu_seqlens,\n        cu_seqlens_kv=cu_seqlens,\n        max_seqlen_q=max_seqlen,\n        max_seqlen_kv=max_seqlen,\n        qkv_format=\"thd\",\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/","title":"Model","text":""},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModelType","title":"<code>MegatronBioBertModelType = TypeVar('MegatronBioBertModelType', bound=MegatronBioBertModel)</code>  <code>module-attribute</code>","text":"<p>A megatron model that is or extends the MegatronBioBertModel.</p>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.PositionEmbeddingKinds","title":"<code>PositionEmbeddingKinds = Literal['learned_absolute', 'rope']</code>  <code>module-attribute</code>","text":"<p>Kinds of supported positional embeddings.</p>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertConfig","title":"<code>BioBertConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType]</code></p> <p>Config class for BioBert model, responsible for the partial configuration of Transformer models.</p> <p>NOTE: do not use this config directly, define a child config that overrides items from this parent config</p> <p><code>configure_model()</code> is ultimately called by the LightningModule using PTL lightning module hooks.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>@dataclass\nclass BioBertConfig(\n    MegatronBioNeMoTrainableModelConfig[MegatronBioBertModelType, MegatronLossType],\n):\n    \"\"\"Config class for BioBert model, responsible for the partial configuration of Transformer models.\n\n    NOTE: do not use this config directly, define a child config that overrides items from this parent config\n\n    `configure_model()` is ultimately called by the LightningModule using PTL lightning module hooks.\n    \"\"\"\n\n    # From megatron.core.models.gpt.bert_model.GPTModel\n    kv_channels: int | None = None\n    fp16_lm_cross_entropy: bool = False\n    apply_rope_fusion: bool = True\n    parallel_output: bool = True\n    bias_dropout_fusion: bool = True\n    bias_activation_fusion: bool = True\n    masked_softmax_fusion: bool = True\n    persist_layer_norm: bool = True\n    get_attention_mask_from_fusion: bool = True\n    share_embeddings_and_output_weights: bool = False  # try True\n    make_vocab_size_divisible_by: int = 128\n    position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\"\n    rotary_base: int = 10000\n    rotary_percent: float = 1.0\n    seq_len_interpolation_factor: Optional[float] = None\n    seq_length: int = 1024\n    hidden_size: int = 512\n    num_attention_heads: int = 8\n    num_layers: int = 6\n    init_method_std: float = 0.02\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    optimizer_fn: Optional[Callable[[\"MegatronBioBertModel\"], Optimizer]] = None\n    # TODO (@skothenhill,@georgea) update to use the nemo2 checkpoint mixins\n    #  support HF (requires weight interleaving on qkv layer) and nemo1 checkpoints ideally.\n    # TODO (@skothenhill,@jstjohn) come up with a nice way of doing fine-tuning checkpoint loading,\n    #  where some acceptible layers (eg lm_head) may or may not be absent from the model, and others\n    #  (like a new head) may be new and missing from the initial checkpoint.\n    nemo1_ckpt_path: Optional[str] = None\n\n    initial_ckpt_path: Optional[str] = None\n    # TODO(@jstjohn, @skothenhill) Was this supposed to be only on the child?\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n    # Used if initializing from a checkpoint, set this to any fields you want to override rather than re-set.\n    #  by default all fields will be overridden.\n    override_parent_fields: List[str] = field(default_factory=lambda: _OVERRIDE_BIOBERT_CONFIG_DEFAULTS)\n    return_embeddings: bool = False\n    include_embeddings: bool = False\n    return_only_hidden_states: bool = False\n    include_hiddens: bool = False  # Include hidden layers in the output of the model\n    skip_logits: bool = False  # useful for inference\n    core_attention_override: Type[torch.nn.Module] | None = None\n\n    # loss reduction class\n    loss_reduction_class: Type[MegatronLossType] = BERTMLMLossWithReduction\n\n    def configure_model(self, tokenizer: AutoTokenizer) -&gt; MegatronBioBertModelType:  # noqa: D102\n        vp_size = self.virtual_pipeline_model_parallel_size\n        if vp_size:\n            p_size = self.pipeline_model_parallel_size\n            assert (\n                self.num_layers // p_size\n            ) % vp_size == 0, \"Make sure the number of model chunks is the same across all pipeline stages.\"\n\n        # The local specs all require the standard full attention mask. For transformer engine only the NVTE_FLASH_ATTN=0\n        #  option requires this full attention mask.\n        use_full_attention_mask: bool = \"transformer_engine\" not in self.biobert_spec_option\n        do_next_sentence = False\n        if self.model_cls is None:\n            raise ValueError(\n                f\"You must supply `model_cls` to the {type(self)} for module to initialization in `configure_model`.\"\n            )\n\n        if self.initial_ckpt_path:\n            self.load_settings_from_checkpoint(self.initial_ckpt_path)\n\n        model = self.model_cls(\n            self,\n            transformer_layer_spec=get_biobert_spec(\n                self.biobert_spec_option,\n                qk_layernorm=self.qk_layernorm,\n                core_attention=self.core_attention_override,\n            ),\n            num_tokentypes=2 if do_next_sentence else 0,\n            vocab_size=get_vocab_size(self, tokenizer.vocab_size, self.make_vocab_size_divisible_by),\n            max_sequence_length=self.seq_length,\n            tokenizer=tokenizer,\n            fp16_lm_cross_entropy=self.fp16_lm_cross_entropy,\n            parallel_output=self.parallel_output,\n            share_embeddings_and_output_weights=self.share_embeddings_and_output_weights,\n            position_embedding_type=self.position_embedding_type,\n            rotary_percent=self.rotary_percent,\n            seq_len_interpolation_factor=self.seq_len_interpolation_factor,\n            return_embeddings=self.return_embeddings,\n            include_embeddings=self.include_embeddings,\n            pre_process=parallel_state.is_pipeline_first_stage(),\n            post_process=parallel_state.is_pipeline_last_stage(),  # set to False for inference\n            add_binary_head=do_next_sentence,\n            use_full_attention_mask=use_full_attention_mask,\n            include_hiddens=self.include_hiddens,\n            skip_logits=self.skip_logits,\n        )\n        # TODO (@skothenhill) this is a hack to load the old checkpoint.\n        # This should be removed once we have a proper checkpoint conversion\n        # see NeMo/nemo/collections/llm/gpt/model/mixtral.py for how we should do it.\n        # We should eventually have an adapter for nemo1 checkpoints, HF checkpoints (at least for ESM2 @georgea)\n        # and an adapter may also be the right way to handle expected missing/extra keys when importing\n        # a checkpoint for fine-tuning (eg ignore misisng lm_head, if not there in model, etc).\n        if self.nemo1_ckpt_path is not None:\n            assert self.initial_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            te_mapping = \"transformer_engine\" in self.biobert_spec_option.value\n            with tarfile.open(self.nemo1_ckpt_path, \"r\") as old_ckpt:\n                ckpt_file = old_ckpt.extractfile(\"./model_weights.ckpt\")\n                if ckpt_file is None:\n                    raise ValueError(f\"Failure to read checkpoint file: {old_ckpt}/model_weights/ckpt\")\n                old_weights = torch.load(ckpt_file)\n                new_state_dict_from_old = {}\n                for k, v in old_weights.items():\n                    new_key = nemo1_to_nemo2_biobert_key_mapping(k, new_model_prefix=\"\", te_mapping=te_mapping)\n                    new_state_dict_from_old[new_key] = v\n                # TE adds non-null ._extra_state objects to layers, which store some kind of buffer bits\n                #  so we need to allow those to pass through if we're loading from bionemo1 which did not\n                #  use TE.\n                model.load_state_dict(new_state_dict_from_old, strict=not te_mapping)\n        if self.initial_ckpt_path is not None:\n            assert self.nemo1_ckpt_path is None, \"Mutually exclusive checkpoint path used twice\"\n            self.update_model_from_checkpoint(model, self.initial_ckpt_path)\n\n        # TODO (@jstjohn) come up with a cleaner way in the biobert module to return hidden states.\n        #  maybe a suite of options like hugging face has so a user can ask for several or only one thing.\n        if self.return_only_hidden_states:\n            # this applies the final layernorm in the encoder to the hidden states which was\n            #  the default in nemo1.\n            model.post_process = False\n            model.encoder.post_process = True\n            model.encoder.post_layer_norm = True\n        return model\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossType]:  # noqa: D102\n        # You could optionally return a different loss reduction class here based on the config settings.\n        return self.loss_reduction_class\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutput","title":"<code>BioBertOutput</code>","text":"<p>               Bases: <code>BioBertOutputCore</code></p> <p>The megatron bionemo bert model inference type.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutput(BioBertOutputCore, total=False):\n    \"\"\"The megatron bionemo bert model inference type.\"\"\"\n\n    hidden_states: Tensor\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.BioBertOutputCore","title":"<code>BioBertOutputCore</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Keys always present in the bionemo bert model inference output.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class BioBertOutputCore(TypedDict):\n    \"\"\"Keys always present in the bionemo bert model inference output.\"\"\"\n\n    token_logits: Tensor\n    binary_logits: Optional[Tensor]\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel","title":"<code>MegatronBioBertModel</code>","text":"<p>               Bases: <code>LanguageModule</code></p> <p>Transformer language model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TransformerConfig</code> <p>transformer config</p> required <code>num_tokentypes</code> <code>int</code> <p>Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.</p> required <code>transformer_layer_spec</code> <code>ModuleSpec</code> <p>Specifies module to use for transformer layers</p> required <code>vocab_size</code> <code>int</code> <p>vocabulary size</p> required <code>max_sequence_length</code> <code>int</code> <p>maximum size of sequence. This is used for positional embedding</p> required <code>pre_process</code> <code>bool</code> <p>Include embedding layer (used with pipeline parallelism)</p> <code>True</code> <code>post_process</code> <code>bool</code> <p>Include an output layer (used with pipeline parallelism)</p> <code>True</code> <code>parallel_output</code> <code>bool</code> <p>Do not gather the outputs, keep them split across tensor parallel ranks</p> <code>True</code> <code>share_embeddings_and_output_weights</code> <code>bool</code> <p>When True, input embeddings and output logit weights are shared. Defaults to False.</p> <code>False</code> <code>position_embedding_type</code> <code>PositionEmbeddingKinds</code> <p>Position embedding type. Options [\"learned_absolute\", \"rope\"]. Defaults is 'learned_absolute'.</p> <code>'learned_absolute'</code> <code>rotary_percent</code> <code>float</code> <p>Percent of rotary dimension to use for rotary position embeddings. Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.</p> <code>1.0</code> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>class MegatronBioBertModel(LanguageModule):\n    \"\"\"Transformer language model.\n\n    Args:\n        config: transformer config\n        num_tokentypes: Set to 2 when args.bert_binary_head is True, and 0 otherwise. Defaults to 0.\n        transformer_layer_spec: Specifies module to use for transformer layers\n        vocab_size: vocabulary size\n        max_sequence_length: maximum size of sequence. This is used for positional embedding\n        pre_process: Include embedding layer (used with pipeline parallelism)\n        post_process: Include an output layer (used with pipeline parallelism)\n        parallel_output: Do not gather the outputs, keep them split across tensor parallel ranks\n        share_embeddings_and_output_weights: When True, input embeddings and output logit weights are shared.\n            Defaults to False.\n        position_embedding_type: Position embedding type. Options [\"learned_absolute\", \"rope\"].\n            Defaults is 'learned_absolute'.\n        rotary_percent: Percent of rotary dimension to use for rotary position embeddings.\n            Defaults to 1.0 (100%). Ignored unless position_embedding_type is 'rope'.\n    \"\"\"\n\n    def __init__(  # noqa: D107\n        self,\n        config: TransformerConfig,\n        num_tokentypes: int,\n        transformer_layer_spec: ModuleSpec,\n        vocab_size: int,\n        max_sequence_length: int,\n        tokenizer: Optional[AutoTokenizer] = None,\n        pre_process: bool = True,\n        post_process: bool = True,\n        fp16_lm_cross_entropy: bool = False,\n        parallel_output: bool = True,\n        share_embeddings_and_output_weights: bool = False,\n        position_embedding_type: PositionEmbeddingKinds = \"learned_absolute\",\n        rotary_percent: float = 1.0,\n        seq_len_interpolation_factor: Optional[float] = None,\n        add_binary_head: bool = True,\n        return_embeddings: bool = False,\n        include_embeddings: bool = False,\n        use_full_attention_mask: bool = False,\n        include_hiddens: bool = False,\n        skip_logits: bool = False,  # Useful for inference time.\n    ):\n        # TODO (@jstjohn) come up with a cleaner way for this model to return a set of things the user wants.\n        #  hidden states, embeddings, logits, etc. The defaults should work for training but we need to make it\n        #  customizable and easy to tell how to make it work well for inference as well as trouble shooting.\n        #  Also make sure that everything returned that the user wants gets transposed to the b,s,h format.\n        super(MegatronBioBertModel, self).__init__(config=config)\n        self.post_process = post_process\n        self.add_binary_head = add_binary_head\n        self.skip_logits = skip_logits\n        if return_embeddings:\n            assert self.post_process, \"only return embeddings on the last pipeline stage\"\n        # `b` = batch, `s` = sequence.\n        # The old flash attention mechanism apparently wants you to use a b x 1 x s x s attention mask while\n        #  the new one wants a b x 1 x 1 x s attention mask. This is a hack to allow us to switch between the two.\n        self.use_full_attention_mask = use_full_attention_mask\n        self.config: TransformerConfig = config\n        self.transformer_layer_spec: ModuleSpec = transformer_layer_spec\n        self.vocab_size = vocab_size\n        self.max_sequence_length = max_sequence_length\n        self.tokenizer = tokenizer\n        self.pre_process = pre_process\n        self.post_process = post_process\n        self.fp16_lm_cross_entropy = fp16_lm_cross_entropy\n        self.parallel_output = parallel_output\n        self.share_embeddings_and_output_weights = share_embeddings_and_output_weights\n        self.position_embedding_type = position_embedding_type\n        self.add_binary_head = add_binary_head\n        self.return_embeddings = return_embeddings\n        self.include_embeddings = include_embeddings\n        self.include_hiddens = include_hiddens\n        self.skip_logits = skip_logits\n\n        # megatron core pipelining currently depends on model type\n        self.model_type = ModelType.encoder_or_decoder\n\n        # Embeddings.\n        if self.pre_process:\n            self.embedding = LanguageModelEmbedding(\n                config=self.config,\n                vocab_size=self.vocab_size,\n                max_sequence_length=self.max_sequence_length,\n                position_embedding_type=position_embedding_type,\n                num_tokentypes=num_tokentypes,\n            )\n\n        if self.position_embedding_type == \"rope\":\n            self.rotary_pos_emb = RotaryEmbedding(\n                kv_channels=self.config.kv_channels,\n                rotary_percent=rotary_percent,\n                rotary_interleaved=self.config.rotary_interleaved,\n                # bug in megatron: they list the type as `float` but they default to `None` so it should be `Optional[float]`\n                seq_len_interpolation_factor=seq_len_interpolation_factor,  # type: ignore\n            )\n\n        # Transformer.\n        self.encoder = TransformerBlock(\n            config=self.config,\n            spec=self.transformer_layer_spec,\n            pre_process=self.pre_process,\n            post_process=self.post_process,  # NOTE: in bionemo1 this is hard-coded to True\n        )\n\n        # Output\n        if post_process:\n            # TODO: Make sure you are passing in the mpu_vocab_size properly\n            self.lm_head = BertLMHead(\n                config.hidden_size,\n                config,\n            )\n\n            self.output_layer = tensor_parallel.ColumnParallelLinear(\n                config.hidden_size,\n                self.vocab_size,\n                config=config,\n                init_method=config.init_method,\n                is_expert=False,\n                bias=True,\n                skip_bias_add=False,\n                gather_output=not self.parallel_output,\n                skip_weight_param_allocation=pre_process and share_embeddings_and_output_weights,\n            )\n\n            self.binary_head = None\n            if self.add_binary_head:\n                # TODO: Shoudl switch this to TE ?\n                self.binary_head = get_linear_layer(\n                    config.hidden_size, 2, config.init_method, config.perform_initialization\n                )\n                self.pooler = Pooler(config.hidden_size, config.init_method, config, config.sequence_parallel)\n\n        if self.pre_process or self.post_process:\n            self.setup_embeddings_and_output_layer()\n\n    def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n        \"\"\"Creates the extended attention mask\n\n        Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n        Args:\n            attention_mask (Tensor): The input attention mask\n\n        Returns:\n            Tensor: The extended binary attention mask\n        \"\"\"  # noqa: D415\n        # We create a 3D attention mask from a 2D tensor mask.\n        # [b, 1, s]\n        attention_mask_b1s = attention_mask.unsqueeze(1)\n\n        if self.use_full_attention_mask:\n            # [b, s, 1]\n            attention_mask_bs1 = attention_mask.unsqueeze(2)\n            # [b, s, s]\n            attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n            # [b, 1, s, s]\n            extended_attention_mask = attention_mask_bss.unsqueeze(1)\n        else:\n            # Tensor Engine requires a 1x1xS attention mask which it internally\n            #  converts into a 1xSxS mask.\n            # [b, 1, 1, s]\n            extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n        # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n        #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n        #  masking out pad positions.\n        extended_attention_mask = extended_attention_mask &lt; 0.5\n\n        return extended_attention_mask\n\n    def bert_position_ids(self, token_ids):  # noqa: D102\n        # Create position ids\n        seq_length = token_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=token_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(token_ids)\n        return position_ids\n\n    def embedding_forward(\n        self,\n        input_ids: Tensor,\n        position_ids: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        attention_mask: Optional[Tensor] = None,\n    ) -&gt; Tensor:\n        \"\"\"Produce embeddings.\"\"\"\n        return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n\n    def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n        \"\"\"Sets input tensor to the model.\n\n        See megatron.model.transformer.set_input_tensor()\n\n        Args:\n            input_tensor: Sets the input tensor for the model.\n\n        Raises:\n            ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n        \"\"\"\n        # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n        if isinstance(input_tensor, list):\n            if len(input_tensor) != 1:\n                raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n            single_input_tensor: Tensor = input_tensor[0]\n        else:\n            single_input_tensor = input_tensor\n        self.encoder.set_input_tensor(single_input_tensor)\n\n    def forward(\n        self,\n        input_ids: Tensor,\n        attention_mask: Tensor,\n        tokentype_ids: Optional[Tensor] = None,\n        lm_labels: Optional[Tensor] = None,\n        inference_params: Any | None = None,\n    ) -&gt; BioBertOutput | Tensor:\n        \"\"\"Forward function of BERT model\n\n        Forward function of the BERT Model This function passes the input tensors\n        through the embedding layer, and then the encoder and finally into the post\n        processing layer (optional).\n\n        It either returns the Loss values if labels are given or the final hidden units.\n        \"\"\"  # noqa: D415\n        # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n        #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n        #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n        extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n        if parallel_state.is_pipeline_first_stage():\n            using_input_ids: Optional[Tensor] = input_ids\n            using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n        else:\n            using_input_ids = None\n            using_position_ids = None\n\n        # Encoder embedding.\n        if self.pre_process:\n            encoder_input: Optional[Tensor] = self.embedding_forward(\n                input_ids=using_input_ids,\n                position_ids=using_position_ids,\n                tokentype_ids=tokentype_ids,\n                attention_mask=attention_mask,\n            )\n        else:\n            # intermediate stage of pipeline\n            # encoder will get hidden_states from encoder.input_tensor\n            encoder_input = None\n\n        # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n        rotary_pos_emb = None\n        if self.position_embedding_type == \"rope\":\n            rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n                inference_params, self.encoder, encoder_input, self.config\n            )\n            rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n        # Run encoder.\n        hidden_states = self.encoder(\n            hidden_states=encoder_input,\n            attention_mask=extended_attention_mask,\n            inference_params=inference_params,\n            rotary_pos_emb=rotary_pos_emb,\n        )\n\n        if not self.post_process:\n            return hidden_states\n\n        if self.add_binary_head:\n            pooled_output = self.pooler(hidden_states, 0)\n\n        if self.return_embeddings or self.include_embeddings:\n            embeddings = torch.transpose(hidden_states, 0, 1)\n            masks = torch.sum(attention_mask, dim=1)\n            # Collect masked embeddings.\n            output_embeddings = torch.zeros(\n                size=(embeddings.shape[0], embeddings.shape[2]),\n                dtype=embeddings.dtype,\n                device=torch.cuda.current_device(),\n            )\n            for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n                output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n        if self.return_embeddings:\n            return output_embeddings\n\n        # logits and loss\n        output_weight = None\n        if self.share_embeddings_and_output_weights:\n            output_weight = self.shared_embedding_or_output_weight()\n\n        hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n        if not self.skip_logits:\n            logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n        else:\n            logits = None\n\n        binary_logits = None\n        if self.binary_head is not None:\n            binary_logits = self.binary_head(pooled_output)\n\n        output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n        if self.include_hiddens:\n            output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n        if self.include_embeddings:\n            output[\"embeddings\"] = output_embeddings\n        return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.bert_extended_attention_mask","title":"<code>bert_extended_attention_mask(attention_mask)</code>","text":"<p>Creates the extended attention mask</p> <p>Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary</p> <p>Parameters:</p> Name Type Description Default <code>attention_mask</code> <code>Tensor</code> <p>The input attention mask</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>The extended binary attention mask</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def bert_extended_attention_mask(self, attention_mask: Tensor) -&gt; Tensor:\n    \"\"\"Creates the extended attention mask\n\n    Converts the attention mask of dimension [batch size, 1, seq len] to [batch size, 1, seq len, seq len] and makes it binary\n\n    Args:\n        attention_mask (Tensor): The input attention mask\n\n    Returns:\n        Tensor: The extended binary attention mask\n    \"\"\"  # noqa: D415\n    # We create a 3D attention mask from a 2D tensor mask.\n    # [b, 1, s]\n    attention_mask_b1s = attention_mask.unsqueeze(1)\n\n    if self.use_full_attention_mask:\n        # [b, s, 1]\n        attention_mask_bs1 = attention_mask.unsqueeze(2)\n        # [b, s, s]\n        attention_mask_bss = attention_mask_b1s * attention_mask_bs1\n        # [b, 1, s, s]\n        extended_attention_mask = attention_mask_bss.unsqueeze(1)\n    else:\n        # Tensor Engine requires a 1x1xS attention mask which it internally\n        #  converts into a 1xSxS mask.\n        # [b, 1, 1, s]\n        extended_attention_mask = attention_mask_b1s.unsqueeze(1)\n\n    # Convert attention mask to binary, and flip the values from 0 to 1 and vice versa so that\n    #  extended_attention_mask._mask_fill(-1000) that megatron does internally result in\n    #  masking out pad positions.\n    extended_attention_mask = extended_attention_mask &lt; 0.5\n\n    return extended_attention_mask\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.embedding_forward","title":"<code>embedding_forward(input_ids, position_ids, tokentype_ids=None, attention_mask=None)</code>","text":"<p>Produce embeddings.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def embedding_forward(\n    self,\n    input_ids: Tensor,\n    position_ids: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    attention_mask: Optional[Tensor] = None,\n) -&gt; Tensor:\n    \"\"\"Produce embeddings.\"\"\"\n    return self.embedding(input_ids=input_ids, position_ids=position_ids, tokentype_ids=tokentype_ids)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.forward","title":"<code>forward(input_ids, attention_mask, tokentype_ids=None, lm_labels=None, inference_params=None)</code>","text":"<p>Forward function of BERT model</p> <p>Forward function of the BERT Model This function passes the input tensors through the embedding layer, and then the encoder and finally into the post processing layer (optional).</p> <p>It either returns the Loss values if labels are given or the final hidden units.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Tensor,\n    attention_mask: Tensor,\n    tokentype_ids: Optional[Tensor] = None,\n    lm_labels: Optional[Tensor] = None,\n    inference_params: Any | None = None,\n) -&gt; BioBertOutput | Tensor:\n    \"\"\"Forward function of BERT model\n\n    Forward function of the BERT Model This function passes the input tensors\n    through the embedding layer, and then the encoder and finally into the post\n    processing layer (optional).\n\n    It either returns the Loss values if labels are given or the final hidden units.\n    \"\"\"  # noqa: D415\n    # TODO! If we upgrade to TE 1.7 why does bit flipping back to 1 help the loss in TE 1.7? It claimed that they now follow standards, did\n    #  nemo/megatron flip again internally to be compatible wtih TE somewhere?\n    #  change the following line to ~self.bert... and see if it helps if we upgrade to TE 1.7 and NeMo/Megatron have not compensated.\n    extended_attention_mask = self.bert_extended_attention_mask(attention_mask)\n\n    if parallel_state.is_pipeline_first_stage():\n        using_input_ids: Optional[Tensor] = input_ids\n        using_position_ids: Optional[Tensor] = self.bert_position_ids(input_ids)\n    else:\n        using_input_ids = None\n        using_position_ids = None\n\n    # Encoder embedding.\n    if self.pre_process:\n        encoder_input: Optional[Tensor] = self.embedding_forward(\n            input_ids=using_input_ids,\n            position_ids=using_position_ids,\n            tokentype_ids=tokentype_ids,\n            attention_mask=attention_mask,\n        )\n    else:\n        # intermediate stage of pipeline\n        # encoder will get hidden_states from encoder.input_tensor\n        encoder_input = None\n\n    # Rotary positional embeddings (Why not move this into BERT/GPTEmberdding ?)\n    rotary_pos_emb = None\n    if self.position_embedding_type == \"rope\":\n        rotary_seq_len = self.rotary_pos_emb.get_rotary_seq_len(\n            inference_params, self.encoder, encoder_input, self.config\n        )\n        rotary_pos_emb = self.rotary_pos_emb(rotary_seq_len)\n\n    # Run encoder.\n    hidden_states = self.encoder(\n        hidden_states=encoder_input,\n        attention_mask=extended_attention_mask,\n        inference_params=inference_params,\n        rotary_pos_emb=rotary_pos_emb,\n    )\n\n    if not self.post_process:\n        return hidden_states\n\n    if self.add_binary_head:\n        pooled_output = self.pooler(hidden_states, 0)\n\n    if self.return_embeddings or self.include_embeddings:\n        embeddings = torch.transpose(hidden_states, 0, 1)\n        masks = torch.sum(attention_mask, dim=1)\n        # Collect masked embeddings.\n        output_embeddings = torch.zeros(\n            size=(embeddings.shape[0], embeddings.shape[2]),\n            dtype=embeddings.dtype,\n            device=torch.cuda.current_device(),\n        )\n        for i, (embedding, mask) in enumerate(zip(embeddings, masks)):\n            output_embeddings[i, :] = torch.mean(embedding[1 : mask - 1], dim=0)\n\n    if self.return_embeddings:\n        return output_embeddings\n\n    # logits and loss\n    output_weight = None\n    if self.share_embeddings_and_output_weights:\n        output_weight = self.shared_embedding_or_output_weight()\n\n    hidden_states_after_lm_head = self.lm_head(hidden_states=hidden_states)\n    if not self.skip_logits:\n        logits, _ = self.output_layer(hidden_states_after_lm_head, weight=output_weight)\n    else:\n        logits = None\n\n    binary_logits = None\n    if self.binary_head is not None:\n        binary_logits = self.binary_head(pooled_output)\n\n    output = {\"token_logits\": logits, \"binary_logits\": binary_logits}\n    if self.include_hiddens:\n        output[\"hidden_states\"] = hidden_states.transpose(0, 1).contiguous()  # [s b h] =&gt; [b s h]\n    if self.include_embeddings:\n        output[\"embeddings\"] = output_embeddings\n    return output\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/model/#bionemo.llm.model.biobert.model.MegatronBioBertModel.set_input_tensor","title":"<code>set_input_tensor(input_tensor)</code>","text":"<p>Sets input tensor to the model.</p> <p>See megatron.model.transformer.set_input_tensor()</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor | list[Tensor]</code> <p>Sets the input tensor for the model.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Iff the input tensor is a list that doesn't have exactly 1 tensor.</p> Source code in <code>bionemo/llm/model/biobert/model.py</code> <pre><code>def set_input_tensor(self, input_tensor: Tensor | list[Tensor]) -&gt; None:\n    \"\"\"Sets input tensor to the model.\n\n    See megatron.model.transformer.set_input_tensor()\n\n    Args:\n        input_tensor: Sets the input tensor for the model.\n\n    Raises:\n        ValueError: Iff the input tensor is a list that doesn't have exactly 1 tensor.\n    \"\"\"\n    # This is usually handled in schedules.py but some inference code still gives us non-lists or None.\n    if isinstance(input_tensor, list):\n        if len(input_tensor) != 1:\n            raise ValueError(f\"input_tensor should only be length 1 for gpt/bert, not length: {len(input_tensor)}\")\n        single_input_tensor: Tensor = input_tensor[0]\n    else:\n        single_input_tensor = input_tensor\n    self.encoder.set_input_tensor(single_input_tensor)\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/testing_utils/","title":"Testing utils","text":""},{"location":"API_reference/bionemo/llm/model/biobert/testing_utils/#bionemo.llm.model.biobert.testing_utils.compute_biobert_loss_singlegpu","title":"<code>compute_biobert_loss_singlegpu(trainer, pl_module)</code>","text":"<p>Computes the loss for BioBert models on a single GPU.</p> <p>This will not function in multi-gpu settings nor with models that do not conform to BioBert.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The Lightning Trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule being trained.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean loss.</p> <p>See Also: - :class: BioBertModel</p> Source code in <code>bionemo/llm/model/biobert/testing_utils.py</code> <pre><code>def compute_biobert_loss_singlegpu(trainer: pl.Trainer, pl_module: pl.LightningModule):\n    \"\"\"Computes the loss for BioBert models on a single GPU.\n\n    This will not function in multi-gpu settings nor with models that do not conform to BioBert.\n\n    Args:\n        trainer (pl.Trainer): The Lightning Trainer object.\n        pl_module (pl.LightningModule): The LightningModule being trained.\n\n    Returns:\n        float: The mean loss.\n\n    See Also:\n    - :class: BioBertModel\n    \"\"\"\n    model = pl_module\n    dl = trainer.datamodule.val_dataloader()\n\n    n, loss = -1, 0.0\n    model.eval()\n    # batch = next(iter(dl))\n    batch = model.data_step(iter(dl))\n    result = model(\n        input_ids=batch[\"text\"].cuda(),  # 'tokens' also a valid input for MockGPTDataModule\n        attention_mask=batch[\"attention_mask\"].cuda(),\n    )\n    loss_mask = batch[\"loss_mask\"].cuda()\n    # Not guaranteed i guess?\n    logits = result[\"token_logits\"]\n    target = batch[\"labels\"].cuda()\n    loss += F.cross_entropy(logits[loss_mask].float(), target[loss_mask], reduction=\"sum\")\n    n += loss_mask.sum()\n\n    mean_loss: float = (loss / n).detach().cpu().numpy().item()\n    model.train()\n    return mean_loss\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/","title":"Transformer specs","text":""},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.BiobertSpecOption","title":"<code>BiobertSpecOption</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model. This is a <code>str, Enum</code> type so that argparse can use the string names as choices.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>class BiobertSpecOption(str, Enum):\n    \"\"\"Options for the BiobertSpec. The spec defines the architecture of the transformer (BERT) block in the biobert model.\n    This is a `str, Enum` type so that argparse can use the string names as choices.\n    \"\"\"  # noqa: D205\n\n    bert_layer_local_spec = \"bert_layer_local_spec\"\n    bert_layer_local_spec_with_qk_ln = \"bert_layer_local_spec_with_qk_ln\"\n    bert_layer_with_transformer_engine_spec = \"bert_layer_with_transformer_engine_spec\"\n    bert_layer_with_transformer_engine_and_qk_ln_spec = \"bert_layer_with_transformer_engine_and_qk_ln_spec\"\n    # ESM2 spec\n    esm2_bert_layer_local_spec = \"esm2_bert_layer_local_spec\"\n    esm2_bert_layer_with_transformer_engine_spec = \"esm2_bert_layer_with_transformer_engine_spec\"\n</code></pre>"},{"location":"API_reference/bionemo/llm/model/biobert/transformer_specs/#bionemo.llm.model.biobert.transformer_specs.get_biobert_spec","title":"<code>get_biobert_spec(biobert_spec_option, qk_layernorm=False, core_attention=None)</code>","text":"<p>Get the spec for the Biobert model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>ModelType</code> <p>The model type.</p> required <code>spec_option</code> <code>BiobertSpecOption</code> <p>The spec option.</p> required <p>Returns:</p> Name Type Description <code>TransformerConfig</code> <code>ModuleSpec</code> <p>The Biobert spec.</p> Source code in <code>bionemo/llm/model/biobert/transformer_specs.py</code> <pre><code>def get_biobert_spec(  # noqa: D417\n    biobert_spec_option: BiobertSpecOption,\n    qk_layernorm: bool = False,\n    core_attention: Optional[Type[Module]] = None,\n) -&gt; spec_utils.ModuleSpec:\n    \"\"\"Get the spec for the Biobert model.\n\n    Args:\n        model_type (ModelType): The model type.\n        spec_option (BiobertSpecOption): The spec option.\n\n    Returns:\n        TransformerConfig: The Biobert spec.\n    \"\"\"\n    #\n    # BEGIN define several specs that are a function of `qk_layernorm`\n    #\n\n    match biobert_spec_option:\n        case BiobertSpecOption.bert_layer_local_spec:\n            return bert_layer_specs.bert_layer_local_spec\n\n        case BiobertSpecOption.bert_layer_local_spec_with_qk_ln:\n            # Use this spec for an implementation using only modules in megatron core\n\n            if core_attention is None:\n                core_attention = DotProductAttention\n\n            bert_layer_local_spec_with_qk_ln = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=FusedLayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return bert_layer_local_spec_with_qk_ln\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_spec:\n            return bert_layer_specs.bert_layer_with_transformer_engine_spec\n\n        case BiobertSpecOption.bert_layer_with_transformer_engine_and_qk_ln_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            bert_layer_with_transformer_engine_and_qk_ln_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                            k_layernorm=TELayerNorm if qk_layernorm else IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return bert_layer_with_transformer_engine_and_qk_ln_spec\n\n        case BiobertSpecOption.esm2_bert_layer_local_spec:\n            if core_attention is None:\n                raise ValueError(f\"Must supply core_attention with {BiobertSpecOption.esm2_bert_layer_local_spec} !\")\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    input_layernorm=FusedLayerNorm,\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=ColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=RowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    pre_mlp_layernorm=FusedLayerNorm,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=ColumnParallelLinear,\n                            linear_fc2=RowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                    sharded_state_dict_keys_map={\n                        \"input_layernorm.\": \"self_attention.linear_qkv.layer_norm_\",\n                        \"pre_mlp_layernorm.\": \"mlp.linear_fc1.layer_norm_\",\n                    },\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec:\n            if core_attention is None:\n                core_attention = TEDotProductAttention\n\n            esm2_bert_layer_local_spec = spec_utils.ModuleSpec(\n                module=TransformerLayer,\n                submodules=TransformerLayerSubmodules(\n                    self_attention=spec_utils.ModuleSpec(\n                        module=SelfAttention,\n                        params={\"attn_mask_type\": AttnMaskType.padding},\n                        submodules=SelfAttentionSubmodules(\n                            linear_qkv=TELayerNormColumnParallelLinear,\n                            core_attention=core_attention,\n                            linear_proj=TERowParallelLinear,\n                            q_layernorm=ESM2QueryScaling,\n                            k_layernorm=IdentityOp,\n                        ),\n                    ),\n                    self_attn_bda=get_bias_dropout_add,\n                    mlp=spec_utils.ModuleSpec(\n                        module=MLP,\n                        submodules=MLPSubmodules(\n                            linear_fc1=TELayerNormColumnParallelLinear,\n                            linear_fc2=TERowParallelLinear,\n                        ),\n                    ),\n                    mlp_bda=get_bias_dropout_add,\n                ),\n            )\n            return esm2_bert_layer_local_spec\n\n        case _:\n            raise NotImplementedError(f\"Spec option {biobert_spec_option} not implemented\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/","title":"Config models","text":""},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig","title":"<code>DataConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[DataModuleT]</code>, <code>ABC</code></p> <p>Base class for all data configurations.</p> <p>This class is used to define the interface for all data configurations. It is used to define the data module that will be used in the training loop.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class DataConfig(BaseModel, Generic[DataModuleT], ABC):\n    \"\"\"Base class for all data configurations.\n\n    This class is used to define the interface for all data configurations. It is used to define the data module that\n    will be used in the training loop.\n    \"\"\"\n\n    micro_batch_size: int = 8\n    result_dir: str | pathlib.Path = \"./results\"\n    num_dataset_workers: int = 0\n    seq_length: int = 128\n\n    @abstractmethod\n    def construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n        \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n        ...\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.data_config == self\n        \"\"\"\n        return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.construct_data_module","title":"<code>construct_data_module(global_batch_size)</code>  <code>abstractmethod</code>","text":"<p>Construct the data module from the configuration. Cannot be defined generically.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@abstractmethod\ndef construct_data_module(self, global_batch_size: int) -&gt; DataModuleT:\n    \"\"\"Construct the data module from the configuration. Cannot be defined generically.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.DataConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.data_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.data_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExperimentConfig","title":"<code>ExperimentConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for setting up and managing experiment parameters.</p> <p>Attributes:</p> Name Type Description <code>save_every_n_steps</code> <code>int</code> <p>Number of steps between saving checkpoints.</p> <code>result_dir</code> <code>str | Path</code> <p>Directory where results will be saved.</p> <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> <code>restore_from_checkpoint_path</code> <code>Optional[str]</code> <p>Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.</p> <code>save_last_checkpoint</code> <code>bool</code> <p>Flag to save the last checkpoint. Default is True.</p> <code>metric_to_monitor_for_checkpoints</code> <code>str</code> <p>Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".</p> <code>save_top_k</code> <code>int</code> <p>Number of top checkpoints to save based on the monitored metric. Default is 2.</p> <code>create_tensorboard_logger</code> <code>bool</code> <p>Flag to create a TensorBoard logger. Default is False.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExperimentConfig(BaseModel):\n    \"\"\"Configuration class for setting up and managing experiment parameters.\n\n    Attributes:\n        save_every_n_steps (int): Number of steps between saving checkpoints.\n        result_dir (str | pathlib.Path): Directory where results will be saved.\n        experiment_name (str): Name of the experiment.\n        restore_from_checkpoint_path (Optional[str]): Path to restore from a checkpoint. Note: This does not invoke the checkpoint callback as expected.\n        save_last_checkpoint (bool): Flag to save the last checkpoint. Default is True.\n        metric_to_monitor_for_checkpoints (str): Metric to monitor for saving top-k checkpoints. Default is \"reduced_train_loss\".\n        save_top_k (int): Number of top checkpoints to save based on the monitored metric. Default is 2.\n        create_tensorboard_logger (bool): Flag to create a TensorBoard logger. Default is False.\n    \"\"\"\n\n    save_every_n_steps: int\n    result_dir: str | pathlib.Path\n    experiment_name: str\n    # NOTE: restore_from_checkpoint_path does not invoke the checkpoint callback in the way we'd like. Avoid using.\n    restore_from_checkpoint_path: Optional[str]\n    save_last_checkpoint: bool = True\n    metric_to_monitor_for_checkpoints: str = \"reduced_train_loss\"\n    save_top_k: int = 2\n    create_tensorboard_logger: bool = False\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig","title":"<code>ExposedModelConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ModelConfigT]</code>, <code>ABC</code></p> <p>BioNeMo model configuration class, wraps TransformerConfig and friends.</p> <p>This class is used to define the interface for all model configurations. It is Exposed to guard against ill-typed or poorly defined fields in the underlying configuration objects. <code>ModelConfigT</code> declares the associated type of the underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar). Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping the more esoteric configuration private to the underlying ModelConfigT.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ExposedModelConfig(BaseModel, Generic[ModelConfigT], ABC):\n    \"\"\"BioNeMo model configuration class, wraps TransformerConfig and friends.\n\n    This class is used to define the interface for all model configurations. It is **Exposed** to guard against ill-typed\n    or poorly defined fields in the underlying configuration objects. `ModelConfigT` declares the associated type of the\n    underlying config (most commonly a BioBertGenericConfig, but could also be a TransformerConfig or something similar).\n    Children should try to expose the minimal set of fields necessary for the user to configure the model while keeping\n    the more esoteric configuration private to the underlying ModelConfigT.\n    \"\"\"\n\n    # Restores weights from a pretrained checkpoint\n    initial_ckpt_path: Optional[str] = None\n    # Does not attempt to load keys with these prefixes (useful if you attached extra parameters and still want to load a set of weights)\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n\n    # Pydantic stuff to allow arbitrary types + validators + serializers\n    class Config:  # noqa: D106\n        arbitrary_types_allowed = True\n\n    def model_class(self) -&gt; Type[ModelConfigT]:\n        \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n        raise NotImplementedError\n\n    def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n        \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n        The following expression will always be true:\n\n        global_cfg.bionemo_model_config == self\n        \"\"\"\n        return global_cfg\n\n    def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n        \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n        The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n        hide fields that are either not serializable by Pydantic or that we do not want to expose.\n        \"\"\"\n        cls: Type[ModelConfigT] = self.model_class()\n        model_dict = {}\n        for attr in self.model_fields:\n            if attr not in model_dict and attr in cls.__dataclass_fields__:\n                model_dict[attr] = getattr(self, attr)\n\n        # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n        #   the only constraint is that both must not be true.\n        model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n        model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n        result = cls(**model_dict)\n\n        return result\n\n    # NOTE: See PrecisionTypes for a list of valid literals that may be deserialized.\n    params_dtype: torch.dtype\n    pipeline_dtype: torch.dtype\n    autocast_dtype: torch.dtype\n\n    num_layers: int = 6\n    hidden_size: int = 256\n    ffn_hidden_size: int = 512\n    num_attention_heads: int = 4\n    seq_length: int = 512\n    fp32_residual_connection: bool = False\n    hidden_dropout: float = 0.02\n    init_method_std: float = 0.02\n    kv_channels: Optional[int] = None\n    apply_query_key_layer_scaling: bool = False\n    make_vocab_size_divisible_by: int = 128\n    masked_softmax_fusion: bool = True\n    fp16_lm_cross_entropy: bool = False\n    gradient_accumulation_fusion: bool = False\n    layernorm_zero_centered_gamma: bool = False\n    layernorm_epsilon: float = 1.0e-12\n    activation_func: Callable[[torch.Tensor, Any], torch.Tensor] = F.gelu\n    qk_layernorm: bool = False\n    apply_residual_connection_post_layernorm: bool = False\n    bias_activation_fusion: bool = True\n    bias_dropout_fusion: bool = True\n    get_attention_mask_from_fusion: bool = False\n    attention_dropout: float = 0.1\n    share_embeddings_and_output_weights: bool = True\n    enable_autocast: bool = False\n    nemo1_ckpt_path: Optional[str] = None\n    biobert_spec_option: BiobertSpecOption = BiobertSpecOption.bert_layer_with_transformer_engine_spec\n\n    @field_validator(\"activation_func\", mode=\"before\")\n    @classmethod\n    def validate_activation_func(cls, activation_func: str) -&gt; Callable:\n        \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n        For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n        validates the provided activation function string and returns a callable function based on the validation\n        context using the provided validator in the base class.\n\n        Args:\n            activation_func (str): The activation function to be validated.\n            context (ValidationInfo): The context for validation.\n\n        Returns:\n            Callable: A callable function after validation.\n\n        See Also:\n            CUSTOM_ACTIVATION_FNS\n        \"\"\"\n        func = getattr(torch.nn.functional, activation_func.lower(), None)\n        if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n            func = CUSTOM_ACTIVATION_FNS[activation_func]\n            return func\n        elif func is None:\n            raise ValueError(\n                f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n            )\n        else:\n            return func\n\n    @field_serializer(\"activation_func\")\n    def serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n        \"\"\"Serializes a given activation function to its corresponding string representation.\n\n        By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n        activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n        top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n        Args:\n            v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n        Returns:\n            str: The name of the activation function if it is a standard PyTorch function,\n                 or the corresponding serialization key if it is a custom activation function.\n\n        Raises:\n            ValueError: If the activation function is not supported.\n        \"\"\"\n        func_name = v.__name__\n        func = getattr(torch.nn.functional, func_name, None)\n        if func is not None:\n            return func_name\n        elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n            return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n        else:\n            raise ValueError(f\"Unsupported activation function: {v}\")\n\n    @field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n    @classmethod\n    def precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n        \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n        return dtypes.get_autocast_dtype(v)\n\n    @field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\n    def serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n        \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n        return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.custom_model_validator","title":"<code>custom_model_validator(global_cfg)</code>","text":"<p>Use custom implementation of this method to define the things inside global_config.</p> <p>The following expression will always be true:</p> <p>global_cfg.bionemo_model_config == self</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def custom_model_validator(self, global_cfg: \"MainConfig\") -&gt; \"MainConfig\":\n    \"\"\"Use custom implementation of this method to define the things inside global_config.\n\n    The following expression will always be true:\n\n    global_cfg.bionemo_model_config == self\n    \"\"\"\n    return global_cfg\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.exposed_to_internal_bionemo_model_config","title":"<code>exposed_to_internal_bionemo_model_config()</code>","text":"<p>Converts the exposed dataclass to the underlying Transformer config.</p> <p>The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to hide fields that are either not serializable by Pydantic or that we do not want to expose.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def exposed_to_internal_bionemo_model_config(self) -&gt; ModelConfigT:\n    \"\"\"Converts the exposed dataclass to the underlying Transformer config.\n\n    The underlying ModelConfigT may both be incomplete and unserializable. We use this transformation as a way to\n    hide fields that are either not serializable by Pydantic or that we do not want to expose.\n    \"\"\"\n    cls: Type[ModelConfigT] = self.model_class()\n    model_dict = {}\n    for attr in self.model_fields:\n        if attr not in model_dict and attr in cls.__dataclass_fields__:\n            model_dict[attr] = getattr(self, attr)\n\n    # Now set fp16 and bf16 based on the precision for the underlying TransformerConfig=&gt;ParallelConfig\n    #   the only constraint is that both must not be true.\n    model_dict[\"bf16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"bf16-mixed\"]\n    model_dict[\"fp16\"] = self.pipeline_dtype == dtypes.precision_to_dtype[\"16-mixed\"]\n    result = cls(**model_dict)\n\n    return result\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.model_class","title":"<code>model_class()</code>","text":"<p>Returns the underlying model class that this config wraps.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>def model_class(self) -&gt; Type[ModelConfigT]:\n    \"\"\"Returns the underlying model class that this config wraps.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.precision_validator","title":"<code>precision_validator(v)</code>  <code>classmethod</code>","text":"<p>Validates the precision type and returns the corresponding torch dtype.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\", mode=\"before\")\n@classmethod\ndef precision_validator(cls, v: dtypes.PrecisionTypes) -&gt; torch.dtype:\n    \"\"\"Validates the precision type and returns the corresponding torch dtype.\"\"\"\n    return dtypes.get_autocast_dtype(v)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_activation_func","title":"<code>serialize_activation_func(v)</code>","text":"<p>Serializes a given activation function to its corresponding string representation.</p> <p>By default, all activation functions from <code>torch.nn.functional</code> are serialized to their name. User defined activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the top of this file. This allows our Pydantic model to serialize and deserialize the activation function.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Callable[[Tensor, Any], Tensor]</code> <p>The activation function to serialize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the activation function if it is a standard PyTorch function,  or the corresponding serialization key if it is a custom activation function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the activation function is not supported.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"activation_func\")\ndef serialize_activation_func(self, v: Callable[[torch.Tensor, Any], torch.Tensor]) -&gt; str:\n    \"\"\"Serializes a given activation function to its corresponding string representation.\n\n    By default, all activation functions from `torch.nn.functional` are serialized to their name. User defined\n    activation functions should also be defined here with a custom mapping in CUSTOM_ACTIVATION_FNS defined at the\n    top of this file. This allows our Pydantic model to serialize and deserialize the activation function.\n\n    Args:\n        v (Callable[[torch.Tensor, Any], torch.Tensor]): The activation function to serialize.\n\n    Returns:\n        str: The name of the activation function if it is a standard PyTorch function,\n             or the corresponding serialization key if it is a custom activation function.\n\n    Raises:\n        ValueError: If the activation function is not supported.\n    \"\"\"\n    func_name = v.__name__\n    func = getattr(torch.nn.functional, func_name, None)\n    if func is not None:\n        return func_name\n    elif func in REVERSE_CUSTOM_ACTIVATION_FNS:\n        return REVERSE_CUSTOM_ACTIVATION_FNS[func]  # Get the serialization key\n    else:\n        raise ValueError(f\"Unsupported activation function: {v}\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.serialize_dtypes","title":"<code>serialize_dtypes(v)</code>","text":"<p>Serializes the torch dtype to the corresponding precision type.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_serializer(\"params_dtype\", \"pipeline_dtype\", \"autocast_dtype\")\ndef serialize_dtypes(self, v: torch.dtype) -&gt; dtypes.PrecisionTypes:\n    \"\"\"Serializes the torch dtype to the corresponding precision type.\"\"\"\n    return dtypes.dtype_to_precision[v]\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ExposedModelConfig.validate_activation_func","title":"<code>validate_activation_func(activation_func)</code>  <code>classmethod</code>","text":"<p>Validates the activation function, assumes this function exists in torch.nn.functional.</p> <p>For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method validates the provided activation function string and returns a callable function based on the validation context using the provided validator in the base class.</p> <p>Parameters:</p> Name Type Description Default <code>activation_func</code> <code>str</code> <p>The activation function to be validated.</p> required <code>context</code> <code>ValidationInfo</code> <p>The context for validation.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A callable function after validation.</p> See Also <p>CUSTOM_ACTIVATION_FNS</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@field_validator(\"activation_func\", mode=\"before\")\n@classmethod\ndef validate_activation_func(cls, activation_func: str) -&gt; Callable:\n    \"\"\"Validates the activation function, assumes this function exists in torch.nn.functional.\n\n    For custom activation functions, use the CUSTOM_ACTIVATION_FUNCTIONS dictionary in the module. This method\n    validates the provided activation function string and returns a callable function based on the validation\n    context using the provided validator in the base class.\n\n    Args:\n        activation_func (str): The activation function to be validated.\n        context (ValidationInfo): The context for validation.\n\n    Returns:\n        Callable: A callable function after validation.\n\n    See Also:\n        CUSTOM_ACTIVATION_FNS\n    \"\"\"\n    func = getattr(torch.nn.functional, activation_func.lower(), None)\n    if func is None and activation_func in CUSTOM_ACTIVATION_FNS:\n        func = CUSTOM_ACTIVATION_FNS[activation_func]\n        return func\n    elif func is None:\n        raise ValueError(\n            f\"activation_func must be a valid function in `torch.nn.functional`, got {activation_func=}\"\n        )\n    else:\n        return func\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig","title":"<code>MainConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[ExModelConfigT, DataConfigT]</code></p> <p>Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.</p> <p>This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users must define in their own environment for execution.</p> <p>Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this class while still allowing for custom validation global logic to be implemented in the underlying classes. For example, some models may want to restrict their Datamodules seq_length to a certain value.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <p>Generic config type that contains instructions on instantiating the required DataModule.</p> required <code>parallel_config</code> <p>The parallel configuration for the model.</p> required <code>training_config</code> <p>The training configuration for the model.</p> required <code>bionemo_model_config</code> <p>Generic ExposedModelConfig type. This class hides extra configuration parameters in the underlying model configuration as well as providing</p> required <code>optim_config</code> <p>The optimizer/scheduler configuration for the model.</p> required <code>experiment_config</code> <p>The experiment configuration for the model.</p> required <code>wandb_config</code> <p>Optional, the wandb configuration for the model.</p> required Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class MainConfig(BaseModel, Generic[ExModelConfigT, DataConfigT]):\n    \"\"\"Main configuration class for BioNeMo. All serialized configs that are a valid MainConfig should be Runnable.\n\n    This class is used to define the main configuration for BioNeMo. It defines the minimal pieces of configuration\n    to execution a training job with the NeMo2 training api. It accepts two generic type parameters which users\n    must define in their own environment for execution.\n\n    Additionally, this class assumes that the configs for ExposedModelConfig and DataConfig may have custom validators\n    implemented that operate on the entire MainConfig. This prevents the need from type based conditionals inside this\n    class while still allowing for custom validation global logic to be implemented in the underlying classes. For example,\n    some models may want to restrict their Datamodules seq_length to a certain value.\n\n\n    Args:\n        data_config: Generic config type that contains instructions on instantiating the required DataModule.\n        parallel_config: The parallel configuration for the model.\n        training_config: The training configuration for the model.\n        bionemo_model_config: Generic ExposedModelConfig type. This class hides extra configuration parameters in the\n            underlying model configuration as well as providing\n        optim_config: The optimizer/scheduler configuration for the model.\n        experiment_config: The experiment configuration for the model.\n        wandb_config: Optional, the wandb configuration for the model.\n    \"\"\"\n\n    data_config: DataConfigT\n    parallel_config: ParallelConfig\n    training_config: TrainingConfig\n    bionemo_model_config: ExModelConfigT\n    optim_config: OptimizerSchedulerConfig\n    experiment_config: ExperimentConfig\n    wandb_config: Optional[WandbConfig] = None\n\n    @model_validator(mode=\"after\")\n    def validate_master_config(self) -&gt; \"MainConfig\":\n        \"\"\"Validates the master configuration object.\"\"\"\n        self.bionemo_model_config.seq_length = self.data_config.seq_length\n        return self\n\n    @model_validator(mode=\"after\")\n    def run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n        return self.bionemo_model_config.custom_model_validator(self)\n\n    @model_validator(mode=\"after\")\n    def run_data_config_model_validators(self) -&gt; \"MainConfig\":\n        \"\"\"Runs the model validators on the data_config.\"\"\"\n        return self.data_config.custom_model_validator(self)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_bionemo_model_config_model_validators","title":"<code>run_bionemo_model_config_model_validators()</code>","text":"<p>Runs the model validators on the bionemo_model_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_bionemo_model_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the bionemo_model_config.\"\"\"\n    return self.bionemo_model_config.custom_model_validator(self)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.run_data_config_model_validators","title":"<code>run_data_config_model_validators()</code>","text":"<p>Runs the model validators on the data_config.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef run_data_config_model_validators(self) -&gt; \"MainConfig\":\n    \"\"\"Runs the model validators on the data_config.\"\"\"\n    return self.data_config.custom_model_validator(self)\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.MainConfig.validate_master_config","title":"<code>validate_master_config()</code>","text":"<p>Validates the master configuration object.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_master_config(self) -&gt; \"MainConfig\":\n    \"\"\"Validates the master configuration object.\"\"\"\n    self.bionemo_model_config.seq_length = self.data_config.seq_length\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.OptimizerSchedulerConfig","title":"<code>OptimizerSchedulerConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the optimizer and learning rate scheduler.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>Learning rate for the optimizer. Default is 1e-4.</p> <code>optimizer</code> <code>str</code> <p>Type of optimizer to use. Default is \"adam\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>interval</code> <code>str</code> <p>Interval for updating the learning rate scheduler. Default is \"step\".</p> <code>monitor</code> <code>str</code> <p>Metric to monitor for learning rate adjustments. Default is \"val_loss\".</p> <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.</p> <code>lr_scheduler</code> <code>Literal['warmup_anneal', 'cosine']</code> <p>Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class OptimizerSchedulerConfig(BaseModel):\n    \"\"\"Configuration for the optimizer and learning rate scheduler.\n\n    Attributes:\n        lr (float): Learning rate for the optimizer. Default is 1e-4.\n        optimizer (str): Type of optimizer to use. Default is \"adam\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        interval (str): Interval for updating the learning rate scheduler. Default is \"step\".\n        monitor (str): Metric to monitor for learning rate adjustments. Default is \"val_loss\".\n        warmup_steps (int): Number of warmup steps for use with the warmup annealing learning rate scheduler. Default is 0.\n        lr_scheduler (Literal['warmup_anneal', 'cosine']): Type of learning rate scheduler to use. Default is 'warmup_anneal'. NOTE this is likely to change.\n    \"\"\"\n\n    lr: float = 1e-4\n    optimizer: str = \"adam\"\n    interval: str = \"step\"\n    monitor: str = \"val_loss\"\n    cosine_rampup_frac: float = 0.01\n    cosine_hold_frac: float = 0.05\n    warmup_steps: int = 0\n    lr_scheduler: Literal[\"warmup_anneal\", \"cosine\"] = \"warmup_anneal\"\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig","title":"<code>ParallelConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>ParallelConfig is a configuration class for setting up parallelism in model training.</p> <p>Attributes:</p> Name Type Description <code>tensor_model_parallel_size</code> <code>int</code> <p>The size of the tensor model parallelism. Default is 1.</p> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The size of the pipeline model parallelism. Default is 1.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>The number of batches to accumulate gradients over. Default is 1.</p> <code>ddp</code> <code>Literal['megatron']</code> <p>The distributed data parallel method to use. Default is \"megatron\".</p> <code>remove_unused_parameters</code> <code>bool</code> <p>Whether to remove unused parameters. Default is True.</p> <code>num_devices</code> <code>int</code> <p>The number of devices to use. Default is 1.</p> <code>num_nodes</code> <code>int</code> <p>The number of nodes to use. Default is 1.</p> <p>Methods:</p> Name Description <code>validate_devices</code> <p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class ParallelConfig(BaseModel):\n    \"\"\"ParallelConfig is a configuration class for setting up parallelism in model training.\n\n    Attributes:\n        tensor_model_parallel_size (int): The size of the tensor model parallelism. Default is 1.\n        pipeline_model_parallel_size (int): The size of the pipeline model parallelism. Default is 1.\n        accumulate_grad_batches (int): The number of batches to accumulate gradients over. Default is 1.\n        ddp (Literal[\"megatron\"]): The distributed data parallel method to use. Default is \"megatron\".\n        remove_unused_parameters (bool): Whether to remove unused parameters. Default is True.\n        num_devices (int): The number of devices to use. Default is 1.\n        num_nodes (int): The number of nodes to use. Default is 1.\n\n    Methods:\n        validate_devices(): Validates the number of devices based on the tensor and pipeline model parallel sizes.\n    \"\"\"\n\n    tensor_model_parallel_size: int = 1\n    pipeline_model_parallel_size: int = 1\n    accumulate_grad_batches: int = 1\n    ddp: Literal[\"megatron\"] = \"megatron\"\n    remove_unused_parameters: bool = True\n    num_devices: int = 1\n    num_nodes: int = 1\n\n    @model_validator(mode=\"after\")\n    def validate_devices(self):\n        \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n        if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n            raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.ParallelConfig.validate_devices","title":"<code>validate_devices()</code>","text":"<p>Validates the number of devices based on the tensor and pipeline model parallel sizes.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_devices(self):\n    \"\"\"Validates the number of devices based on the tensor and pipeline model parallel sizes.\"\"\"\n    if self.num_devices &lt; self.tensor_model_parallel_size * self.pipeline_model_parallel_size:\n        raise ValueError(\"devices must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size\")\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/llm/run/config_models/#bionemo.llm.run.config_models.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>TrainingConfig is a configuration class for training models.</p> <p>Attributes:</p> Name Type Description <code>max_steps</code> <code>int</code> <p>The maximum number of training steps.</p> <code>limit_val_batches</code> <code>int | float</code> <p>The number of validation batches to use. Can be a fraction or a count.</p> <code>val_check_interval</code> <code>int</code> <p>The interval (in steps) at which to check validation.</p> <code>precision</code> <code>Literal['32', 'bf16-mixed', '16-mixed']</code> <p>The precision to use for training. Defaults to \"bf16-mixed\".</p> <code>accelerator</code> <code>str</code> <p>The type of accelerator to use for training. Defaults to \"gpu\".</p> <code>gc_interval</code> <code>int</code> <p>The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.</p> <code>include_perplexity</code> <code>bool</code> <p>Whether to include perplexity in the validation logs. Defaults to False.</p> Source code in <code>bionemo/llm/run/config_models.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"TrainingConfig is a configuration class for training models.\n\n    Attributes:\n        max_steps (int): The maximum number of training steps.\n        limit_val_batches (int | float): The number of validation batches to use. Can be a fraction or a count.\n        val_check_interval (int): The interval (in steps) at which to check validation.\n        precision (Literal[\"32\", \"bf16-mixed\", \"16-mixed\"], optional): The precision to use for training. Defaults to \"bf16-mixed\".\n        accelerator (str, optional): The type of accelerator to use for training. Defaults to \"gpu\".\n        gc_interval (int, optional): The interval of global steps at which to run synchronized garbage collection. Useful for synchronizing garbage collection when performing distributed training. Defaults to 0.\n        include_perplexity (bool, optional): Whether to include perplexity in the validation logs. Defaults to False.\n    \"\"\"\n\n    max_steps: int\n    limit_val_batches: int | float  # Because this can be a fraction or a count...\n    val_check_interval: int\n    precision: Literal[\"32\", \"bf16-mixed\", \"16-mixed\"] = \"bf16-mixed\"\n    accelerator: str = \"gpu\"\n    # NOTE: VERY important for distributed training performance.\n    gc_interval: int = 0\n    include_perplexity: bool = False\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/","title":"Datamodule utils","text":""},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.float_or_int_or_none","title":"<code>float_or_int_or_none(value)</code>","text":"<p>Converts a given value into a float, int, or None.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, float, int, None]</code> <p>A value that can be either a string, float, int, or None.</p> required <p>Returns:</p> Type Description <code>Union[float, int, None]</code> <p>Union[float, int, None]: A float, int, or None based on the input value.</p> <p>If the input value is None or \"None\", it returns None. If the input value is an int or float, it returns the same value. If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def float_or_int_or_none(value: Union[str, float, int, None]) -&gt; Union[float, int, None]:\n    \"\"\"Converts a given value into a float, int, or None.\n\n    Args:\n        value (Union[str, float, int, None]): A value that can be either a string, float, int, or None.\n\n    Returns:\n        Union[float, int, None]: A float, int, or None based on the input value.\n\n    If the input value is None or \"None\", it returns None.\n    If the input value is an int or float, it returns the same value.\n    If the input value is a string, it tries to convert it into an int if possible, otherwise into a float.\n    \"\"\"\n    if value is None or value == \"None\":\n        return\n    if isinstance(value, (int, float)):\n        return value\n    if value.isdigit():\n        return int(value)\n    return float(value)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_global_batch_size","title":"<code>infer_global_batch_size(micro_batch_size, num_nodes, devices, accumulate_grad_batches=1, tensor_model_parallel_size=1, pipeline_model_parallel_size=1)</code>","text":"<p>Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.</p> <p>Parameters:</p> Name Type Description Default <code>micro_batch_size</code> <code>int</code> <p>The micro batch size.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes.</p> required <code>devices</code> <code>int</code> <p>The number of devices.</p> required <code>accumulate_grad_batches</code> <code>int</code> <p>The accumulation of gradient batches. Defaults to 1.</p> <code>1</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>The tensor model parallel size. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>The pipeline model parallel size. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The global batch size.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_global_batch_size(\n    micro_batch_size: int,\n    num_nodes: int,\n    devices: int,\n    accumulate_grad_batches: int = 1,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n) -&gt; int:\n    \"\"\"Infers the global batch size based on the micro batch size, number of nodes, devices, accumulation of gradient batches, and model parallel sizes.\n\n    Args:\n        micro_batch_size (int): The micro batch size.\n        num_nodes (int): The number of nodes.\n        devices (int): The number of devices.\n        accumulate_grad_batches (int): The accumulation of gradient batches. Defaults to 1.\n        tensor_model_parallel_size (int): The tensor model parallel size. Defaults to 1.\n        pipeline_model_parallel_size (int): The pipeline model parallel size. Defaults to 1.\n\n    Returns:\n        int: The global batch size.\n    \"\"\"\n    if not all(\n        isinstance(arg, int)\n        for arg in [\n            micro_batch_size,\n            num_nodes,\n            devices,\n            accumulate_grad_batches,\n            tensor_model_parallel_size,\n            pipeline_model_parallel_size,\n        ]\n    ):\n        raise ValueError(\n            f\"All arguments must be of type int, got {type(micro_batch_size)}, {type(num_nodes)}, {type(devices)}, \"\n            f\"{type(accumulate_grad_batches)}, {type(tensor_model_parallel_size)}, and {type(pipeline_model_parallel_size)}\"\n        )\n    if micro_batch_size &lt;= 0:\n        raise ValueError(f\"micro_batch_size must be greater than 0, got {micro_batch_size}\")\n    if num_nodes &lt;= 0:\n        raise ValueError(f\"num_nodes must be greater than 0, got {num_nodes}\")\n    if devices &lt;= 0:\n        raise ValueError(f\"devices must be greater than 0, got {devices}\")\n    if accumulate_grad_batches &lt;= 0:\n        raise ValueError(f\"accumulate_grad_batches must be greater than 0, got {accumulate_grad_batches}\")\n    if tensor_model_parallel_size &lt;= 0:\n        raise ValueError(f\"tensor_model_parallel_size must be greater than 0, got {tensor_model_parallel_size}\")\n    if pipeline_model_parallel_size &lt;= 0:\n        raise ValueError(f\"pipeline_model_parallel_size must be greater than 0, got {pipeline_model_parallel_size}\")\n\n    world_size = num_nodes * devices\n    if world_size % (tensor_model_parallel_size * pipeline_model_parallel_size) != 0:\n        raise ValueError(\n            f\"world_size must be divisible by tensor_model_parallel_size * pipeline_model_parallel_size, \"\n            f\"got {world_size} and {tensor_model_parallel_size} * {pipeline_model_parallel_size}\"\n        )\n\n    model_parallel_size = tensor_model_parallel_size * pipeline_model_parallel_size\n    data_parallel_size = world_size // model_parallel_size\n    global_batch_size = micro_batch_size * data_parallel_size * accumulate_grad_batches\n    return global_batch_size\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.infer_num_samples","title":"<code>infer_num_samples(limit_batches, num_samples_in_dataset, global_batch_size, stage)</code>","text":"<p>Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.</p> <p>Parameters:</p> Name Type Description Default <code>limit_batches</code> <code>Union[float, int, str, None]</code> <p>The limit on the number of batches. Can be a float between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.</p> required <code>num_samples_in_dataset</code> <code>int</code> <p>The number of samples in the dataset.</p> required <code>global_batch_size</code> <code>int</code> <p>The global batch size.</p> required <code>stage</code> <code>str</code> <p>The stage of the training.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of samples from the limit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the limited number of samples is less than the global batch size, or if the limit_batches parameter is invalid.</p> <p>If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred as the product of limit_batches and global batch size. If limit_batches is None, it defaultsto 1.0, indicating that all dataset samples should be used.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def infer_num_samples(\n    limit_batches: Union[float, int, str, None], num_samples_in_dataset: int, global_batch_size: int, stage: str\n):\n    \"\"\"Infers the number of samples based on the limit_batches parameter, the length of the dataset, and the global batch size.\n\n    Args:\n        limit_batches (Union[float, int, str, None]): The limit on the number of batches. Can be a float\n            between 0 and 1, an integer, a string, or None. If None, defaults to 1.0.\n        num_samples_in_dataset (int): The number of samples in the dataset.\n        global_batch_size (int): The global batch size.\n        stage (str): The stage of the training.\n\n    Returns:\n        int: The number of samples from the limit.\n\n    Raises:\n        ValueError: If the limited number of samples is less than the global batch size, or if the\n            limit_batches parameter is invalid.\n\n    If limit_batches is a float between 0 and 1, the number of samples is inferred as a fraction of the number of samples\n    in the dataset. If limit_batches is an integer greater than or equal to 1, the number of limited samples is inferred\n    as the product of limit_batches and global batch size. If limit_batches is None, it defaultsto 1.0, indicating that\n    all dataset samples should be used.\n    \"\"\"\n    limit_batches = 1.0 if limit_batches is None else limit_batches  # validation data does not require upsampling\n    if 0 &lt; limit_batches &lt;= 1.0 and isinstance(limit_batches, float):\n        num_limited_samples = int(num_samples_in_dataset * limit_batches)\n        if num_limited_samples &lt; global_batch_size:\n            raise ValueError(\n                \"The limited number of %s samples %s is less than the global batch size %s\"\n                % (stage, num_limited_samples, global_batch_size)\n            )\n    elif limit_batches &gt;= 1 and isinstance(limit_batches, int):\n        num_limited_samples = int(limit_batches * global_batch_size)\n    else:\n        raise ValueError(\"Invalid choice of limit_%s_batches size: %s\" % (stage, limit_batches))\n\n    return num_limited_samples\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/datamodule_utils/#bionemo.llm.utils.datamodule_utils.parse_kwargs_to_arglist","title":"<code>parse_kwargs_to_arglist(kwargs)</code>","text":"<p>Converts a dictionary of keyword arguments into a list of command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>A dictionary where keys are argument names and values are argument values.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings, where each string is a command-line argument in the format '--argument-name value'.</p> Source code in <code>bionemo/llm/utils/datamodule_utils.py</code> <pre><code>def parse_kwargs_to_arglist(kwargs: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"Converts a dictionary of keyword arguments into a list of command-line arguments.\n\n    Args:\n        kwargs (Dict[str, Any]): A dictionary where keys are argument names and values are argument values.\n\n    Returns:\n        A list of strings, where each string is a command-line argument in the format '--argument-name value'.\n    \"\"\"\n    arglist = []\n    for k, v in kwargs.items():\n        arglist.extend([f\"--{k.replace('_', '-')}\", str(v)])\n    return arglist\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/","title":"Iomixin utils","text":""},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters","title":"<code>IOMixinWithGettersSetters</code>","text":"<p>               Bases: <code>WillHaveGetSetHparam</code>, <code>IOMixin</code></p> <p>An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.io added to your classes.</p> <p>This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class IOMixinWithGettersSetters(WillHaveGetSetHparam, io.IOMixin):\n    \"\"\"An implementation of WillHaveGetSetHparam which makes use of the io.IOMixin.__io__ added to your classes.\n\n    This enables you to mutate the hyper-parameters of your classes which will later be saved in configs.\n    \"\"\"\n\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        # Change the attribute of self and also change the io tracker so it gets updated in the config\n        if also_change_value:\n            setattr(self, attribute, value)\n        setattr(self.__io__, attribute, value)\n\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        if attribute not in dir(self.__io__):\n            raise KeyError(\n                f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n            )\n        return getattr(self.__io__, attribute)\n\n    def get_non_default_hparams(self) -&gt; List[str]:\n        \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n        Returns:\n            List[str]: A list of hyper-parameters that have been changed from their default values.\n        \"\"\"\n        return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparam","title":"<code>get_hparam(attribute)</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    if attribute not in dir(self.__io__):\n        raise KeyError(\n            f\"Attribute '{attribute}' not found in hyper-parameters. Options: {sorted(self.get_hparams().keys())}\"\n        )\n    return getattr(self.__io__, attribute)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_hparams","title":"<code>get_hparams()</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    return {k: getattr(self.__io__, k) for k in self.get_non_default_hparams()}\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.get_non_default_hparams","title":"<code>get_non_default_hparams()</code>","text":"<p>Returns a list of hyper-parameters that have been changed from their default values.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of hyper-parameters that have been changed from their default values.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def get_non_default_hparams(self) -&gt; List[str]:\n    \"\"\"Returns a list of hyper-parameters that have been changed from their default values.\n\n    Returns:\n        List[str]: A list of hyper-parameters that have been changed from their default values.\n    \"\"\"\n    return [k for k in self.__io__.__dict__[\"__argument_history__\"].keys() if k != \"__fn_or_cls__\"]\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.IOMixinWithGettersSetters.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    # Change the attribute of self and also change the io tracker so it gets updated in the config\n    if also_change_value:\n        setattr(self, attribute, value)\n    setattr(self.__io__, attribute, value)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam","title":"<code>WillHaveGetSetHparam</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An ABC that states that a particular class will have our mutatable IO Mixin variant added to it.</p> <p>This is a placeholder until a similar piece of functionality is added in NeMo.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>You must implement set_hparam, get_hparam, and get_hparams</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>class WillHaveGetSetHparam(ABC):\n    \"\"\"An ABC that states that a particular class _will_ have our mutatable IO Mixin variant added to it.\n\n    This is a placeholder until a similar piece of functionality is added in NeMo.\n\n\n    Raises:\n        NotImplementedError: You must implement set_hparam, get_hparam, and get_hparams\n    \"\"\"\n\n    @abstractmethod\n    def set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n        \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n        If you would like to only change the saved hyper-param\n            for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n            entities by deterministic rules after init, then use `also_change_value=False` to only update the\n            hyper-parameter.\n\n        Args:\n            attribute: The element name to modify within the saved init settings for self\n            value: New parameter for the saved init settings\n            also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n                value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n                do not set this and modify the self attribute separately in the normal pythonic way.\n\n        Returns:\n            None.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparam(self, attribute: str) -&gt; Any:\n        \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n        Args:\n            attribute: The element name to look up within the saved init settings for self\n        Returns:\n            Value\n        Raises:\n            KeyError if the attribute does not exist in the saved init settings.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_hparams(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n        Returns:\n            Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparam","title":"<code>get_hparam(attribute)</code>  <code>abstractmethod</code>","text":"<p>Looks up the saved hyper-parameter for the io mixed class.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to look up within the saved init settings for self</p> required <p>Returns:     Value Raises:     KeyError if the attribute does not exist in the saved init settings.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparam(self, attribute: str) -&gt; Any:\n    \"\"\"Looks up the saved hyper-parameter for the io mixed class.\n\n    Args:\n        attribute: The element name to look up within the saved init settings for self\n    Returns:\n        Value\n    Raises:\n        KeyError if the attribute does not exist in the saved init settings.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.get_hparams","title":"<code>get_hparams()</code>  <code>abstractmethod</code>","text":"<p>Returns the hyper-parameters of init in a dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of the init hyper-parameters on this object.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef get_hparams(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns the hyper-parameters of init in a dictionary format.\n\n    Returns:\n        Dict[str, Any]: A dictionary of the init hyper-parameters on this object.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/iomixin_utils/#bionemo.llm.utils.iomixin_utils.WillHaveGetSetHparam.set_hparam","title":"<code>set_hparam(attribute, value, also_change_value=True)</code>  <code>abstractmethod</code>","text":"<p>Mutates the saved hyper-parameter for the io mixed class.</p> <p>If you would like to only change the saved hyper-param     for example in the case of loading a dataclass where the same variables are mutated to other non-savable     entities by deterministic rules after init, then use <code>also_change_value=False</code> to only update the     hyper-parameter.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The element name to modify within the saved init settings for self</p> required <code>value</code> <code>Any</code> <p>New parameter for the saved init settings</p> required <code>also_change_value</code> <code>bool</code> <p>If you also want to mutate the attribute of this same name in self to be the desired value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then do not set this and modify the self attribute separately in the normal pythonic way.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>bionemo/llm/utils/iomixin_utils.py</code> <pre><code>@abstractmethod\ndef set_hparam(self, attribute: str, value: Any, also_change_value: bool = True) -&gt; None:\n    \"\"\"Mutates the saved hyper-parameter for the io mixed class.\n\n    If you would like to only change the saved hyper-param\n        for example in the case of loading a dataclass where the same variables are mutated to other non-savable\n        entities by deterministic rules after init, then use `also_change_value=False` to only update the\n        hyper-parameter.\n\n    Args:\n        attribute: The element name to modify within the saved init settings for self\n        value: New parameter for the saved init settings\n        also_change_value: If you also want to mutate the attribute of this same name in self to be the desired\n            value, set this to True, otherwise if the init arg and self arg are expected to be divergent, then\n            do not set this and modify the self attribute separately in the normal pythonic way.\n\n    Returns:\n        None.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/logger_utils/","title":"Logger utils","text":""},{"location":"API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.WandbConfig","title":"<code>WandbConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Note: <code>name</code> controls the exp name is handled by the NeMoLogger so it is ommitted here. <code>directory</code> is also omitted since it is set by the NeMoLogger.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <p>The team posting this run (default: your username or your default team)</p> required <code>project</code> <p>The name of the project to which this run will belong.</p> required <code>tags</code> <p>Tags associated with this run.</p> required <code>group</code> <p>A unique string shared by all runs in a given group</p> required <code>offline</code> <p>Run offline (data can be streamed later to wandb servers).</p> required <code>id</code> <p>Sets the version, mainly used to resume a previous run.</p> required <code>anonymous</code> <p>Enables or explicitly disables anonymous logging.</p> required Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>class WandbConfig(BaseModel):\n    \"\"\"Note: `name` controls the exp name is handled by the NeMoLogger so it is ommitted here.\n    `directory` is also omitted since it is set by the NeMoLogger.\n\n    Args:\n        entity: The team posting this run (default: your username or your default team)\n        project: The name of the project to which this run will belong.\n        tags: Tags associated with this run.\n        group: A unique string shared by all runs in a given group\n        offline: Run offline (data can be streamed later to wandb servers).\n        id: Sets the version, mainly used to resume a previous run.\n        anonymous: Enables or explicitly disables anonymous logging.\n    \"\"\"  # noqa: D205\n\n    entity: str | None  # The team posting this run (default: your username or your default team)\n    project: str  # The name of the project to which this run will belong.\n    # name: #Display name for the run. \"This is handled by NeMoLogger\"\n    # save_dir: #Path where data is saved. \"This is handled by NeMoLogger\"\n    tags: List[str] | None  # Tags associated with this run.\n    group: str | None  # A unique string shared by all runs in a given group\n    offline: bool  # Run offline (data can be streamed later to wandb servers).\n    id: str | None  # Sets the version, mainly used to resume a previous run.\n    anonymous: bool  # Enables or explicitly disables anonymous logging.\n    log_model: bool  # Save checkpoints in wandb dir to upload on W&amp;B servers.\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/logger_utils/#bionemo.llm.utils.logger_utils.setup_nemo_lightning_logger","title":"<code>setup_nemo_lightning_logger(name='default-name', root_dir='./results', initialize_tensorboard_logger=False, wandb_config=None, ckpt_callback=None, **kwargs)</code>","text":"<p>Setup the logger for the experiment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the experiment. Results go into <code>root_dir</code>/<code>name</code></p> <code>'default-name'</code> <code>root_dir</code> <code>str | Path</code> <p>The root directory to create the <code>name</code> directory in for saving run results.</p> <code>'./results'</code> <code>initialize_tensorboard_logger</code> <code>bool</code> <p>Whether to initialize the tensorboard logger.</p> <code>False</code> <code>wandb_config</code> <code>Optional[WandbConfig]</code> <p>The remaining configuration options for the wandb logger.</p> <code>None</code> <code>ckpt_callback</code> <code>Optional[ModelCheckpoint]</code> <p>The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback. NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.</p> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The kwargs for the NeMoLogger.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>NeMoLogger</code> <code>NeMoLogger</code> <p>NeMo logger instance.</p> Source code in <code>bionemo/llm/utils/logger_utils.py</code> <pre><code>def setup_nemo_lightning_logger(\n    name: str = \"default-name\",\n    root_dir: str | pathlib.Path = \"./results\",\n    initialize_tensorboard_logger: bool = False,\n    wandb_config: Optional[WandbConfig] = None,\n    ckpt_callback: Optional[nemo_callbacks.ModelCheckpoint] = None,\n    **kwargs: Dict[str, Any],\n) -&gt; NeMoLogger:\n    \"\"\"Setup the logger for the experiment.\n\n    Arguments:\n        name: The name of the experiment. Results go into `root_dir`/`name`\n        root_dir: The root directory to create the `name` directory in for saving run results.\n        initialize_tensorboard_logger: Whether to initialize the tensorboard logger.\n        wandb_config: The remaining configuration options for the wandb logger.\n        ckpt_callback: The checkpoint callback to use, must be a child of the pytorch lightning ModelCheckpoint callback.\n            NOTE the type annotation in the underlying NeMoCheckpoint constructor is incorrect.\n        **kwargs: The kwargs for the NeMoLogger.\n\n    Returns:\n        NeMoLogger: NeMo logger instance.\n    \"\"\"\n    # The directory that the logger will save to\n    save_dir = pathlib.Path(root_dir) / name\n    if wandb_config is not None:\n        wandb_logger = WandbLogger(save_dir=save_dir, name=name, **wandb_config.model_dump())\n    else:\n        wandb_logger = None\n        logging.warning(\"WandB is currently turned off.\")\n    if initialize_tensorboard_logger:\n        tb_logger = TensorBoardLogger(save_dir=save_dir, name=name)\n    else:\n        tb_logger = None\n        logging.warning(\"User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\")\n    logger: NeMoLogger = NeMoLogger(\n        name=name,\n        log_dir=str(root_dir),\n        tensorboard=tb_logger,\n        wandb=wandb_logger,\n        ckpt=ckpt_callback,\n        use_datetime_version=False,\n        version=\"dev\",\n        **kwargs,\n    )\n    # Needed so that the trainer can find an output directory for the profiler\n    logger.save_dir = save_dir\n    return logger\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/megatron_utils/","title":"Megatron utils","text":""},{"location":"API_reference/bionemo/llm/utils/megatron_utils/#bionemo.llm.utils.megatron_utils.is_only_data_parallel","title":"<code>is_only_data_parallel()</code>","text":"<p>Checks to see if you are in a distributed megatron environment with only data parallelism active.</p> <p>This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model parallelism. You can test that the only kind of parallelism in use is data parallelism.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if data parallel is the only parallel mode, False otherwise.</p> Source code in <code>bionemo/llm/utils/megatron_utils.py</code> <pre><code>def is_only_data_parallel() -&gt; bool:\n    \"\"\"Checks to see if you are in a distributed megatron environment with only data parallelism active.\n\n    This is useful if you are working on a model, loss, etc and you know that you do not yet support megatron model\n    parallelism. You can test that the only kind of parallelism in use is data parallelism.\n\n    Returns:\n        True if data parallel is the only parallel mode, False otherwise.\n    \"\"\"\n    if not (torch.distributed.is_available() and parallel_state.is_initialized()):\n        raise RuntimeError(\"This function is only defined within an initialized megatron parallel environment.\")\n    # Idea: when world_size == data_parallel_world_size, then you know that you are fully DDP, which means you are not\n    #  using model parallelism (meaning virtual GPUs composed of several underlying GPUs that you need to reduce over).\n\n    world_size: int = torch.distributed.get_world_size()\n    dp_world_size: int = parallel_state.get_data_parallel_world_size()\n    return world_size == dp_world_size\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/","title":"Remote","text":""},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource","title":"<code>FTPRemoteResource</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RemoteResource</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>class FTPRemoteResource(RemoteResource):  # noqa: D101\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.FTPRemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        request.urlretrieve(self.url, self.fully_qualified_dest_filename)\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource","title":"<code>RemoteResource</code>  <code>dataclass</code>","text":"<p>Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.</p> <p>Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method. <code>download_resource()</code> contains the core functionality, which is to download the file at <code>url</code> to the fully qualified filename. Class methods can be used to further configure this process.</p> Receive <p>a file, its checksum, a destination directory, and a root directory</p> <p>Our dataclass then provides some useful things:     - fully qualified destination folder (property)     - fully qualified destination file (property)     - check_exists()     - download_resource()</p> <p>Form the fully qualified destination folder. Create a fully qualified path for the file</p> <p>(all lives in the download routine) Check that the fq destination folder exists, otherwise create it Download the file. Checksum the download. Done.</p> <p>Postprocessing should be their own method with their own configuration.</p> Example usage <p>Attributes:</p> Name Type Description <code>dest_directory</code> <code>str</code> <p>The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}</p> <code>dest_filename</code> <code>str</code> <p>The desired name for the file upon completing the download.</p> <code>checksum</code> <code>Optional[str]</code> <p>checksum associated with the file located at url. If set to None, check_exists only checks for the existance of <code>{dest_directory}/{dest_filename}</code></p> <code>url</code> <code>Optional[str]</code> <p>URL of the file to download</p> <code>root_directory</code> <code>str | PathLike</code> <p>the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@dataclass\nclass RemoteResource:\n    \"\"\"Responsible for downloading remote files, along with optional processing of downloaded files for downstream usecases.\n\n    Each object is invoked through either its constructor (setting up the destination and checksum), or through a pre-configured class method.\n    `download_resource()` contains the core functionality, which is to download the file at `url` to the fully qualified filename. Class methods\n    can be used to further configure this process.\n\n    Receive:\n        a file, its checksum, a destination directory, and a root directory\n\n        Our dataclass then provides some useful things:\n            - fully qualified destination folder (property)\n            - fully qualified destination file (property)\n            - check_exists()\n            - download_resource()\n\n        Form the fully qualified destination folder.\n        Create a fully qualified path for the file\n\n        (all lives in the download routine)\n        Check that the fq destination folder exists, otherwise create it\n        Download the file.\n        Checksum the download.\n        Done.\n\n        Postprocessing should be their own method with their own configuration.\n\n    Example usage:\n        &gt;&gt;&gt; # The following will download and preprocess the prepackaged resources.\n        &gt;&gt;&gt; GRCh38Ensembl99ResourcePreparer().prepare()\n        &gt;&gt;&gt; Hg38chromResourcePreparer().prepare()\n        &gt;&gt;&gt; GRCh38p13_ResourcePreparer().prepare()\n\n\n    Attributes:\n        dest_directory: The directory to place the desired file upon completing the download. Should have the form {dest_directory}/{dest_filename}\n        dest_filename: The desired name for the file upon completing the download.\n        checksum: checksum associated with the file located at url. If set to None, check_exists only checks for the existance of `{dest_directory}/{dest_filename}`\n        url: URL of the file to download\n        root_directory: the bottom-level directory, the fully qualified path is formed by joining root_directory, dest_directory, and dest_filename.\n    \"\"\"\n\n    checksum: Optional[str]\n    dest_filename: str\n    dest_directory: str\n    root_directory: str | os.PathLike = BIONEMO_CACHE_DIR\n    url: Optional[str] = None\n\n    @property\n    def fully_qualified_dest_folder(self):  # noqa: D102\n        return Path(self.root_directory) / self.dest_directory\n\n    @property\n    def fully_qualified_dest_filename(self):\n        \"\"\"Returns the fully qualified destination path of the file.\n\n        Example:\n            /tmp/my_folder/file.tar.gz\n        \"\"\"\n        return os.path.join(self.fully_qualified_dest_folder, self.dest_filename)\n\n    def exists_or_create_destination_directory(self, exist_ok=True):\n        \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n        exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n        \"\"\"\n        os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n\n    @staticmethod\n    def get_env_tmpdir():\n        \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n        return os.environ.get(\"TMPDIR\", \"/tmp\")\n\n    def download_resource(self, overwrite=False) -&gt; str:\n        \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n        Returns: the fully qualified destination filename.\n        \"\"\"\n        self.exists_or_create_destination_directory()\n\n        if not self.check_exists() or overwrite:\n            logging.info(f\"Downloading resource: {self.url}\")\n            with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n                r.raise_for_status()\n                for bytes in r:\n                    fd.write(bytes)\n        else:\n            logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n        self.check_exists()\n        return self.fully_qualified_dest_filename\n\n    def check_exists(self):\n        \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n        if os.path.exists(self.fully_qualified_dest_filename):\n            with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n                data = fd.read()\n                result = md5(data).hexdigest()\n            if self.checksum is None:\n                logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n                matches = True\n            else:\n                matches = result == self.checksum\n            return matches\n\n        return False\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource--the-following-will-download-and-preprocess-the-prepackaged-resources","title":"The following will download and preprocess the prepackaged resources.","text":"<p>GRCh38Ensembl99ResourcePreparer().prepare() Hg38chromResourcePreparer().prepare() GRCh38p13_ResourcePreparer().prepare()</p>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.fully_qualified_dest_filename","title":"<code>fully_qualified_dest_filename</code>  <code>property</code>","text":"<p>Returns the fully qualified destination path of the file.</p> Example <p>/tmp/my_folder/file.tar.gz</p>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.check_exists","title":"<code>check_exists()</code>","text":"<p>Returns true if <code>fully_qualified_dest_filename</code> exists and the checksum matches <code>self.checksum</code></p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def check_exists(self):\n    \"\"\"Returns true if `fully_qualified_dest_filename` exists and the checksum matches `self.checksum`\"\"\"  # noqa: D415\n    if os.path.exists(self.fully_qualified_dest_filename):\n        with open(self.fully_qualified_dest_filename, \"rb\") as fd:\n            data = fd.read()\n            result = md5(data).hexdigest()\n        if self.checksum is None:\n            logging.info(\"No checksum provided, filename exists. Assuming it is complete.\")\n            matches = True\n        else:\n            matches = result == self.checksum\n        return matches\n\n    return False\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.download_resource","title":"<code>download_resource(overwrite=False)</code>","text":"<p>Downloads the resource to its specified fully_qualified_dest name.</p> <p>Returns: the fully qualified destination filename.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def download_resource(self, overwrite=False) -&gt; str:\n    \"\"\"Downloads the resource to its specified fully_qualified_dest name.\n\n    Returns: the fully qualified destination filename.\n    \"\"\"\n    self.exists_or_create_destination_directory()\n\n    if not self.check_exists() or overwrite:\n        logging.info(f\"Downloading resource: {self.url}\")\n        with requests.get(self.url, stream=True) as r, open(self.fully_qualified_dest_filename, \"wb\") as fd:\n            r.raise_for_status()\n            for bytes in r:\n                fd.write(bytes)\n    else:\n        logging.info(f\"Resource already exists, skipping download: {self.url}\")\n\n    self.check_exists()\n    return self.fully_qualified_dest_filename\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.exists_or_create_destination_directory","title":"<code>exists_or_create_destination_directory(exist_ok=True)</code>","text":"<p>Checks that the <code>fully_qualified_destination_directory</code> exists, if it does not, the directory is created (or fails).</p> <p>exists_ok: Triest to create <code>fully_qualified_dest_folder</code> if it doesnt already exist.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>def exists_or_create_destination_directory(self, exist_ok=True):\n    \"\"\"Checks that the `fully_qualified_destination_directory` exists, if it does not, the directory is created (or fails).\n\n    exists_ok: Triest to create `fully_qualified_dest_folder` if it doesnt already exist.\n    \"\"\"\n    os.makedirs(self.fully_qualified_dest_folder, exist_ok=exist_ok)\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/remote/#bionemo.llm.utils.remote.RemoteResource.get_env_tmpdir","title":"<code>get_env_tmpdir()</code>  <code>staticmethod</code>","text":"<p>Convenience method that exposes the environment TMPDIR variable.</p> Source code in <code>bionemo/llm/utils/remote.py</code> <pre><code>@staticmethod\ndef get_env_tmpdir():\n    \"\"\"Convenience method that exposes the environment TMPDIR variable.\"\"\"\n    return os.environ.get(\"TMPDIR\", \"/tmp\")\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/weight_utils/","title":"Weight utils","text":""},{"location":"API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.load_weights_sharded_inplace_nemo2_to_mcore","title":"<code>load_weights_sharded_inplace_nemo2_to_mcore(model, distributed_checkpoint_dir, skip_keys_with_these_prefixes)</code>","text":"<p>Given a megatron module, this function will determine which keys/subsets of weights to load given the     parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places     the <code>module.</code> prefix on all key names, but we are then going to load directly in to the megatron module     without the <code>module.</code> prefix. Note that if there are any extra keys that you do not want to search the     checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix     path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning     strategies where you load weights partially from other models with partially overlapping structures.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MegatronModelType</code> <p>Megatron model that you want to load weights into.</p> required <code>distributed_checkpoint_dir</code> <code>str | Path</code> <p>description</p> required <code>skip_keys_with_these_prefixes</code> <code>Set[str]</code> <p>description</p> required Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def load_weights_sharded_inplace_nemo2_to_mcore(\n    model: MegatronModelType, distributed_checkpoint_dir: str | Path, skip_keys_with_these_prefixes: Set[str]\n) -&gt; None:\n    \"\"\"Given a megatron module, this function will determine which keys/subsets of weights to load given the\n        parallel/distributed state. This operates assuming a checkpoint was saved by a nemo2 trainer which places\n        the `module.` prefix on all key names, but we are then going to load directly in to the megatron module\n        without the `module.` prefix. Note that if there are any _extra_ keys that you do not want to search the\n        checkpoint for, for example if you add new layers/heads onto your module, you need to supply the prefix\n        path to those keys in your model and they will be ignored. This latter feature is key for flexible fine-tuning\n        strategies where you load weights partially from other models with partially overlapping structures.\n\n    Args:\n        model: Megatron model that you want to load weights into.\n        distributed_checkpoint_dir: _description_\n        skip_keys_with_these_prefixes: _description_\n    \"\"\"  # noqa: D205\n    sharded_state_dict = {\n        _munge_key_megatron_to_nemo2(k): _munge_sharded_tensor_key_megatron_to_nemo2(v)\n        for k, v in model.sharded_state_dict().items()\n        if not _key_in_filter(k, skip_keys_with_these_prefixes)\n    }\n    dist_checkpointing.load(\n        sharded_state_dict=sharded_state_dict,\n        checkpoint_dir=str(Path(distributed_checkpoint_dir) / \"weights\"),\n        strict=dist_checkpointing.serialization.StrictHandling.ASSUME_OK_UNEXPECTED,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/llm/utils/weight_utils/#bionemo.llm.utils.weight_utils.nemo1_to_nemo2_biobert_key_mapping","title":"<code>nemo1_to_nemo2_biobert_key_mapping(old_key, new_model_prefix='module', old_model_prefix='model', te_mapping=False)</code>","text":"<p>This function is used to map the keys from the old nemo BERT models to the new BioBERT models</p> <p>Parameters:</p> Name Type Description Default <code>old_key</code> <code>str</code> <p>old key we want to map to the expected new key name.</p> required <code>new_model_prefix</code> <code>str</code> <p>The new key for the base weights. If you point this at the core megatron model set it to \"\". For the regular nemo2 lightning module following standards, set it to \"module\". Defaults to \"module\".</p> <code>'module'</code> <code>old_model_prefix</code> <code>str</code> <p>The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.</p> <code>'model'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>New key name</p> Source code in <code>bionemo/llm/utils/weight_utils.py</code> <pre><code>def nemo1_to_nemo2_biobert_key_mapping(  # noqa: D417\n    old_key: str,\n    new_model_prefix: str = \"module\",\n    old_model_prefix: str = \"model\",\n    te_mapping: bool = False,\n) -&gt; str:\n    \"\"\"This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n\n    Args:\n        old_key (str): old key we want to map to the expected new key name.\n        new_model_prefix (str, optional): The new key for the base weights.\n            If you point this at the core megatron model set it to \"\".\n            For the regular nemo2 lightning module following standards, set it to \"module\".\n            Defaults to \"module\".\n        old_model_prefix (str, optional): The previous saved weight prefix. Defaults to \"model\" which was the standard in nemo1.\n\n    Returns:\n        str: New key name\n    \"\"\"  # noqa: D415\n    # add the . to the end of the input prefixes if they are not the empty string,\n    #  unless the user has already done so.\n    if old_model_prefix != \"\":\n        old_model_prefix = f\"{old_model_prefix.rstrip('.')}.\"\n    if new_model_prefix != \"\":\n        new_model_prefix = f\"{new_model_prefix.rstrip('.')}.\"\n\n    # This function is used to map the keys from the old nemo BERT models to the new BioBERT models\n    base_rename = old_key.replace(f\"{old_model_prefix}language_model.\", f\"{new_model_prefix}\")\n    base_rename = base_rename.replace(f\"{old_model_prefix}\", f\"{new_model_prefix}\")\n    if \"dense_h_to_4h\" in base_rename:\n        return base_rename.replace(\"dense_h_to_4h\", \"linear_fc1\")\n    if \"dense_4h_to_h\" in base_rename:\n        return base_rename.replace(\"dense_4h_to_h\", \"linear_fc2\")\n    if \"query_key_value\" in base_rename:\n        return base_rename.replace(\"query_key_value\", \"linear_qkv\")\n    if \"self_attention.dense\" in base_rename:\n        #  This is definitely the linear_proj and not the qkv. The linear_proj shapes are 256x256\n        #   which match dense but not query_key_value\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_proj.weight'].shape\n        #  torch.Size([256, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.weight'].shape\n        # torch.Size([768, 256])\n        # (Pdb) new_state_dict['encoder.layers.4.self_attention.linear_qkv.bias'].shape\n        # torch.Size([768])\n        return base_rename.replace(\"self_attention.dense\", \"self_attention.linear_proj\")\n    if \"lm_head.bias\" in base_rename:\n        return base_rename.replace(\"lm_head.bias\", \"output_layer.bias\")\n    if \"lm_head.weight\" in base_rename:\n        return base_rename.replace(\"lm_head.weight\", \"output_layer.weight\")\n    if \"lm_head.layernorm\" in base_rename:\n        return base_rename.replace(\"lm_head.layernorm\", \"lm_head.layer_norm\")\n\n    if \"post_attention_layernorm\" in base_rename:\n        base_rename = base_rename.replace(\"post_attention_layernorm\", \"pre_mlp_layernorm\")\n\n    # Handle the transformer engine spec's differences in layer naming and where things like layernorm are stored.\n    #  TE moves layernorm from  an object that's part of the main attention layer to being an internal component of\n    #  the linear layers, probably for efficiency/fusion of some sort.\n    if te_mapping:\n        if \".input_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".input_layernorm.weight\", \".self_attention.linear_qkv.layer_norm_weight\")\n        if \".input_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".input_layernorm.bias\", \".self_attention.linear_qkv.layer_norm_bias\")\n        if \".pre_mlp_layernorm.bias\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.bias\", \".mlp.linear_fc1.layer_norm_bias\")\n        if \".pre_mlp_layernorm.weight\" in base_rename:\n            return base_rename.replace(\".pre_mlp_layernorm.weight\", \".mlp.linear_fc1.layer_norm_weight\")\n    return base_rename\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/","title":"Single cell row dataset","text":""},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset","title":"<code>SingleCellRowDataset</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code>, <code>Dataset</code></p> <p>One row in an ann dataframe (hdf5 file with a spare array format).</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDataset(SingleCellRowDatasetCore, Dataset):\n    \"\"\"One row in an ann dataframe (hdf5 file with a spare array format).\"\"\"\n\n    @abstractmethod\n    def load(self, data_path: str) -&gt; None:\n        \"\"\"Loads the data from datapath.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def save(self, data_path: str) -&gt; None:\n        \"\"\"Saves the class to an archive at datapath.\"\"\"\n        raise NotImplementedError()\n\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.load","title":"<code>load(data_path)</code>  <code>abstractmethod</code>","text":"<p>Loads the data from datapath.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load(self, data_path: str) -&gt; None:\n    \"\"\"Loads the data from datapath.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDataset.save","title":"<code>save(data_path)</code>  <code>abstractmethod</code>","text":"<p>Saves the class to an archive at datapath.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef save(self, data_path: str) -&gt; None:\n    \"\"\"Saves the class to an archive at datapath.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore","title":"<code>SingleCellRowDatasetCore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Implements the actual ann data-like interface.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>class SingleCellRowDatasetCore(ABC):\n    \"\"\"Implements the actual ann data-like interface.\"\"\"\n\n    @abstractmethod\n    def load_h5ad(self, h5ad_path: str) -&gt; None:\n        \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n        Calls to __len__ and __getitem__ Must be valid after a call to\n        this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Return the number of non-zero values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_values(self) -&gt; int:\n        \"\"\"Return the total number of values in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def number_of_rows(self) -&gt; int:\n        \"\"\"Return the number of rows in the data.\"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Returns the shape of the object, which may be ragged.\n\n        A ragged dataset is where the number and dimension of features\n        can be different at every row.\n        \"\"\"\n        raise NotImplementedError()\n\n    def sparsity(self) -&gt; float:\n        \"\"\"Return the sparsity of the underlying data.\n\n        Sparsity is defined as the fraction of zero values in the data.\n        It is within the range [0, 1.0]. If there are no values, the\n        sparsity is defined as 0.0.\n        \"\"\"\n        total_values = self.number_of_values()\n        if total_values == 0:\n            return 0.0\n\n        nonzero_values = self.number_nonzero_values()\n        zero_values = total_values - nonzero_values\n        sparsity_value = zero_values / total_values\n        return sparsity_value\n\n    @abstractmethod\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.load_h5ad","title":"<code>load_h5ad(h5ad_path)</code>  <code>abstractmethod</code>","text":"<p>Loads an H5AD file and converts it into the backing representation.</p> <p>Calls to len and getitem Must be valid after a call to this method.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef load_h5ad(self, h5ad_path: str) -&gt; None:\n    \"\"\"Loads an H5AD file and converts it into the backing representation.\n\n    Calls to __len__ and __getitem__ Must be valid after a call to\n    this method.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_nonzero_values","title":"<code>number_nonzero_values()</code>  <code>abstractmethod</code>","text":"<p>Return the number of non-zero values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_nonzero_values(self) -&gt; int:\n    \"\"\"Return the number of non-zero values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_rows","title":"<code>number_of_rows()</code>  <code>abstractmethod</code>","text":"<p>Return the number of rows in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_rows(self) -&gt; int:\n    \"\"\"Return the number of rows in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.number_of_values","title":"<code>number_of_values()</code>  <code>abstractmethod</code>","text":"<p>Return the total number of values in the data.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef number_of_values(self) -&gt; int:\n    \"\"\"Return the total number of values in the data.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.shape","title":"<code>shape()</code>  <code>abstractmethod</code>","text":"<p>Returns the shape of the object, which may be ragged.</p> <p>A ragged dataset is where the number and dimension of features can be different at every row.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Returns the shape of the object, which may be ragged.\n\n    A ragged dataset is where the number and dimension of features\n    can be different at every row.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.sparsity","title":"<code>sparsity()</code>","text":"<p>Return the sparsity of the underlying data.</p> <p>Sparsity is defined as the fraction of zero values in the data. It is within the range [0, 1.0]. If there are no values, the sparsity is defined as 0.0.</p> Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>def sparsity(self) -&gt; float:\n    \"\"\"Return the sparsity of the underlying data.\n\n    Sparsity is defined as the fraction of zero values in the data.\n    It is within the range [0, 1.0]. If there are no values, the\n    sparsity is defined as 0.0.\n    \"\"\"\n    total_values = self.number_of_values()\n    if total_values == 0:\n        return 0.0\n\n    nonzero_values = self.number_nonzero_values()\n    zero_values = total_values - nonzero_values\n    sparsity_value = zero_values / total_values\n    return sparsity_value\n</code></pre>"},{"location":"API_reference/bionemo/scdl/api/single_cell_row_dataset/#bionemo.scdl.api.single_cell_row_dataset.SingleCellRowDatasetCore.version","title":"<code>version()</code>  <code>abstractmethod</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/api/single_cell_row_dataset.py</code> <pre><code>@abstractmethod\ndef version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/","title":"Row feature index","text":""},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex","title":"<code>RowFeatureIndex</code>","text":"<p>Maintains a mapping between a row and its features.</p> <p>This is a ragged dataset, where the number and dimension of features can be different at every row.</p> <p>Attributes:</p> Name Type Description <code>_cumulative_sum_index</code> <code>array</code> <p>Pointer that deliniates which entries</p> <code>_feature_arr</code> <code>List[DataFrame]</code> <p>list of feature dataframes</p> <code>_labels</code> <code>List[str]</code> <p>list of labels</p> <code>_version</code> <p>The version of the dataset</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>class RowFeatureIndex:\n    \"\"\"Maintains a mapping between a row and its features.\n\n    This is a ragged dataset, where the number and dimension of features\n    can be different at every row.\n\n    Attributes:\n        _cumulative_sum_index: Pointer that deliniates which entries\n        correspondto a given row. For examples if the array is [-1, 200, 201],\n        rows 0 to 199 correspond to _feature_arr[0] and 200 corresponds to\n        _feature_arr[1]\n        _feature_arr: list of feature dataframes\n        _labels: list of labels\n        _version: The version of the dataset\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Instantiates the index.\"\"\"\n        self._cumulative_sum_index: np.array = np.array([-1])\n        self._feature_arr: List[pd.DataFrame] = []\n        self._version = importlib.metadata.version(\"bionemo.scdl\")\n        self._labels: List[str] = []\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def __len__(self) -&gt; int:\n        \"\"\"The length is the number of rows or RowFeatureIndex length.\"\"\"\n        return len(self._feature_arr)\n\n    def append_features(self, n_obs: int, features: pd.DataFrame, label: Optional[str] = None) -&gt; None:\n        \"\"\"Updates the index with the given features.\n\n        The dataframe is inserted into the feature array by adding a\n        new span to the row lookup index.\n\n        Args:\n            n_obs (int): The number of times that these feature occur in the\n            class.\n            features (pd.DataFrame): Corresponding features.\n            label (str): Label for the features.\n        \"\"\"\n        csum = max(self._cumulative_sum_index[-1], 0)\n        self._cumulative_sum_index = np.append(self._cumulative_sum_index, csum + n_obs)\n        self._feature_arr.append(features)\n        self._labels.append(label)\n\n    def lookup(self, row: int, select_features: Optional[List[str]] = None) -&gt; Tuple[pd.DataFrame, str]:\n        \"\"\"Find the features at a given row.\n\n        It is assumed that the row is\n        non-zero._cumulative_sum_index contains pointers to which rows correspond\n        to given dataframes. To obtain a specific row, we determine where it is\n        located in _cumulative_sum_index and then look up that dataframe in\n        _feature_arr\n        Args:\n            row (int): The row in the feature index.\n            select_features (List[str]): a list of features to select\n        Returns\n            pd.DataFrame: dataframe of features in that row\n            str: optional label for the row\n        Raises:\n            IndexError: An error occured due to input row being negative or it\n            exceeding the larger row of the rows in the index. It is also raised\n            if there are no entries in the index yet.\n        \"\"\"\n        if row &lt; 0:\n            raise IndexError(f\"Row index {row} is not valid. It must be non-negative.\")\n        if len(self._cumulative_sum_index) &lt; 2:\n            raise IndexError(\"There are no dataframes to lookup.\")\n\n        if row &gt; self._cumulative_sum_index[-1]:\n            raise IndexError(\n                f\"Row index {row} is larger than number of rows in FeatureIndex ({self._cumulative_sum_index[-1]}).\"\n            )\n        # This line does the following:\n        # creates a mask for values where cumulative sum &gt; row\n        mask = ~(self._cumulative_sum_index &gt; row)\n        # Sum these to get the index of the first range &gt; row\n        # Subtract one to get the range containing row.\n        d_id = sum(mask) - 1\n\n        # Retrieve the features for the identified value.\n        features = self._feature_arr[d_id]\n\n        # If specific features are to be selected, filter the features.\n        if select_features is not None:\n            features = features[select_features]\n\n        # Return the features for the identified range.\n        return features, self._labels[d_id]\n\n    def number_vars_at_row(self, row: int) -&gt; int:\n        \"\"\"Return number of variables (legnth of the dataframe) in a given row.\n\n        Args:\n            row (int): The row in the feature index.\n\n        Returns:\n            The length of the features at the row\n        \"\"\"\n        feats, _ = self.lookup(row=row)\n        return len(feats)\n\n    def column_dims(self) -&gt; List[int]:\n        \"\"\"Return the number of columns in all rows.\n\n        Args:\n            length of features at every row is returned.\n\n        Returns:\n            A list containing the lengths of the features in every row\n        \"\"\"\n        # Just take the total dim of the DataFrame(s)\n        return [len(feats) for feats in self._feature_arr]\n\n    def number_of_values(self) -&gt; List[int]:\n        \"\"\"Get the total number of values in the array.\n\n        For each row, the length of the corresponding dataframe is counted.\n\n        Returns:\n            A list containing the lengths of the features in every block of rows\n        \"\"\"\n        if len(self._feature_arr) == 0:\n            return [0]\n        rows = [\n            self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n            for i in range(1, len(self._cumulative_sum_index))\n        ]\n\n        vals = [n_rows * len(self._feature_arr[i]) for i, n_rows in enumerate(rows)]\n        return vals\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataframe.\n\n        Returns:\n            An integer corresponding to the number or rows in the index\n        \"\"\"\n        return int(max(self._cumulative_sum_index[-1], 0))\n\n    def concat(self, other_row_index: RowFeatureIndex, fail_on_empty_index: bool = True) -&gt; RowFeatureIndex:\n        \"\"\"Concatenates the other FeatureIndex to this one.\n\n        Returns the new, updated index. Warning: modifies this index in-place.\n\n        Args:\n            other_row_index: another RowFeatureIndex\n            fail_on_empty_index: A boolean flag that sets whether to raise an\n            error if an empty row index is passed in.\n\n        Returns:\n            self, the RowIndexFeature after the concatenations.\n\n        Raises:\n            TypeError if other_row_index is not a RowFeatureIndex\n            ValueError if an empty RowFeatureIndex is passed and the function is\n            set to fail in this case.\n        \"\"\"\n        match other_row_index:\n            case self.__class__():\n                pass\n            case _:\n                raise TypeError(\"Error: trying to concatenate something that's not a RowFeatureIndex.\")\n\n        if fail_on_empty_index and not len(other_row_index._feature_arr) &gt; 0:\n            raise ValueError(\"Error: Cannot append empty FeatureIndex.\")\n        for i, feats in enumerate(list(other_row_index._feature_arr)):\n            c_span = other_row_index._cumulative_sum_index[i + 1]\n            label = other_row_index._labels[i]\n            self.append_features(c_span, feats, label)\n\n        return self\n\n    def save(self, datapath: str) -&gt; None:\n        \"\"\"Saves the RowFeatureIndex to a given path.\n\n        Args:\n            datapath: path to save the index\n        \"\"\"\n        Path(datapath).mkdir(parents=True, exist_ok=True)\n        num_digits = len(str(len(self._feature_arr)))\n\n        for dataframe_index, dataframe in enumerate(self._feature_arr):\n            dataframe_str_index = f\"{dataframe_index:0{num_digits}d}\"\n            dataframe.to_parquet(f\"{datapath}/dataframe_{dataframe_str_index}.parquet\", index=False)\n        np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n        np.save(Path(datapath) / \"labels.npy\", self._labels)\n        np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n\n    @staticmethod\n    def load(datapath: str) -&gt; RowFeatureIndex:\n        \"\"\"Loads the data from datapath.\n\n        Args:\n            datapath: the path to load from\n        Returns:\n            An instance of RowFeatureIndex\n        \"\"\"\n        new_row_feat_index = RowFeatureIndex()\n        parquet_data_paths = sorted(Path(datapath).rglob(\"*.parquet\"))\n        new_row_feat_index._feature_arr = [pd.read_parquet(csv_path) for csv_path in parquet_data_paths]\n        new_row_feat_index._cumulative_sum_index = np.load(Path(datapath) / \"cumulative_sum_index.npy\")\n        new_row_feat_index._labels = np.load(Path(datapath) / \"labels.npy\", allow_pickle=True)\n        new_row_feat_index._version = np.load(Path(datapath) / \"version.npy\").item()\n        return new_row_feat_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__init__","title":"<code>__init__()</code>","text":"<p>Instantiates the index.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Instantiates the index.\"\"\"\n    self._cumulative_sum_index: np.array = np.array([-1])\n    self._feature_arr: List[pd.DataFrame] = []\n    self._version = importlib.metadata.version(\"bionemo.scdl\")\n    self._labels: List[str] = []\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.__len__","title":"<code>__len__()</code>","text":"<p>The length is the number of rows or RowFeatureIndex length.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"The length is the number of rows or RowFeatureIndex length.\"\"\"\n    return len(self._feature_arr)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.append_features","title":"<code>append_features(n_obs, features, label=None)</code>","text":"<p>Updates the index with the given features.</p> <p>The dataframe is inserted into the feature array by adding a new span to the row lookup index.</p> <p>Parameters:</p> Name Type Description Default <code>n_obs</code> <code>int</code> <p>The number of times that these feature occur in the</p> required <code>features</code> <code>DataFrame</code> <p>Corresponding features.</p> required <code>label</code> <code>str</code> <p>Label for the features.</p> <code>None</code> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def append_features(self, n_obs: int, features: pd.DataFrame, label: Optional[str] = None) -&gt; None:\n    \"\"\"Updates the index with the given features.\n\n    The dataframe is inserted into the feature array by adding a\n    new span to the row lookup index.\n\n    Args:\n        n_obs (int): The number of times that these feature occur in the\n        class.\n        features (pd.DataFrame): Corresponding features.\n        label (str): Label for the features.\n    \"\"\"\n    csum = max(self._cumulative_sum_index[-1], 0)\n    self._cumulative_sum_index = np.append(self._cumulative_sum_index, csum + n_obs)\n    self._feature_arr.append(features)\n    self._labels.append(label)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.column_dims","title":"<code>column_dims()</code>","text":"<p>Return the number of columns in all rows.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list containing the lengths of the features in every row</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def column_dims(self) -&gt; List[int]:\n    \"\"\"Return the number of columns in all rows.\n\n    Args:\n        length of features at every row is returned.\n\n    Returns:\n        A list containing the lengths of the features in every row\n    \"\"\"\n    # Just take the total dim of the DataFrame(s)\n    return [len(feats) for feats in self._feature_arr]\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.concat","title":"<code>concat(other_row_index, fail_on_empty_index=True)</code>","text":"<p>Concatenates the other FeatureIndex to this one.</p> <p>Returns the new, updated index. Warning: modifies this index in-place.</p> <p>Parameters:</p> Name Type Description Default <code>other_row_index</code> <code>RowFeatureIndex</code> <p>another RowFeatureIndex</p> required <code>fail_on_empty_index</code> <code>bool</code> <p>A boolean flag that sets whether to raise an</p> <code>True</code> <p>Returns:</p> Type Description <code>RowFeatureIndex</code> <p>self, the RowIndexFeature after the concatenations.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def concat(self, other_row_index: RowFeatureIndex, fail_on_empty_index: bool = True) -&gt; RowFeatureIndex:\n    \"\"\"Concatenates the other FeatureIndex to this one.\n\n    Returns the new, updated index. Warning: modifies this index in-place.\n\n    Args:\n        other_row_index: another RowFeatureIndex\n        fail_on_empty_index: A boolean flag that sets whether to raise an\n        error if an empty row index is passed in.\n\n    Returns:\n        self, the RowIndexFeature after the concatenations.\n\n    Raises:\n        TypeError if other_row_index is not a RowFeatureIndex\n        ValueError if an empty RowFeatureIndex is passed and the function is\n        set to fail in this case.\n    \"\"\"\n    match other_row_index:\n        case self.__class__():\n            pass\n        case _:\n            raise TypeError(\"Error: trying to concatenate something that's not a RowFeatureIndex.\")\n\n    if fail_on_empty_index and not len(other_row_index._feature_arr) &gt; 0:\n        raise ValueError(\"Error: Cannot append empty FeatureIndex.\")\n    for i, feats in enumerate(list(other_row_index._feature_arr)):\n        c_span = other_row_index._cumulative_sum_index[i + 1]\n        label = other_row_index._labels[i]\n        self.append_features(c_span, feats, label)\n\n    return self\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.load","title":"<code>load(datapath)</code>  <code>staticmethod</code>","text":"<p>Loads the data from datapath.</p> <p>Parameters:</p> Name Type Description Default <code>datapath</code> <code>str</code> <p>the path to load from</p> required <p>Returns:     An instance of RowFeatureIndex</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>@staticmethod\ndef load(datapath: str) -&gt; RowFeatureIndex:\n    \"\"\"Loads the data from datapath.\n\n    Args:\n        datapath: the path to load from\n    Returns:\n        An instance of RowFeatureIndex\n    \"\"\"\n    new_row_feat_index = RowFeatureIndex()\n    parquet_data_paths = sorted(Path(datapath).rglob(\"*.parquet\"))\n    new_row_feat_index._feature_arr = [pd.read_parquet(csv_path) for csv_path in parquet_data_paths]\n    new_row_feat_index._cumulative_sum_index = np.load(Path(datapath) / \"cumulative_sum_index.npy\")\n    new_row_feat_index._labels = np.load(Path(datapath) / \"labels.npy\", allow_pickle=True)\n    new_row_feat_index._version = np.load(Path(datapath) / \"version.npy\").item()\n    return new_row_feat_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.lookup","title":"<code>lookup(row, select_features=None)</code>","text":"<p>Find the features at a given row.</p> <p>It is assumed that the row is non-zero._cumulative_sum_index contains pointers to which rows correspond to given dataframes. To obtain a specific row, we determine where it is located in _cumulative_sum_index and then look up that dataframe in _feature_arr Args:     row (int): The row in the feature index.     select_features (List[str]): a list of features to select Returns     pd.DataFrame: dataframe of features in that row     str: optional label for the row Raises:     IndexError: An error occured due to input row being negative or it     exceeding the larger row of the rows in the index. It is also raised     if there are no entries in the index yet.</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def lookup(self, row: int, select_features: Optional[List[str]] = None) -&gt; Tuple[pd.DataFrame, str]:\n    \"\"\"Find the features at a given row.\n\n    It is assumed that the row is\n    non-zero._cumulative_sum_index contains pointers to which rows correspond\n    to given dataframes. To obtain a specific row, we determine where it is\n    located in _cumulative_sum_index and then look up that dataframe in\n    _feature_arr\n    Args:\n        row (int): The row in the feature index.\n        select_features (List[str]): a list of features to select\n    Returns\n        pd.DataFrame: dataframe of features in that row\n        str: optional label for the row\n    Raises:\n        IndexError: An error occured due to input row being negative or it\n        exceeding the larger row of the rows in the index. It is also raised\n        if there are no entries in the index yet.\n    \"\"\"\n    if row &lt; 0:\n        raise IndexError(f\"Row index {row} is not valid. It must be non-negative.\")\n    if len(self._cumulative_sum_index) &lt; 2:\n        raise IndexError(\"There are no dataframes to lookup.\")\n\n    if row &gt; self._cumulative_sum_index[-1]:\n        raise IndexError(\n            f\"Row index {row} is larger than number of rows in FeatureIndex ({self._cumulative_sum_index[-1]}).\"\n        )\n    # This line does the following:\n    # creates a mask for values where cumulative sum &gt; row\n    mask = ~(self._cumulative_sum_index &gt; row)\n    # Sum these to get the index of the first range &gt; row\n    # Subtract one to get the range containing row.\n    d_id = sum(mask) - 1\n\n    # Retrieve the features for the identified value.\n    features = self._feature_arr[d_id]\n\n    # If specific features are to be selected, filter the features.\n    if select_features is not None:\n        features = features[select_features]\n\n    # Return the features for the identified range.\n    return features, self._labels[d_id]\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataframe.</p> <p>Returns:</p> Type Description <code>int</code> <p>An integer corresponding to the number or rows in the index</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataframe.\n\n    Returns:\n        An integer corresponding to the number or rows in the index\n    \"\"\"\n    return int(max(self._cumulative_sum_index[-1], 0))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Get the total number of values in the array.</p> <p>For each row, the length of the corresponding dataframe is counted.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list containing the lengths of the features in every block of rows</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_of_values(self) -&gt; List[int]:\n    \"\"\"Get the total number of values in the array.\n\n    For each row, the length of the corresponding dataframe is counted.\n\n    Returns:\n        A list containing the lengths of the features in every block of rows\n    \"\"\"\n    if len(self._feature_arr) == 0:\n        return [0]\n    rows = [\n        self._cumulative_sum_index[i] - max(self._cumulative_sum_index[i - 1], 0)\n        for i in range(1, len(self._cumulative_sum_index))\n    ]\n\n    vals = [n_rows * len(self._feature_arr[i]) for i, n_rows in enumerate(rows)]\n    return vals\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.number_vars_at_row","title":"<code>number_vars_at_row(row)</code>","text":"<p>Return number of variables (legnth of the dataframe) in a given row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>int</code> <p>The row in the feature index.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The length of the features at the row</p> Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def number_vars_at_row(self, row: int) -&gt; int:\n    \"\"\"Return number of variables (legnth of the dataframe) in a given row.\n\n    Args:\n        row (int): The row in the feature index.\n\n    Returns:\n        The length of the features at the row\n    \"\"\"\n    feats, _ = self.lookup(row=row)\n    return len(feats)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.save","title":"<code>save(datapath)</code>","text":"<p>Saves the RowFeatureIndex to a given path.</p> <p>Parameters:</p> Name Type Description Default <code>datapath</code> <code>str</code> <p>path to save the index</p> required Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def save(self, datapath: str) -&gt; None:\n    \"\"\"Saves the RowFeatureIndex to a given path.\n\n    Args:\n        datapath: path to save the index\n    \"\"\"\n    Path(datapath).mkdir(parents=True, exist_ok=True)\n    num_digits = len(str(len(self._feature_arr)))\n\n    for dataframe_index, dataframe in enumerate(self._feature_arr):\n        dataframe_str_index = f\"{dataframe_index:0{num_digits}d}\"\n        dataframe.to_parquet(f\"{datapath}/dataframe_{dataframe_str_index}.parquet\", index=False)\n    np.save(Path(datapath) / \"cumulative_sum_index.npy\", self._cumulative_sum_index)\n    np.save(Path(datapath) / \"labels.npy\", self._labels)\n    np.save(Path(datapath) / \"version.npy\", np.array(self._version))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/index/row_feature_index/#bionemo.scdl.index.row_feature_index.RowFeatureIndex.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/index/row_feature_index.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/","title":"Single cell collection","text":""},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Names of files that are generated in SingleCellCollection.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n\n    VERSION = \"version.json\"\n    METADATA = \"metadata.json\"\n    FEATURES = \"features\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection","title":"<code>SingleCellCollection</code>","text":"<p>               Bases: <code>SingleCellRowDatasetCore</code></p> <p>A collection of one or more SingleCellMemMapDatasets.</p> <p>SingleCellCollection support most of the functionality of the SingleCellDataSet API. An SingleCellCollection can be converted to a single SingleCellMemMapDataset. A SingleCellCollection enables the use of heterogeneous datasets, such as those composed of many AnnData files.</p> <p>Attributes:</p> Name Type Description <code>_version</code> <code>str</code> <p>The version of the dataset</p> <code>data_path</code> <code>str</code> <p>The directory where the colleection of datasets is stored.</p> <code>_feature_index</code> <code>RowFeatureIndex</code> <p>The corresponding RowFeatureIndex where features are</p> <code>fname_to_mmap</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>dictionary to hold each SingleCellMemMapDataset object.</p> <code>False</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>not ragged; all SingleCellMemMapDataset have same column dimemsion</p> <code>True</code> <code>Dict[str, SingleCellMemMapDataset]</code> <p>ragged; scmmap column dimemsions vary</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>class SingleCellCollection(SingleCellRowDatasetCore):\n    \"\"\"A collection of one or more SingleCellMemMapDatasets.\n\n    SingleCellCollection support most of the functionality of the\n    SingleCellDataSet API. An SingleCellCollection can be converted\n    to a single SingleCellMemMapDataset. A SingleCellCollection\n    enables the use of heterogeneous datasets, such as those composed of many\n    AnnData files.\n\n    Attributes:\n        _version: The version of the dataset\n        data_path: The directory where the colleection of datasets is stored.\n        _feature_index: The corresponding RowFeatureIndex where features are\n        stored.\n        fname_to_mmap:  dictionary to hold each SingleCellMemMapDataset object.\n        This maps from the path to the dataset.\n        ragged dataset is an dataset of arrays where the arrays have different\n        lengths\n        False: not ragged; all SingleCellMemMapDataset have same column dimemsion\n        True: ragged; scmmap column dimemsions vary\n    \"\"\"\n\n    def __init__(self, data_path: str) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: Where the class will be stored.\n        \"\"\"\n        self.data_path: str = data_path\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.metadata: Dict[str, int] = {}\n        self._feature_index: RowFeatureIndex = RowFeatureIndex()\n        self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n\n        Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def load_h5ad(self, h5ad_path: str) -&gt; None:\n        \"\"\"Loads data from an existing AnnData archive.\n\n        This creates and saves a new backing data structure.\n        Then, the location and the data and the dataset are stored.\n\n        Args:\n            h5ad_path: the path to AnnData archive\n        \"\"\"\n        mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n        self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n            h5ad_path=h5ad_path, base_directory_path=self.data_path\n        )\n        self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n\n    def load_h5ad_multi(self, directory_path: str, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n        \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n        Args:\n            directory_path: The path to the directory with the AnnData files\n            max_workers: the maximal number of workers to use\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use\n                ThreadPoolExecutor\n        Raises:\n            FileNotFoundError: If no h5ad files are found in the directory.\n            RuntimeError: If an error occurs in the loading of any of the h5ad files.\n        \"\"\"\n        directory_path = Path(directory_path)\n        ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n        if len(ann_data_paths) == 0:\n            raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n        mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n        queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n        for ann in ann_data_paths:\n            queue.submit_task(_create_single_cell_memmap_dataset_from_h5ad, ann, base_directory_path=self.data_path)\n        queue.wait()\n        mmaps = queue.get_task_results()\n\n        for result in mmaps:\n            if isinstance(result, Exception):\n                raise RuntimeError(f\"Error in processing file {ann}: {result}\") from result\n\n        for mmap_path, mmap in zip(mmap_paths, mmaps):\n            if isinstance(mmap, Exception):\n                raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n            self.fname_to_mmap[mmap_path] = mmap\n            self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Sum of the number of values in each dataset.\"\"\"\n        return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        row_sum_from_datasets = sum(\n            [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n        )\n        if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != row_sum_from_datasets:\n            raise ValueError(\n                f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n            )\n\n        return row_sum_from_datasets\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"If ragged, returns a list of variable lengths.\n\n        If not ragged, returns a list with one entry. A ragged\n        collection is one where the datasets have different lengths.\n        \"\"\"\n        if len(self._feature_index) == 0:\n            return [0]\n        else:\n            num_vars = self._feature_index.column_dims()\n            return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The total number of elements across dataset\n            A list containing the number of variables for each entry in the\n                RowFeatureIndex.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def flatten(\n        self,\n        output_path: str,\n        destroy_on_copy: bool = False,\n    ) -&gt; None:\n        \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n        Args:\n            output_path: location to store new dataset\n            destroy_on_copy: Whether to remove the current data_path\n        \"\"\"\n        output = SingleCellMemMapDataset(\n            output_path,\n            num_elements=self.number_of_rows(),\n            num_rows=self.number_nonzero_values(),\n            mode=Mode.CREATE_APPEND,\n        )\n\n        output.concat(list(self.fname_to_mmap.values()))\n\n        # Hit save!\n        output.save()\n\n        if destroy_on_copy:\n            shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.__init__","title":"<code>__init__(data_path)</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Where the class will be stored.</p> required Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def __init__(self, data_path: str) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: Where the class will be stored.\n    \"\"\"\n    self.data_path: str = data_path\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.metadata: Dict[str, int] = {}\n    self._feature_index: RowFeatureIndex = RowFeatureIndex()\n    self.fname_to_mmap: Dict[str, SingleCellMemMapDataset] = {}\n\n    Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.flatten","title":"<code>flatten(output_path, destroy_on_copy=False)</code>","text":"<p>Flattens the collection into a single SingleCellMemMapDataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>location to store new dataset</p> required <code>destroy_on_copy</code> <code>bool</code> <p>Whether to remove the current data_path</p> <code>False</code> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def flatten(\n    self,\n    output_path: str,\n    destroy_on_copy: bool = False,\n) -&gt; None:\n    \"\"\"Flattens the collection into a single SingleCellMemMapDataset.\n\n    Args:\n        output_path: location to store new dataset\n        destroy_on_copy: Whether to remove the current data_path\n    \"\"\"\n    output = SingleCellMemMapDataset(\n        output_path,\n        num_elements=self.number_of_rows(),\n        num_rows=self.number_nonzero_values(),\n        mode=Mode.CREATE_APPEND,\n    )\n\n    output.concat(list(self.fname_to_mmap.values()))\n\n    # Hit save!\n    output.save()\n\n    if destroy_on_copy:\n        shutil.rmtree(self.data_path)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad","title":"<code>load_h5ad(h5ad_path)</code>","text":"<p>Loads data from an existing AnnData archive.</p> <p>This creates and saves a new backing data structure. Then, the location and the data and the dataset are stored.</p> <p>Parameters:</p> Name Type Description Default <code>h5ad_path</code> <code>str</code> <p>the path to AnnData archive</p> required Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad(self, h5ad_path: str) -&gt; None:\n    \"\"\"Loads data from an existing AnnData archive.\n\n    This creates and saves a new backing data structure.\n    Then, the location and the data and the dataset are stored.\n\n    Args:\n        h5ad_path: the path to AnnData archive\n    \"\"\"\n    mmap_path = Path(self.data_path) / Path(h5ad_path).stem\n    self.fname_to_mmap[mmap_path] = _create_single_cell_memmap_dataset_from_h5ad(\n        h5ad_path=h5ad_path, base_directory_path=self.data_path\n    )\n    self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.load_h5ad_multi","title":"<code>load_h5ad_multi(directory_path, max_workers=5, use_processes=False)</code>","text":"<p>Loads one or more AnnData files and adds them to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path to the directory with the AnnData files</p> required <code>max_workers</code> <code>int</code> <p>the maximal number of workers to use</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor</p> <code>False</code> <p>Raises:     FileNotFoundError: If no h5ad files are found in the directory.     RuntimeError: If an error occurs in the loading of any of the h5ad files.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def load_h5ad_multi(self, directory_path: str, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n    \"\"\"Loads one or more AnnData files and adds them to the collection.\n\n    Args:\n        directory_path: The path to the directory with the AnnData files\n        max_workers: the maximal number of workers to use\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use\n            ThreadPoolExecutor\n    Raises:\n        FileNotFoundError: If no h5ad files are found in the directory.\n        RuntimeError: If an error occurs in the loading of any of the h5ad files.\n    \"\"\"\n    directory_path = Path(directory_path)\n    ann_data_paths = sorted(directory_path.rglob(\"*.h5ad\"))\n    if len(ann_data_paths) == 0:\n        raise FileNotFoundError(f\"There a no h5ad files in {directory_path}.\")\n    mmap_paths = [Path(self.data_path) / Path(ann_datapath).stem for ann_datapath in ann_data_paths]\n    queue = AsyncWorkQueue(max_workers=max_workers, use_processes=use_processes)\n    for ann in ann_data_paths:\n        queue.submit_task(_create_single_cell_memmap_dataset_from_h5ad, ann, base_directory_path=self.data_path)\n    queue.wait()\n    mmaps = queue.get_task_results()\n\n    for result in mmaps:\n        if isinstance(result, Exception):\n            raise RuntimeError(f\"Error in processing file {ann}: {result}\") from result\n\n    for mmap_path, mmap in zip(mmap_paths, mmaps):\n        if isinstance(mmap, Exception):\n            raise RuntimeError(f\"Error in processing file {mmap_path}: {mmap}\") from mmap\n\n        self.fname_to_mmap[mmap_path] = mmap\n        self._feature_index.concat(self.fname_to_mmap[mmap_path]._feature_index)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Sum of the number of non zero entries in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Sum of the number of non zero entries in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_nonzero_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    row_sum_from_datasets = sum(\n        [self.fname_to_mmap[mmap_path].number_of_rows() for mmap_path in self.fname_to_mmap]\n    )\n    if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != row_sum_from_datasets:\n        raise ValueError(\n            f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the datasets {row_sum_from_datasets}\"\"\"\n        )\n\n    return row_sum_from_datasets\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Sum of the number of values in each dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Sum of the number of values in each dataset.\"\"\"\n    return sum([self.fname_to_mmap[mmap_path].number_of_values() for mmap_path in self.fname_to_mmap])\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>If ragged, returns a list of variable lengths.</p> <p>If not ragged, returns a list with one entry. A ragged collection is one where the datasets have different lengths.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"If ragged, returns a list of variable lengths.\n\n    If not ragged, returns a list with one entry. A ragged\n    collection is one where the datasets have different lengths.\n    \"\"\"\n    if len(self._feature_index) == 0:\n        return [0]\n    else:\n        num_vars = self._feature_index.column_dims()\n        return num_vars\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The total number of elements across dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each entry in the RowFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The total number of elements across dataset\n        A list containing the number of variables for each entry in the\n            RowFeatureIndex.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_collection/#bionemo.scdl.io.single_cell_collection.SingleCellCollection.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_collection.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/","title":"Single cell memmap dataset","text":""},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.FileNames","title":"<code>FileNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Names of files that are generated in SingleCellCollection.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class FileNames(str, Enum):\n    \"\"\"Names of files that are generated in SingleCellCollection.\"\"\"\n\n    DATA = \"data.npy\"\n    COLPTR = \"col_ptr.npy\"\n    ROWPTR = \"row_ptr.npy\"\n    METADATA = \"metadata.json\"\n    DTYPE = \"dtypes.json\"\n    FEATURES = \"features\"\n    VERSION = \"version.json\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.METADATA","title":"<code>METADATA</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Stored metadata.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class METADATA(str, Enum):\n    \"\"\"Stored metadata.\"\"\"\n\n    NUM_ROWS = \"num_rows\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid modes for the single cell memory mapped dataset.</p> <p>The write append mode is 'w+' while the read append mode is 'r+'.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class Mode(str, Enum):\n    \"\"\"Valid modes for the single cell memory mapped dataset.\n\n    The write append mode is 'w+' while the read append mode is 'r+'.\n    \"\"\"\n\n    CREATE_APPEND = \"w+\"\n    READ_APPEND = \"r+\"\n    READ = \"r\"\n    CREATE = \"w\"\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset","title":"<code>SingleCellMemMapDataset</code>","text":"<p>               Bases: <code>SingleCellRowDataset</code></p> <p>Represents one or more AnnData matrices.</p> <p>Data is stored in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system. SCMMAP implements a consistent API defined in SingleCellRowDataset.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Location of np.memmap files to be loaded from or that will be</p> <code>mode</code> <code>Mode</code> <p>Whether the dataset will be read in (r+) from np.memmap files or</p> <code>data</code> <code>Optional[ndarray]</code> <p>A numpy array of the data</p> <code>row_index</code> <code>Optional[ndarray]</code> <p>A numpy array of row pointers</p> <code>col_index</code> <code>Optional[ndarray]</code> <p>A numpy array of column values</p> <code>metadata</code> <code>Dict[str, int]</code> <p>Various metata about the dataset.</p> <code>_feature_index</code> <code>RowFeatureIndex</code> <p>The corresponding RowFeatureIndex where features are</p> <code>dtypes</code> <code>Dict[FileNames, str]</code> <p>A dictionary containing the datatypes of the data, row_index,</p> <code>_version</code> <code>str</code> <p>The version of the dataset</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>class SingleCellMemMapDataset(SingleCellRowDataset):\n    \"\"\"Represents one or more AnnData matrices.\n\n    Data is stored in large, memory-mapped arrays that enables fast access of\n    datasets larger than the available amount of RAM on a system. SCMMAP\n    implements a consistent API defined in SingleCellRowDataset.\n\n    Attributes:\n        data_path: Location of np.memmap files to be loaded from or that will be\n        created.\n        mode: Whether the dataset will be read in (r+) from np.memmap files or\n        written to np.memmap files (w+).\n        data: A numpy array of the data\n        row_index: A numpy array of row pointers\n        col_index: A numpy array of column values\n        metadata: Various metata about the dataset.\n        _feature_index: The corresponding RowFeatureIndex where features are\n        stored\n        dtypes: A dictionary containing the datatypes of the data, row_index,\n        and col_index arrays.\n        _version: The version of the dataset\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        h5ad_path: Optional[str] = None,\n        num_elements: Optional[int] = None,\n        num_rows: Optional[int] = None,\n        mode: Mode = Mode.READ_APPEND,\n        paginated_load_cutoff: int = 10_000,\n        load_block_row_size: int = 1_000_000,\n    ) -&gt; None:\n        \"\"\"Instantiate the class.\n\n        Args:\n            data_path: The location where the data np.memmap files are read from\n            or stored.\n            h5ad_path: Optional, the location of the h5_ad path.\n            num_elements: The total number of elements in the array.\n            num_rows: The number of rows in the data frame.\n            mode: Whether to read or write from the data_path.\n            paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n            load_block_row_size: Number of rows to load into memory with paginated load\n        \"\"\"\n        self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n        self.data_path: str = data_path\n        self.mode: Mode = mode\n        self.paginated_load_cutoff = paginated_load_cutoff\n        self.load_block_row_size = load_block_row_size\n        # Backing arrays\n        self.data: Optional[np.ndarray] = None\n        self.row_index: Optional[np.ndarray] = None\n        self.row_index: Optional[np.ndarray] = None\n\n        # Metadata and attributes\n        self.metadata: Dict[str, int] = {}\n\n        # Stores the Feature Index, which tracks\n        # the original AnnData features (e.g., gene names)\n        # and allows us to store ragged arrays in our SCMMAP structure.\n        self._feature_index: RowFeatureIndex = RowFeatureIndex()\n\n        # Variables for int packing / reduced precision\n        self.dtypes: Dict[FileNames, str] = {\n            f\"{FileNames.DATA.value}\": \"float32\",\n            f\"{FileNames.COLPTR.value}\": \"uint32\",\n            f\"{FileNames.ROWPTR.value}\": \"uint64\",\n        }\n\n        if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n            raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n        if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n            raise FileExistsError(\n                \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n                \"Please pass either an existing SCMMAP or an h5ad file.\"\n            )\n\n        # If there is only a data path, and it exists already, load SCMMAP data.\n        elif data_path is not None and os.path.exists(data_path):\n            self.__init__obj()\n            self.load(data_path)\n\n        # If there is only an h5ad path, load the HDF5 data\n        elif h5ad_path is not None:\n            self.__init__obj()\n            self.load_h5ad(h5ad_path)\n        else:\n            match num_rows, num_elements:\n                case (int(), int()):\n                    self.__init__obj()\n                    self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n                case _:\n                    raise ValueError(\n                        \"An np.memmap path, an h5ad path, or the number of elements and rows is required\" \"\"\n                    )\n\n    def __init__obj(self):\n        \"\"\"Initializes the datapath and writes the version.\"\"\"\n        os.makedirs(self.data_path, exist_ok=True)\n\n        # Write the version\n        if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n            with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n                json.dump(self.version(), vfi)\n\n    def _init_arrs(self, num_elements: int, num_rows: int) -&gt; None:\n        self.mode = Mode.CREATE_APPEND\n        data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n            num_elements=num_elements,\n            num_rows=num_rows,\n            memmap_dir_path=Path(self.data_path),\n            mode=self.mode,\n            dtypes=self.dtypes,\n        )\n        self.data = data_arr\n        self.col_index = col_arr\n        self.row_index = row_arr\n\n    def version(self) -&gt; str:\n        \"\"\"Returns a version number.\n\n        (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n        \"\"\"\n        return self._version\n\n    def get_row(\n        self,\n        index: int,\n        return_features: bool = False,\n        feature_vars: Optional[List[str]] = None,\n    ) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], pd.DataFrame]:\n        \"\"\"Returns a given row in the dataset along with optional features.\n\n        Args:\n            index: The row to be returned. This is in the range of [0, num_rows)\n            return_features: boolean that indicates whether to return features\n            feature_vars: Optional, feature variables to extract\n        Return:\n            [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n            pd.DataFrame: optional, corresponding features.\n        \"\"\"\n        start = self.row_index[index]\n        end = self.row_index[index + 1]\n        values = self.data[start:end]\n        columns = self.col_index[start:end]\n        ret = (values, columns)\n        if return_features:\n            return ret, self._feature_index.lookup(index, select_features=feature_vars)[0]\n        else:\n            return ret, None\n\n    def get_row_padded(\n        self,\n        index: int,\n        return_features: bool = False,\n        feature_vars: Optional[List[str]] = None,\n    ) -&gt; Tuple[np.ndarray, pd.DataFrame]:\n        \"\"\"Returns a padded version of a row in the dataset.\n\n        A padded version is one where the a sparse array representation is\n        converted to a conventional represenentation. Optionally, features are\n        returned.\n\n        Args:\n            index: The row to be returned\n            return_features: boolean that indicates whether to return features\n            feature_vars: Optional, feature variables to extract\n        Return:\n            np.ndarray: conventional row representation\n            pd.DataFrame: optional, corresponding features.\n        \"\"\"\n        (row_values, row_column_pointer), features = self.get_row(index, return_features, feature_vars)\n        return (\n            _pad_sparse_array(row_values, row_column_pointer, self._feature_index.number_vars_at_row(index)),\n            features,\n        )\n\n    def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n        \"\"\"Returns the value at a given index and the corresponding column.\n\n        Args:\n            index: The index to be returned\n            column: The column to be returned\n            impute_missing_zeros: boolean that indicates whether to set missing\n            data to 0\n        Return:\n            A float that is the value in the array or None.\n        \"\"\"\n        (row_values, row_column_pointer), _ = self.get_row(index)\n        if column is not None:\n            for col_index, col in enumerate(row_column_pointer):\n                if col == column:\n                    # return the value at this position\n                    return row_values[col_index]\n                elif col &gt; column:\n                    try:\n                        raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                    except ValueError:\n                        break\n            return 0.0 if impute_missing_zeros else None\n\n    def features(self) -&gt; Optional[RowFeatureIndex]:\n        \"\"\"Return the corresponding RowFeatureIndex.\"\"\"\n        return self._feature_index\n\n    def _load_mmap_file_if_exists(self, file_path, dtype):\n        if os.path.exists(file_path):\n            return np.memmap(file_path, dtype=dtype, mode=self.mode)\n        else:\n            raise FileNotFoundError(f\"The mmap file at {file_path} is missing\")\n\n    def load(self, stored_path: str) -&gt; None:\n        \"\"\"Loads the data at store_path that is an np.memmap format.\n\n        Args:\n            stored_path: directory with np.memmap files\n        Raises:\n            FileNotFoundError if the corresponding directory or files are not\n            found, or if the metadata file is not present.\n        \"\"\"\n        if not os.path.exists(stored_path):\n            raise FileNotFoundError(\n                f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                    Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                    be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                    with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n            )\n        self.data_path = stored_path\n        self.mode = Mode.READ_APPEND\n\n        # Metadata is required, so we must check if it exists and fail if not.\n        if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n            raise FileNotFoundError(\n                f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n            )\n\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n            self.metadata = json.load(mfi)\n\n        if os.path.exists(f\"{self.data_path}/{FileNames.FEATURES.value}\"):\n            self._feature_index = RowFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n        if os.path.exists(f\"{self.data_path}/{FileNames.DTYPE.value}\"):\n            with open(f\"{self.data_path}/{FileNames.DTYPE.value}\") as dfi:\n                self.dtypes = json.load(dfi)\n\n        # mmap the existing arrays\n        self.data = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n        )\n        self.row_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n        )\n        self.col_index = self._load_mmap_file_if_exists(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n        )\n\n    def _write_metadata(self) -&gt; None:\n        with open(f\"{self.data_path}/{FileNames.METADATA.value}\", f\"{Mode.CREATE.value}\") as mfi:\n            json.dump(self.metadata, mfi)\n\n    def regular_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            NotImplementedError if the data is not in scipy.sparse.spmatrix format\n            ValueError it there is not count data\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path)  # slow\n\n        if not isinstance(adata.X, scipy.sparse.spmatrix):\n            raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n        # Check if raw data is present\n        raw = getattr(adata, \"raw\", None)\n        count_data = None\n        if raw is not None:\n            # If it is, attempt to get the counts in the raw data.\n            count_data = getattr(raw, \"X\", None)\n\n        if count_data is None:\n            # No raw counts were present, resort to normalized\n            count_data = getattr(adata, \"X\")\n        if count_data is None:\n            raise ValueError(\"This file does not have count data\")\n\n        shape = count_data.shape\n        num_rows = shape[0]\n\n        num_elements_stored = count_data.nnz\n\n        self.dtypes[f\"{FileNames.DATA.value}\"] = count_data.dtype\n\n        # Create the arrays.\n        self._init_arrs(num_elements_stored, num_rows)\n        # Store data\n        self.data[0:num_elements_stored] = count_data.data\n\n        # Store the col idx array\n        self.col_index[0:num_elements_stored] = count_data.indices.astype(int)\n\n        # Store the row idx array\n        self.row_index[0 : num_rows + 1] = count_data.indptr.astype(int)\n\n        return adata.var, num_rows\n\n    def paginated_load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; Tuple[pd.DataFrame, int]:\n        \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n        This should be used in the case when the entire anndata file cannot be loaded into memory.\n        The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n        is converted into numpy memory maps which are then concatenated together.\n\n        Raises:\n            NotImplementedError if the data is not loaded in the CSRDataset format.\n\n        Returns:\n            pd.DataFrame: var variables for features\n            int: number of rows in the dataframe.\n        \"\"\"\n        adata = ad.read_h5ad(anndata_path, backed=True)\n\n        if not isinstance(adata.X, ad.experimental.CSRDataset):\n            raise NotImplementedError(\"Non-sparse format cannot be loaded: {type(adata.X)}.\")\n        num_rows = adata.X.shape[0]\n\n        self.dtypes[f\"{FileNames.DATA.value}\"] = adata.X.dtype\n\n        # Read the row indices into a memory map.\n        mode = Mode.CREATE_APPEND\n        self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n        self.row_index[:] = adata.X._indptr.astype(int)\n\n        # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n        # location of the memmap file. In this step, it is saved in the binary file format.\n        memmap_dir_path = Path(self.data_path)\n        with (\n            open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n            open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n        ):\n            n_elements = 0\n            for row_start in range(0, num_rows, self.load_block_row_size):\n                # Write each array's data to the file in binary format\n                col_block = adata.X[row_start : row_start + self.load_block_row_size].indices\n                col_file.write(col_block.tobytes())\n\n                data_block = adata.X[row_start : row_start + self.load_block_row_size].data\n                data_file.write(data_block.tobytes())\n\n                n_elements += len(data_block)\n\n        # The column and data files are re-opened as memory-mapped arrays with the final shape\n        mode = Mode.READ_APPEND\n        self.col_index = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n            self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        self.data = np.memmap(\n            f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            mode=mode,\n            shape=(n_elements,),\n        )\n        return adata.var, num_rows\n\n    def load_h5ad(\n        self,\n        anndata_path: str,\n    ) -&gt; None:\n        \"\"\"Loads an existing AnnData archive from disk.\n\n        This creates a new backing data structure which is saved.\n        Note: the storage utilized will roughly double. Currently, the data must\n        be in a scipy.sparse.spmatrix format.\n\n        Args:\n            anndata_path: location of data to load\n        Raises:\n            FileNotFoundError if the data path does not exist.\n            NotImplementedError if the data is not in scipy.sparse.spmatrix\n            format\n            ValueError it there is not count data\n        \"\"\"\n        if not os.path.exists(anndata_path):\n            raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n        file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n        if file_size_MB &lt; self.paginated_load_cutoff:\n            features, num_rows = self.regular_load_h5ad(anndata_path)\n\n        else:\n            features, num_rows = self.paginated_load_h5ad(anndata_path)\n\n        # Collect features and store in FeatureIndex\n        self._feature_index.append_features(n_obs=num_rows, features=features, label=anndata_path)\n\n        self.save()\n\n    def save(self, output_path: Optional[str] = None) -&gt; None:\n        \"\"\"Saves the class to a given output path.\n\n        Args:\n            output_path: The location to save - not yet implemented and should\n            be self.data_path\n\n        Raises:\n           NotImplementedError if output_path is not None.\n        \"\"\"\n        if f\"{METADATA.NUM_ROWS.value}\" not in self.metadata:\n            self.metadata[f\"{METADATA.NUM_ROWS.value}\"] = self.number_of_rows()\n\n        self._write_metadata()\n        # Write the feature index. This may not exist.\n        self._feature_index.save(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n        # Ensure the object is in a valid state. These are saved at creation!\n        for postfix in [\n            f\"{FileNames.VERSION.value}\",\n            f\"{FileNames.DATA.value}\",\n            f\"{FileNames.COLPTR.value}\",\n            f\"{FileNames.ROWPTR.value}\",\n            f\"{FileNames.FEATURES.value}\",\n        ]:\n            if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n                raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n        self.data.flush()\n        self.row_index.flush()\n        self.col_index.flush()\n\n        if output_path is not None:\n            raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n        return True\n\n    def number_of_values(self) -&gt; int:\n        \"\"\"Get the total number of values in the array.\n\n        For each index, the length of the corresponding dataframe is counted.\n\n        Returns:\n            The sum of lengths of the features in every row\n        \"\"\"\n        return sum(self._feature_index.number_of_values())\n\n    def number_of_rows(self) -&gt; int:\n        \"\"\"The number of rows in the dataset.\n\n        Returns:\n            The number of rows in the dataset\n        Raises:\n            ValueError if the length of the number of rows in the feature\n            index does not correspond to the number of stored rows.\n        \"\"\"\n        if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != self.row_index.size - 1:\n            raise ValueError(\n                f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                             does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n            )\n        return self._feature_index.number_of_rows()\n\n    def number_nonzero_values(self) -&gt; int:\n        \"\"\"Number of non zero entries in the dataset.\"\"\"\n        return self.data.size\n\n    def __len__(self):\n        \"\"\"Return the number of rows.\"\"\"\n        return self.number_of_rows()\n\n    def __getitem__(self, idx: int) -&gt; torch.Tensor:\n        \"\"\"Get the row values located and index idx.\"\"\"\n        return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n\n    def number_of_variables(self) -&gt; List[int]:\n        \"\"\"Get the number of features in every entry in the dataset.\n\n        Returns:\n            A list containing the lengths of the features in every row\n        \"\"\"\n        feats = self._feature_index\n        if len(feats) == 0:\n            return [0]\n        num_vars = feats.column_dims()\n        return num_vars\n\n    def shape(self) -&gt; Tuple[int, List[int]]:\n        \"\"\"Get the shape of the dataset.\n\n        This is the number of entries by the the length of the feature index\n        corresponding to that variable.\n\n        Returns:\n            The number of elements in the dataset\n            A list containing the number of variables for each row.\n        \"\"\"\n        return self.number_of_rows(), self.number_of_variables()\n\n    def concat(\n        self,\n        other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n    ) -&gt; None:\n        \"\"\"Concatenates another SingleCellMemMapDataset to the existing one.\n\n        The data is stored in the same place as for the original data set. This\n        necessitates using _swap_memmap_array.\n\n        Args:\n            other_dataset: A SingleCellMemMapDataset or a list of\n            SingleCellMemMapDatasets\n\n        Raises:\n           ValueError if the other dataset(s) are not of the same version or\n           something of another type is passed in.\n        \"\"\"\n        # Verify the other dataset or datasets are of the same type.\n        match other_dataset:\n            case self.__class__():\n                other_dataset = [other_dataset]\n            case list():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n                )\n\n        for dataset in other_dataset:\n            if self.version() != dataset.version():\n                raise ValueError(\n                    f\"\"\"Incompatable versions: input version: {dataset.version()},\n            this version:  {self.version}\"\"\"\n                )\n\n        # Set our mode:\n        self.mode: Mode = Mode.READ_APPEND\n\n        mmaps = []\n        mmaps.extend(other_dataset)\n        # Calculate the size of our new dataset arrays\n        total_num_elements = (self.number_nonzero_values() if self.number_of_rows() &gt; 0 else 0) + sum(\n            [m.number_nonzero_values() for m in mmaps]\n        )\n        total_num_rows = self.number_of_rows() + sum([m.number_of_rows() for m in mmaps])\n\n        # Create new arrays to store the data, colptr, and rowptr.\n        with tempfile.TemporaryDirectory(prefix=\"_tmp\", dir=self.data_path) as tmp:\n            data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n                num_elements=total_num_elements,\n                num_rows=total_num_rows,\n                memmap_dir_path=Path(tmp),\n                mode=Mode.CREATE_APPEND,\n                dtypes=self.dtypes,\n            )\n            # Copy the data from self and other into the new arrays.\n            cumulative_elements = 0\n            cumulative_rows = 0\n            if self.number_of_rows() &gt; 0:\n                data_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.data.data\n                col_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.col_index.data\n                row_arr[cumulative_rows : cumulative_rows + self.number_of_rows() + 1] = self.row_index.data\n                cumulative_elements += self.number_nonzero_values()\n                cumulative_rows += self.number_of_rows()\n            for mmap in mmaps:\n                # Fill the data array for the span of this scmmap\n                data_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.data.data\n                # fill the col array for the span of this scmmap\n                col_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.col_index.data\n                # Fill the row array for the span of this scmmap\n                row_arr[cumulative_rows : cumulative_rows + mmap.number_of_rows() + 1] = (\n                    mmap.row_index + int(cumulative_elements)\n                ).data\n\n                self._feature_index.concat(mmap._feature_index)\n                # Update counters\n                cumulative_elements += mmap.number_nonzero_values()\n                cumulative_rows += mmap.number_of_rows()\n            # The arrays are swapped to ensure that the data remains stored at self.data_path and\n            # not at a temporary filepath.\n            _swap_mmap_array(\n                data_arr,\n                f\"{tmp}/{FileNames.DATA.value}\",\n                self.data,\n                f\"{self.data_path}/{FileNames.DATA.value}\",\n                destroy_src=True,\n            )\n            _swap_mmap_array(\n                col_arr,\n                f\"{tmp}/{FileNames.COLPTR.value}\",\n                self.col_index,\n                f\"{self.data_path}/{FileNames.COLPTR.value}\",\n                destroy_src=True,\n            )\n            _swap_mmap_array(\n                row_arr,\n                f\"{tmp}/{FileNames.ROWPTR.value}\",\n                self.row_index,\n                f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n                destroy_src=True,\n            )\n            # Reopen the data, colptr, and rowptr arrays\n            self.data = np.memmap(\n                f\"{self.data_path}/{FileNames.DATA.value}\",\n                dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n                shape=(cumulative_elements,),\n                mode=Mode.READ_APPEND.value,\n            )\n            self.row_index = np.memmap(\n                f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n                dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n                shape=(cumulative_rows + 1,),\n                mode=Mode.READ_APPEND.value,\n            )\n            self.col_index = np.memmap(\n                f\"{self.data_path}/{FileNames.COLPTR.value}\",\n                dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n                shape=(cumulative_elements,),\n                mode=Mode.READ_APPEND.value,\n            )\n\n        self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get the row values located and index idx.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; torch.Tensor:\n    \"\"\"Get the row values located and index idx.\"\"\"\n    return torch.from_numpy(np.stack(self.get_row(idx)[0]))\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__","title":"<code>__init__(data_path, h5ad_path=None, num_elements=None, num_rows=None, mode=Mode.READ_APPEND, paginated_load_cutoff=10000, load_block_row_size=1000000)</code>","text":"<p>Instantiate the class.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The location where the data np.memmap files are read from</p> required <code>h5ad_path</code> <code>Optional[str]</code> <p>Optional, the location of the h5_ad path.</p> <code>None</code> <code>num_elements</code> <code>Optional[int]</code> <p>The total number of elements in the array.</p> <code>None</code> <code>num_rows</code> <code>Optional[int]</code> <p>The number of rows in the data frame.</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>Whether to read or write from the data_path.</p> <code>READ_APPEND</code> <code>paginated_load_cutoff</code> <code>int</code> <p>MB size on disk at which to load the h5ad structure with paginated load.</p> <code>10000</code> <code>load_block_row_size</code> <code>int</code> <p>Number of rows to load into memory with paginated load</p> <code>1000000</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    h5ad_path: Optional[str] = None,\n    num_elements: Optional[int] = None,\n    num_rows: Optional[int] = None,\n    mode: Mode = Mode.READ_APPEND,\n    paginated_load_cutoff: int = 10_000,\n    load_block_row_size: int = 1_000_000,\n) -&gt; None:\n    \"\"\"Instantiate the class.\n\n    Args:\n        data_path: The location where the data np.memmap files are read from\n        or stored.\n        h5ad_path: Optional, the location of the h5_ad path.\n        num_elements: The total number of elements in the array.\n        num_rows: The number of rows in the data frame.\n        mode: Whether to read or write from the data_path.\n        paginated_load_cutoff: MB size on disk at which to load the h5ad structure with paginated load.\n        load_block_row_size: Number of rows to load into memory with paginated load\n    \"\"\"\n    self._version: str = importlib.metadata.version(\"bionemo.scdl\")\n    self.data_path: str = data_path\n    self.mode: Mode = mode\n    self.paginated_load_cutoff = paginated_load_cutoff\n    self.load_block_row_size = load_block_row_size\n    # Backing arrays\n    self.data: Optional[np.ndarray] = None\n    self.row_index: Optional[np.ndarray] = None\n    self.row_index: Optional[np.ndarray] = None\n\n    # Metadata and attributes\n    self.metadata: Dict[str, int] = {}\n\n    # Stores the Feature Index, which tracks\n    # the original AnnData features (e.g., gene names)\n    # and allows us to store ragged arrays in our SCMMAP structure.\n    self._feature_index: RowFeatureIndex = RowFeatureIndex()\n\n    # Variables for int packing / reduced precision\n    self.dtypes: Dict[FileNames, str] = {\n        f\"{FileNames.DATA.value}\": \"float32\",\n        f\"{FileNames.COLPTR.value}\": \"uint32\",\n        f\"{FileNames.ROWPTR.value}\": \"uint64\",\n    }\n\n    if mode == Mode.CREATE_APPEND and os.path.exists(data_path):\n        raise FileExistsError(f\"Output directory already exists: {data_path}\")\n\n    if h5ad_path is not None and (data_path is not None and os.path.exists(data_path)):\n        raise FileExistsError(\n            \"Invalid input; both an existing SCMMAP and an h5ad file were passed. \"\n            \"Please pass either an existing SCMMAP or an h5ad file.\"\n        )\n\n    # If there is only a data path, and it exists already, load SCMMAP data.\n    elif data_path is not None and os.path.exists(data_path):\n        self.__init__obj()\n        self.load(data_path)\n\n    # If there is only an h5ad path, load the HDF5 data\n    elif h5ad_path is not None:\n        self.__init__obj()\n        self.load_h5ad(h5ad_path)\n    else:\n        match num_rows, num_elements:\n            case (int(), int()):\n                self.__init__obj()\n                self._init_arrs(num_elements=num_elements, num_rows=num_rows)\n            case _:\n                raise ValueError(\n                    \"An np.memmap path, an h5ad path, or the number of elements and rows is required\" \"\"\n                )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__init__obj","title":"<code>__init__obj()</code>","text":"<p>Initializes the datapath and writes the version.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __init__obj(self):\n    \"\"\"Initializes the datapath and writes the version.\"\"\"\n    os.makedirs(self.data_path, exist_ok=True)\n\n    # Write the version\n    if not os.path.exists(f\"{self.data_path}/{FileNames.VERSION.value}\"):\n        with open(f\"{self.data_path}/{FileNames.VERSION.value}\", \"w\") as vfi:\n            json.dump(self.version(), vfi)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of rows.\"\"\"\n    return self.number_of_rows()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.concat","title":"<code>concat(other_dataset)</code>","text":"<p>Concatenates another SingleCellMemMapDataset to the existing one.</p> <p>The data is stored in the same place as for the original data set. This necessitates using _swap_memmap_array.</p> <p>Parameters:</p> Name Type Description Default <code>other_dataset</code> <code>Union[list[SingleCellMemMapDataset], SingleCellMemMapDataset]</code> <p>A SingleCellMemMapDataset or a list of</p> required Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def concat(\n    self,\n    other_dataset: Union[list[\"SingleCellMemMapDataset\"], \"SingleCellMemMapDataset\"],\n) -&gt; None:\n    \"\"\"Concatenates another SingleCellMemMapDataset to the existing one.\n\n    The data is stored in the same place as for the original data set. This\n    necessitates using _swap_memmap_array.\n\n    Args:\n        other_dataset: A SingleCellMemMapDataset or a list of\n        SingleCellMemMapDatasets\n\n    Raises:\n       ValueError if the other dataset(s) are not of the same version or\n       something of another type is passed in.\n    \"\"\"\n    # Verify the other dataset or datasets are of the same type.\n    match other_dataset:\n        case self.__class__():\n            other_dataset = [other_dataset]\n        case list():\n            pass\n        case _:\n            raise ValueError(\n                f\"Expecting either a {SingleCellMemMapDataset} or a list thereof. Actually got: {type(other_dataset)}\"\n            )\n\n    for dataset in other_dataset:\n        if self.version() != dataset.version():\n            raise ValueError(\n                f\"\"\"Incompatable versions: input version: {dataset.version()},\n        this version:  {self.version}\"\"\"\n            )\n\n    # Set our mode:\n    self.mode: Mode = Mode.READ_APPEND\n\n    mmaps = []\n    mmaps.extend(other_dataset)\n    # Calculate the size of our new dataset arrays\n    total_num_elements = (self.number_nonzero_values() if self.number_of_rows() &gt; 0 else 0) + sum(\n        [m.number_nonzero_values() for m in mmaps]\n    )\n    total_num_rows = self.number_of_rows() + sum([m.number_of_rows() for m in mmaps])\n\n    # Create new arrays to store the data, colptr, and rowptr.\n    with tempfile.TemporaryDirectory(prefix=\"_tmp\", dir=self.data_path) as tmp:\n        data_arr, col_arr, row_arr = _create_compressed_sparse_row_memmaps(\n            num_elements=total_num_elements,\n            num_rows=total_num_rows,\n            memmap_dir_path=Path(tmp),\n            mode=Mode.CREATE_APPEND,\n            dtypes=self.dtypes,\n        )\n        # Copy the data from self and other into the new arrays.\n        cumulative_elements = 0\n        cumulative_rows = 0\n        if self.number_of_rows() &gt; 0:\n            data_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.data.data\n            col_arr[cumulative_elements : cumulative_elements + self.number_nonzero_values()] = self.col_index.data\n            row_arr[cumulative_rows : cumulative_rows + self.number_of_rows() + 1] = self.row_index.data\n            cumulative_elements += self.number_nonzero_values()\n            cumulative_rows += self.number_of_rows()\n        for mmap in mmaps:\n            # Fill the data array for the span of this scmmap\n            data_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.data.data\n            # fill the col array for the span of this scmmap\n            col_arr[cumulative_elements : cumulative_elements + mmap.number_nonzero_values()] = mmap.col_index.data\n            # Fill the row array for the span of this scmmap\n            row_arr[cumulative_rows : cumulative_rows + mmap.number_of_rows() + 1] = (\n                mmap.row_index + int(cumulative_elements)\n            ).data\n\n            self._feature_index.concat(mmap._feature_index)\n            # Update counters\n            cumulative_elements += mmap.number_nonzero_values()\n            cumulative_rows += mmap.number_of_rows()\n        # The arrays are swapped to ensure that the data remains stored at self.data_path and\n        # not at a temporary filepath.\n        _swap_mmap_array(\n            data_arr,\n            f\"{tmp}/{FileNames.DATA.value}\",\n            self.data,\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            destroy_src=True,\n        )\n        _swap_mmap_array(\n            col_arr,\n            f\"{tmp}/{FileNames.COLPTR.value}\",\n            self.col_index,\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            destroy_src=True,\n        )\n        _swap_mmap_array(\n            row_arr,\n            f\"{tmp}/{FileNames.ROWPTR.value}\",\n            self.row_index,\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            destroy_src=True,\n        )\n        # Reopen the data, colptr, and rowptr arrays\n        self.data = np.memmap(\n            f\"{self.data_path}/{FileNames.DATA.value}\",\n            dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n            shape=(cumulative_elements,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.row_index = np.memmap(\n            f\"{self.data_path}/{FileNames.ROWPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"],\n            shape=(cumulative_rows + 1,),\n            mode=Mode.READ_APPEND.value,\n        )\n        self.col_index = np.memmap(\n            f\"{self.data_path}/{FileNames.COLPTR.value}\",\n            dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"],\n            shape=(cumulative_elements,),\n            mode=Mode.READ_APPEND.value,\n        )\n\n    self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.features","title":"<code>features()</code>","text":"<p>Return the corresponding RowFeatureIndex.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def features(self) -&gt; Optional[RowFeatureIndex]:\n    \"\"\"Return the corresponding RowFeatureIndex.\"\"\"\n    return self._feature_index\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row","title":"<code>get_row(index, return_features=False, feature_vars=None)</code>","text":"<p>Returns a given row in the dataset along with optional features.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned. This is in the range of [0, num_rows)</p> required <code>return_features</code> <code>bool</code> <p>boolean that indicates whether to return features</p> <code>False</code> <code>feature_vars</code> <code>Optional[List[str]]</code> <p>Optional, feature variables to extract</p> <code>None</code> <p>Return:     [Tuple[np.ndarray, np.ndarray]: data values and column pointes     pd.DataFrame: optional, corresponding features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row(\n    self,\n    index: int,\n    return_features: bool = False,\n    feature_vars: Optional[List[str]] = None,\n) -&gt; Tuple[Tuple[np.ndarray, np.ndarray], pd.DataFrame]:\n    \"\"\"Returns a given row in the dataset along with optional features.\n\n    Args:\n        index: The row to be returned. This is in the range of [0, num_rows)\n        return_features: boolean that indicates whether to return features\n        feature_vars: Optional, feature variables to extract\n    Return:\n        [Tuple[np.ndarray, np.ndarray]: data values and column pointes\n        pd.DataFrame: optional, corresponding features.\n    \"\"\"\n    start = self.row_index[index]\n    end = self.row_index[index + 1]\n    values = self.data[start:end]\n    columns = self.col_index[start:end]\n    ret = (values, columns)\n    if return_features:\n        return ret, self._feature_index.lookup(index, select_features=feature_vars)[0]\n    else:\n        return ret, None\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_column","title":"<code>get_row_column(index, column, impute_missing_zeros=True)</code>","text":"<p>Returns the value at a given index and the corresponding column.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index to be returned</p> required <code>column</code> <code>int</code> <p>The column to be returned</p> required <code>impute_missing_zeros</code> <code>bool</code> <p>boolean that indicates whether to set missing</p> <code>True</code> <p>Return:     A float that is the value in the array or None.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_column(self, index: int, column: int, impute_missing_zeros: bool = True) -&gt; Optional[float]:\n    \"\"\"Returns the value at a given index and the corresponding column.\n\n    Args:\n        index: The index to be returned\n        column: The column to be returned\n        impute_missing_zeros: boolean that indicates whether to set missing\n        data to 0\n    Return:\n        A float that is the value in the array or None.\n    \"\"\"\n    (row_values, row_column_pointer), _ = self.get_row(index)\n    if column is not None:\n        for col_index, col in enumerate(row_column_pointer):\n            if col == column:\n                # return the value at this position\n                return row_values[col_index]\n            elif col &gt; column:\n                try:\n                    raise ValueError(f\"Column pointer {col} is larger than the column {column}.\")\n                except ValueError:\n                    break\n        return 0.0 if impute_missing_zeros else None\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.get_row_padded","title":"<code>get_row_padded(index, return_features=False, feature_vars=None)</code>","text":"<p>Returns a padded version of a row in the dataset.</p> <p>A padded version is one where the a sparse array representation is converted to a conventional represenentation. Optionally, features are returned.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The row to be returned</p> required <code>return_features</code> <code>bool</code> <p>boolean that indicates whether to return features</p> <code>False</code> <code>feature_vars</code> <code>Optional[List[str]]</code> <p>Optional, feature variables to extract</p> <code>None</code> <p>Return:     np.ndarray: conventional row representation     pd.DataFrame: optional, corresponding features.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def get_row_padded(\n    self,\n    index: int,\n    return_features: bool = False,\n    feature_vars: Optional[List[str]] = None,\n) -&gt; Tuple[np.ndarray, pd.DataFrame]:\n    \"\"\"Returns a padded version of a row in the dataset.\n\n    A padded version is one where the a sparse array representation is\n    converted to a conventional represenentation. Optionally, features are\n    returned.\n\n    Args:\n        index: The row to be returned\n        return_features: boolean that indicates whether to return features\n        feature_vars: Optional, feature variables to extract\n    Return:\n        np.ndarray: conventional row representation\n        pd.DataFrame: optional, corresponding features.\n    \"\"\"\n    (row_values, row_column_pointer), features = self.get_row(index, return_features, feature_vars)\n    return (\n        _pad_sparse_array(row_values, row_column_pointer, self._feature_index.number_vars_at_row(index)),\n        features,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load","title":"<code>load(stored_path)</code>","text":"<p>Loads the data at store_path that is an np.memmap format.</p> <p>Parameters:</p> Name Type Description Default <code>stored_path</code> <code>str</code> <p>directory with np.memmap files</p> required <p>Raises:     FileNotFoundError if the corresponding directory or files are not     found, or if the metadata file is not present.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load(self, stored_path: str) -&gt; None:\n    \"\"\"Loads the data at store_path that is an np.memmap format.\n\n    Args:\n        stored_path: directory with np.memmap files\n    Raises:\n        FileNotFoundError if the corresponding directory or files are not\n        found, or if the metadata file is not present.\n    \"\"\"\n    if not os.path.exists(stored_path):\n        raise FileNotFoundError(\n            f\"\"\"Error: the specified data path to the mmap files {stored_path} does not exist.\n                                Specify an updated filepath or provide an h5ad path to the dataset. The data can\n                                be loaded with SingleCellMemMapDataset.load_h5ad. Alternatively, the class can be instantiated\n                                with  SingleCellMemMapDataset(&lt;path to data that will be created&gt;, h5ad_path=&lt;path to h5ad file&gt;\"\"\"\n        )\n    self.data_path = stored_path\n    self.mode = Mode.READ_APPEND\n\n    # Metadata is required, so we must check if it exists and fail if not.\n    if not os.path.exists(f\"{self.data_path}/{FileNames.METADATA.value}\"):\n        raise FileNotFoundError(\n            f\"Error: the metadata file {self.data_path}/{FileNames.METADATA.value} does not exist.\"\n        )\n\n    with open(f\"{self.data_path}/{FileNames.METADATA.value}\", Mode.READ_APPEND.value) as mfi:\n        self.metadata = json.load(mfi)\n\n    if os.path.exists(f\"{self.data_path}/{FileNames.FEATURES.value}\"):\n        self._feature_index = RowFeatureIndex.load(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n    if os.path.exists(f\"{self.data_path}/{FileNames.DTYPE.value}\"):\n        with open(f\"{self.data_path}/{FileNames.DTYPE.value}\") as dfi:\n            self.dtypes = json.load(dfi)\n\n    # mmap the existing arrays\n    self.data = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.DATA.value}\", self.dtypes[f\"{FileNames.DATA.value}\"]\n    )\n    self.row_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.ROWPTR.value}\", dtype=self.dtypes[f\"{FileNames.ROWPTR.value}\"]\n    )\n    self.col_index = self._load_mmap_file_if_exists(\n        f\"{self.data_path}/{FileNames.COLPTR.value}\", dtype=self.dtypes[f\"{FileNames.COLPTR.value}\"]\n    )\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.load_h5ad","title":"<code>load_h5ad(anndata_path)</code>","text":"<p>Loads an existing AnnData archive from disk.</p> <p>This creates a new backing data structure which is saved. Note: the storage utilized will roughly double. Currently, the data must be in a scipy.sparse.spmatrix format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     FileNotFoundError if the data path does not exist.     NotImplementedError if the data is not in scipy.sparse.spmatrix     format     ValueError it there is not count data</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; None:\n    \"\"\"Loads an existing AnnData archive from disk.\n\n    This creates a new backing data structure which is saved.\n    Note: the storage utilized will roughly double. Currently, the data must\n    be in a scipy.sparse.spmatrix format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        FileNotFoundError if the data path does not exist.\n        NotImplementedError if the data is not in scipy.sparse.spmatrix\n        format\n        ValueError it there is not count data\n    \"\"\"\n    if not os.path.exists(anndata_path):\n        raise FileNotFoundError(f\"Error: could not find h5ad path {anndata_path}\")\n    file_size_MB = os.path.getsize(anndata_path) / (1_024**2)\n\n    if file_size_MB &lt; self.paginated_load_cutoff:\n        features, num_rows = self.regular_load_h5ad(anndata_path)\n\n    else:\n        features, num_rows = self.paginated_load_h5ad(anndata_path)\n\n    # Collect features and store in FeatureIndex\n    self._feature_index.append_features(n_obs=num_rows, features=features, label=anndata_path)\n\n    self.save()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_nonzero_values","title":"<code>number_nonzero_values()</code>","text":"<p>Number of non zero entries in the dataset.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_nonzero_values(self) -&gt; int:\n    \"\"\"Number of non zero entries in the dataset.\"\"\"\n    return self.data.size\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_rows","title":"<code>number_of_rows()</code>","text":"<p>The number of rows in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the dataset</p> <p>Raises:     ValueError if the length of the number of rows in the feature     index does not correspond to the number of stored rows.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_rows(self) -&gt; int:\n    \"\"\"The number of rows in the dataset.\n\n    Returns:\n        The number of rows in the dataset\n    Raises:\n        ValueError if the length of the number of rows in the feature\n        index does not correspond to the number of stored rows.\n    \"\"\"\n    if len(self._feature_index) &gt; 0 and self._feature_index.number_of_rows() != self.row_index.size - 1:\n        raise ValueError(\n            f\"\"\"The nuber of rows in the feature index {self._feature_index.number_of_rows()}\n                         does not correspond to the number of rows in the row_index {self.row_index.size - 1}\"\"\"\n        )\n    return self._feature_index.number_of_rows()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_values","title":"<code>number_of_values()</code>","text":"<p>Get the total number of values in the array.</p> <p>For each index, the length of the corresponding dataframe is counted.</p> <p>Returns:</p> Type Description <code>int</code> <p>The sum of lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_values(self) -&gt; int:\n    \"\"\"Get the total number of values in the array.\n\n    For each index, the length of the corresponding dataframe is counted.\n\n    Returns:\n        The sum of lengths of the features in every row\n    \"\"\"\n    return sum(self._feature_index.number_of_values())\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.number_of_variables","title":"<code>number_of_variables()</code>","text":"<p>Get the number of features in every entry in the dataset.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list containing the lengths of the features in every row</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def number_of_variables(self) -&gt; List[int]:\n    \"\"\"Get the number of features in every entry in the dataset.\n\n    Returns:\n        A list containing the lengths of the features in every row\n    \"\"\"\n    feats = self._feature_index\n    if len(feats) == 0:\n        return [0]\n    num_vars = feats.column_dims()\n    return num_vars\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.paginated_load_h5ad","title":"<code>paginated_load_h5ad(anndata_path)</code>","text":"<p>Method for block loading a larger h5ad file and converting it to the SCDL format.</p> <p>This should be used in the case when the entire anndata file cannot be loaded into memory. The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk is converted into numpy memory maps which are then concatenated together.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>pd.DataFrame: var variables for features</p> <code>int</code> <code>int</code> <p>number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def paginated_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for block loading a larger h5ad file and converting it to the SCDL format.\n\n    This should be used in the case when the entire anndata file cannot be loaded into memory.\n    The anndata is loaded into memory load_block_row_size number of rows at a time. Each chunk\n    is converted into numpy memory maps which are then concatenated together.\n\n    Raises:\n        NotImplementedError if the data is not loaded in the CSRDataset format.\n\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path, backed=True)\n\n    if not isinstance(adata.X, ad.experimental.CSRDataset):\n        raise NotImplementedError(\"Non-sparse format cannot be loaded: {type(adata.X)}.\")\n    num_rows = adata.X.shape[0]\n\n    self.dtypes[f\"{FileNames.DATA.value}\"] = adata.X.dtype\n\n    # Read the row indices into a memory map.\n    mode = Mode.CREATE_APPEND\n    self.row_index = _create_row_memmaps(num_rows, Path(self.data_path), mode, self.dtypes)\n    self.row_index[:] = adata.X._indptr.astype(int)\n\n    # The data from each column and data chunk of the original anndata file is read in. This is saved into the final\n    # location of the memmap file. In this step, it is saved in the binary file format.\n    memmap_dir_path = Path(self.data_path)\n    with (\n        open(f\"{memmap_dir_path}/{FileNames.COLPTR.value}\", \"wb\") as col_file,\n        open(f\"{memmap_dir_path}/{FileNames.DATA.value}\", \"wb\") as data_file,\n    ):\n        n_elements = 0\n        for row_start in range(0, num_rows, self.load_block_row_size):\n            # Write each array's data to the file in binary format\n            col_block = adata.X[row_start : row_start + self.load_block_row_size].indices\n            col_file.write(col_block.tobytes())\n\n            data_block = adata.X[row_start : row_start + self.load_block_row_size].data\n            data_file.write(data_block.tobytes())\n\n            n_elements += len(data_block)\n\n    # The column and data files are re-opened as memory-mapped arrays with the final shape\n    mode = Mode.READ_APPEND\n    self.col_index = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.COLPTR.value}\",\n        self.dtypes[f\"{FileNames.COLPTR.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    self.data = np.memmap(\n        f\"{memmap_dir_path}/{FileNames.DATA.value}\",\n        dtype=self.dtypes[f\"{FileNames.DATA.value}\"],\n        mode=mode,\n        shape=(n_elements,),\n    )\n    return adata.var, num_rows\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.regular_load_h5ad","title":"<code>regular_load_h5ad(anndata_path)</code>","text":"<p>Method for loading an h5ad file into memorySu and converting it to the SCDL format.</p> <p>Parameters:</p> Name Type Description Default <code>anndata_path</code> <code>str</code> <p>location of data to load</p> required <p>Raises:     NotImplementedError if the data is not in scipy.sparse.spmatrix format     ValueError it there is not count data Returns:     pd.DataFrame: var variables for features     int: number of rows in the dataframe.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def regular_load_h5ad(\n    self,\n    anndata_path: str,\n) -&gt; Tuple[pd.DataFrame, int]:\n    \"\"\"Method for loading an h5ad file into memorySu and converting it to the SCDL format.\n\n    Args:\n        anndata_path: location of data to load\n    Raises:\n        NotImplementedError if the data is not in scipy.sparse.spmatrix format\n        ValueError it there is not count data\n    Returns:\n        pd.DataFrame: var variables for features\n        int: number of rows in the dataframe.\n\n    \"\"\"\n    adata = ad.read_h5ad(anndata_path)  # slow\n\n    if not isinstance(adata.X, scipy.sparse.spmatrix):\n        raise NotImplementedError(\"Error: dense matrix loading not yet implemented.\")\n\n    # Check if raw data is present\n    raw = getattr(adata, \"raw\", None)\n    count_data = None\n    if raw is not None:\n        # If it is, attempt to get the counts in the raw data.\n        count_data = getattr(raw, \"X\", None)\n\n    if count_data is None:\n        # No raw counts were present, resort to normalized\n        count_data = getattr(adata, \"X\")\n    if count_data is None:\n        raise ValueError(\"This file does not have count data\")\n\n    shape = count_data.shape\n    num_rows = shape[0]\n\n    num_elements_stored = count_data.nnz\n\n    self.dtypes[f\"{FileNames.DATA.value}\"] = count_data.dtype\n\n    # Create the arrays.\n    self._init_arrs(num_elements_stored, num_rows)\n    # Store data\n    self.data[0:num_elements_stored] = count_data.data\n\n    # Store the col idx array\n    self.col_index[0:num_elements_stored] = count_data.indices.astype(int)\n\n    # Store the row idx array\n    self.row_index[0 : num_rows + 1] = count_data.indptr.astype(int)\n\n    return adata.var, num_rows\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.save","title":"<code>save(output_path=None)</code>","text":"<p>Saves the class to a given output path.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Optional[str]</code> <p>The location to save - not yet implemented and should</p> <code>None</code> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def save(self, output_path: Optional[str] = None) -&gt; None:\n    \"\"\"Saves the class to a given output path.\n\n    Args:\n        output_path: The location to save - not yet implemented and should\n        be self.data_path\n\n    Raises:\n       NotImplementedError if output_path is not None.\n    \"\"\"\n    if f\"{METADATA.NUM_ROWS.value}\" not in self.metadata:\n        self.metadata[f\"{METADATA.NUM_ROWS.value}\"] = self.number_of_rows()\n\n    self._write_metadata()\n    # Write the feature index. This may not exist.\n    self._feature_index.save(f\"{self.data_path}/{FileNames.FEATURES.value}\")\n\n    # Ensure the object is in a valid state. These are saved at creation!\n    for postfix in [\n        f\"{FileNames.VERSION.value}\",\n        f\"{FileNames.DATA.value}\",\n        f\"{FileNames.COLPTR.value}\",\n        f\"{FileNames.ROWPTR.value}\",\n        f\"{FileNames.FEATURES.value}\",\n    ]:\n        if not os.path.exists(f\"{self.data_path}/{postfix}\"):\n            raise FileNotFoundError(f\"This file should exist from object creation: {self.data_path}/{postfix}\")\n\n    self.data.flush()\n    self.row_index.flush()\n    self.col_index.flush()\n\n    if output_path is not None:\n        raise NotImplementedError(\"Saving to separate path is not yet implemented.\")\n\n    return True\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the dataset.</p> <p>This is the number of entries by the the length of the feature index corresponding to that variable.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of elements in the dataset</p> <code>List[int]</code> <p>A list containing the number of variables for each row.</p> Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def shape(self) -&gt; Tuple[int, List[int]]:\n    \"\"\"Get the shape of the dataset.\n\n    This is the number of entries by the the length of the feature index\n    corresponding to that variable.\n\n    Returns:\n        The number of elements in the dataset\n        A list containing the number of variables for each row.\n    \"\"\"\n    return self.number_of_rows(), self.number_of_variables()\n</code></pre>"},{"location":"API_reference/bionemo/scdl/io/single_cell_memmap_dataset/#bionemo.scdl.io.single_cell_memmap_dataset.SingleCellMemMapDataset.version","title":"<code>version()</code>","text":"<p>Returns a version number.</p> <p>(following .. convention). Source code in <code>bionemo/scdl/io/single_cell_memmap_dataset.py</code> <pre><code>def version(self) -&gt; str:\n    \"\"\"Returns a version number.\n\n    (following &lt;major&gt;.&lt;minor&gt;.&lt;point&gt; convention).\n    \"\"\"\n    return self._version\n</code></pre>"},{"location":"API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/","title":"Convert h5ad to scdl","text":""},{"location":"API_reference/bionemo/scdl/scripts/convert_h5ad_to_scdl/#bionemo.scdl.scripts.convert_h5ad_to_scdl.main","title":"<code>main()</code>","text":"<p>Parse the arguments to process the single cell collection.</p> Source code in <code>bionemo/scdl/scripts/convert_h5ad_to_scdl.py</code> <pre><code>def main():\n    \"\"\"Parse the arguments to process the single cell collection.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--num-workers\", type=int, default=4, help=\"The number of AnnData loaders to run in parallel [4].\"\n    )\n    parser.add_argument(\n        \"--use-mp\",\n        action=\"store_true\",\n        default=False,\n        help=\"Use a subprocess for each worker rather than a lightweight OS thread [False].\",\n    )\n    parser.add_argument(\n        \"--data-path\",\n        type=str,\n        required=True,\n        help=\"A path containing AnnData files. Note: These will all be concatenated.\",\n    )\n    parser.add_argument(\n        \"--save-path\", required=True, type=str, help=\"An output path where an SCDataset will be stored.\"\n    )\n    args = parser.parse_args()\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        coll = SingleCellCollection(temp_dir)\n        coll.load_h5ad_multi(args.data_path, max_workers=args.num_workers, use_processes=args.use_mp)\n        coll.flatten(args.save_path, destroy_on_copy=True)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/","title":"Async worker queue","text":""},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue","title":"<code>AsyncWorkQueue</code>","text":"<p>Implements an asynchronous queue.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>class AsyncWorkQueue:\n    \"\"\"Implements an asynchronous queue.\"\"\"\n\n    def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n        \"\"\"Initialize the AsyncWorkQueue.\n\n        Args:\n            max_workers: The maximum number of worker threads or processes.\n            use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n        \"\"\"\n        self.use_processes = use_processes\n        if use_processes:\n            self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n                concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n            )\n        else:\n            self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n        self.lock = threading.Lock()\n        self.tasks: List[concurrent.futures.Future] = []\n\n    def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n        \"\"\"Submit a task to the work queue.\n\n        Args:\n            func: The function to be executed asynchronously.\n            args: Positional arguments to pass to the function.\n            kwargs: Keyword arguments to pass to the function.\n            A Future object representing the execution of the function.\n\n        Returns:\n            Future: placeholder for the asynchronous operation.\n        \"\"\"\n        with self.lock:\n            future = self.executor.submit(func, *args, **kwargs)\n            self.tasks.append(future)\n            return future\n\n    def shutdown(self, wait: bool = True) -&gt; None:\n        \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n        Args:\n            wait: If True, wait for all tasks to complete before shutting down.\n        \"\"\"\n        self.executor.shutdown(wait=wait)\n\n    def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of completed tasks.\n\n        Returns:\n            A list of Future objects that are completed.\n        \"\"\"\n        with self.lock:\n            completed_tasks = [task for task in self.tasks if task.done()]\n            return completed_tasks\n\n    def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n        \"\"\"Get the list of pending tasks.\n\n        Returns:\n            A list of Future objects that are not yet completed.\n        \"\"\"\n        with self.lock:\n            pending_tasks = [task for task in self.tasks if not task.done()]\n            return pending_tasks\n\n    def get_task_results(self) -&gt; List[Any]:\n        \"\"\"Get the results of all completed tasks.\n\n        Returns:\n            A list of results from the completed tasks.\n\n        Raises:\n            Exception: This would be expected if the task fails to complete or\n            if is cancelled.\n        \"\"\"\n        completed_tasks = self.get_completed_tasks()\n        results = []\n        for task in completed_tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n        return results\n\n    def wait(self) -&gt; List[Any]:\n        \"\"\"Wait for all submitted tasks to complete and return their results.\n\n        Returns:\n            A list of results from all completed tasks.\n        \"\"\"\n        # Wait for all tasks to complete\n        concurrent.futures.wait(self.tasks)\n\n        # Collect results from all tasks\n        results = []\n        for task in self.tasks:\n            try:\n                results.append(task.result())\n            except Exception as e:\n                results.append(e)\n\n        return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.__init__","title":"<code>__init__(max_workers=5, use_processes=False)</code>","text":"<p>Initialize the AsyncWorkQueue.</p> <p>Parameters:</p> Name Type Description Default <code>max_workers</code> <code>int</code> <p>The maximum number of worker threads or processes.</p> <code>5</code> <code>use_processes</code> <code>bool</code> <p>If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.</p> <code>False</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def __init__(self, max_workers: int = 5, use_processes: bool = False) -&gt; None:\n    \"\"\"Initialize the AsyncWorkQueue.\n\n    Args:\n        max_workers: The maximum number of worker threads or processes.\n        use_processes: If True, use ProcessPoolExecutor; otherwise, use ThreadPoolExecutor.\n    \"\"\"\n    self.use_processes = use_processes\n    if use_processes:\n        self.executor: Union[concurrent.futures.ThreadPoolExecutor, concurrent.futures.ProcessPoolExecutor] = (\n            concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n        )\n    else:\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n    self.lock = threading.Lock()\n    self.tasks: List[concurrent.futures.Future] = []\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_completed_tasks","title":"<code>get_completed_tasks()</code>","text":"<p>Get the list of completed tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_completed_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of completed tasks.\n\n    Returns:\n        A list of Future objects that are completed.\n    \"\"\"\n    with self.lock:\n        completed_tasks = [task for task in self.tasks if task.done()]\n        return completed_tasks\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_pending_tasks","title":"<code>get_pending_tasks()</code>","text":"<p>Get the list of pending tasks.</p> <p>Returns:</p> Type Description <code>List[Future]</code> <p>A list of Future objects that are not yet completed.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_pending_tasks(self) -&gt; List[concurrent.futures.Future]:\n    \"\"\"Get the list of pending tasks.\n\n    Returns:\n        A list of Future objects that are not yet completed.\n    \"\"\"\n    with self.lock:\n        pending_tasks = [task for task in self.tasks if not task.done()]\n        return pending_tasks\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.get_task_results","title":"<code>get_task_results()</code>","text":"<p>Get the results of all completed tasks.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from the completed tasks.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>This would be expected if the task fails to complete or</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def get_task_results(self) -&gt; List[Any]:\n    \"\"\"Get the results of all completed tasks.\n\n    Returns:\n        A list of results from the completed tasks.\n\n    Raises:\n        Exception: This would be expected if the task fails to complete or\n        if is cancelled.\n    \"\"\"\n    completed_tasks = self.get_completed_tasks()\n    results = []\n    for task in completed_tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.shutdown","title":"<code>shutdown(wait=True)</code>","text":"<p>Shutdown the executor and wait for the tasks to complete.</p> <p>Parameters:</p> Name Type Description Default <code>wait</code> <code>bool</code> <p>If True, wait for all tasks to complete before shutting down.</p> <code>True</code> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def shutdown(self, wait: bool = True) -&gt; None:\n    \"\"\"Shutdown the executor and wait for the tasks to complete.\n\n    Args:\n        wait: If True, wait for all tasks to complete before shutting down.\n    \"\"\"\n    self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.submit_task","title":"<code>submit_task(func, *args, **kwargs)</code>","text":"<p>Submit a task to the work queue.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to be executed asynchronously.</p> required <code>args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Future</code> <code>Future</code> <p>placeholder for the asynchronous operation.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def submit_task(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -&gt; concurrent.futures.Future:\n    \"\"\"Submit a task to the work queue.\n\n    Args:\n        func: The function to be executed asynchronously.\n        args: Positional arguments to pass to the function.\n        kwargs: Keyword arguments to pass to the function.\n        A Future object representing the execution of the function.\n\n    Returns:\n        Future: placeholder for the asynchronous operation.\n    \"\"\"\n    with self.lock:\n        future = self.executor.submit(func, *args, **kwargs)\n        self.tasks.append(future)\n        return future\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/async_worker_queue/#bionemo.scdl.util.async_worker_queue.AsyncWorkQueue.wait","title":"<code>wait()</code>","text":"<p>Wait for all submitted tasks to complete and return their results.</p> <p>Returns:</p> Type Description <code>List[Any]</code> <p>A list of results from all completed tasks.</p> Source code in <code>bionemo/scdl/util/async_worker_queue.py</code> <pre><code>def wait(self) -&gt; List[Any]:\n    \"\"\"Wait for all submitted tasks to complete and return their results.\n\n    Returns:\n        A list of results from all completed tasks.\n    \"\"\"\n    # Wait for all tasks to complete\n    concurrent.futures.wait(self.tasks)\n\n    # Collect results from all tasks\n    results = []\n    for task in self.tasks:\n        try:\n            results.append(task.result())\n        except Exception as e:\n            results.append(e)\n\n    return results\n</code></pre>"},{"location":"API_reference/bionemo/scdl/util/torch_dataloader_utils/","title":"Torch dataloader utils","text":""},{"location":"API_reference/bionemo/scdl/util/torch_dataloader_utils/#bionemo.scdl.util.torch_dataloader_utils.collate_sparse_matrix_batch","title":"<code>collate_sparse_matrix_batch(batch)</code>","text":"<p>Collate function to create a batch out of sparse tensors.</p> <p>This is necessary to collate sparse matrices of various lengths.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[Tensor]</code> <p>A list of Tensors to collate into a batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The tensors collated into a CSR (Compressed Sparse Row) Format.</p> Source code in <code>bionemo/scdl/util/torch_dataloader_utils.py</code> <pre><code>def collate_sparse_matrix_batch(batch: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Collate function to create a batch out of sparse tensors.\n\n    This is necessary to collate sparse matrices of various lengths.\n\n    Args:\n        batch: A list of Tensors to collate into a batch.\n\n    Returns:\n        The tensors collated into a CSR (Compressed Sparse Row) Format.\n    \"\"\"\n    batch_rows = torch.cumsum(\n        torch.tensor([0] + [sparse_representation.shape[1] for sparse_representation in batch]), dim=0\n    )\n    batch_cols = torch.cat([sparse_representation[1] for sparse_representation in batch]).to(torch.int32)\n    batch_values = torch.cat([sparse_representation[0] for sparse_representation in batch])\n    if len(batch_cols) == 0:\n        max_pointer = 0\n    else:\n        max_pointer = int(batch_cols.max().item() + 1)\n    batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n    return batch_sparse_tensor\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/","title":"Sampler","text":""},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler","title":"<code>BucketBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class BucketBatchSampler(Sampler[List[int]]):\n    \"\"\"A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.\n\n    Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements.\n    Then, a base batch sampler is used for each bucket to create mini-batches.\n\n    The bucket ranges are specified by `bucket_boundaries`, which will be first sorted internally and used to create\n    `len(bucket_boundaries) - 1` left-closed right-open intervals.\n    e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created\n    with ranges: [0, 5), [5, 10), [10, 16).\n\n    The base batch sampler will be created by passing the element indices in each bucket as the data source, and\n    `base_batch_sampler_shared_kwargs` and `base_batch_sampler_individual_kwargs`\n    to the constructor of the base batch sampler class specified as `base_batch_sampler_class`.\n    e.g. `base_batch_sampler_shared_kwargs = {'drop_last': True}` and `base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}`\n    will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like\n    `base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)`.\n\n    In the `__iter__` method, if `shuffle` is `True`, the element indices in each bucket will be shuffled, and a bucket\n    is randomly selected each time to create a mini-batch. If `shuffle` is `False`, there is no shuffle on element indices,\n    and the bucket is selected in ascending order of its interval boundaries.\n\n    This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.\n\n    Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n    &gt;&gt;&gt; # Define the sizes for a dataset\n    &gt;&gt;&gt; sizes = torch.arange(25)\n    &gt;&gt;&gt; # Define bucket ranges\n    &gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n    &gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n    &gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=False,\n        )\n\n    &gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n    &gt;&gt;&gt; # randomize the dataset and buckets\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=torch.utils.data.BatchSampler,\n            base_batch_sampler_shared_kwargs={'drop_last': False},\n            base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n    &gt;&gt;&gt; print(list(batch_sampler))\n    [[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n    &gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n    &gt;&gt;&gt; item_costs = sizes.tolist()\n    &gt;&gt;&gt; def cost_of_element(index):\n            return item_costs[index]\n    &gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n            sizes=sizes,\n            bucket_boundaries=bucket_boundaries,\n            base_batch_sampler_class=SizeAwareBatchSampler,\n            base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n            base_batch_sampler_individual_kwargs={},\n            shuffle=True,\n            generator=torch.Generator().manual_seed(0),\n        )\n    &gt;&gt;&gt; print(list(iter(batch_sampler)))\n    [[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sizes: torch.Tensor,\n        bucket_boundaries: torch.Tensor,\n        base_batch_sampler_class: Type[S],\n        base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n        base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n        shuffle: Optional[bool] = True,\n        generator: Optional[torch.Generator] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the BucketBatchSampler.\n\n        Args:\n            sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n            bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n                It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n                It should not contain any duplicate values.\n            base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n                `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n            base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n            base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n                each bucket batch sampler with the corresponding key value pairs.\n                Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n                Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n                Default to  {}.\n            shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n            generator: Generator used in sampling. Defaults to None.\n\n        Raises:\n            ValueError: If `sizes` is not a 1D tensor of real numbers.\n            ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n            ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n            ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n            RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n        \"\"\"\n        if not torch.is_tensor(sizes):\n            raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n        if sizes.ndim != 1:\n            raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n        if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n            )\n\n        if not torch.is_tensor(bucket_boundaries):\n            raise TypeError(\n                f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if bucket_boundaries.ndim != 1:\n            raise ValueError(\n                f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n            )\n\n        if len(bucket_boundaries) &lt; 2:\n            raise ValueError(\n                f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n            )\n\n        if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n            raise ValueError(\n                f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n            )\n\n        bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n        if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n            raise ValueError(\n                f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n            )\n\n        if not isinstance(shuffle, bool):\n            raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n        self.sizes = sizes\n        self.bucket_boundaries = bucket_boundaries\n        self.num_buckets = len(bucket_boundaries) - 1\n        self.shuffle = shuffle\n        self.generator = generator\n        if self.shuffle and self.generator is None:\n            self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n        if not issubclass(base_batch_sampler_class, Sampler):\n            raise TypeError(\n                f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n            )\n\n        if not isinstance(base_batch_sampler_shared_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n            )\n\n        if not isinstance(base_batch_sampler_individual_kwargs, dict):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n            )\n\n        if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n            raise TypeError(\n                f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n            )\n\n        if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n            raise ValueError(\n                f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n                f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n            )\n\n        self.base_batch_sampler_class = base_batch_sampler_class\n        self.base_batch_sampler_shared_kwargs = (\n            {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n        )\n        base_batch_sampler_individual_kwargs = (\n            {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n        )\n        self.base_batch_sampler_individual_kwargs = [\n            {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n            for k in range(self.num_buckets)\n        ]\n\n        self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n        self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n        # bucket index for each element\n        element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n        # element indices reordered for each bucket\n        reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n        # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n        # bucket segments\n        bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n        self.bucket_element_indices = []\n        # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n        for bucket_idx in range(self.num_buckets):\n            self.bucket_element_indices.append(\n                reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n            )\n        self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n        self.num_samples = torch.sum(self.bucket_sizes).item()\n        if self.num_samples == 0:\n            raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n        if self.num_samples &lt; len(self.sizes):\n            warnings.warn(\n                f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n            )\n\n        self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n\n    def _init_base_batch_samplers(self) -&gt; list[Sampler[List[int]]]:\n        \"\"\"Initialize batch samplers for each bucket.\n\n        Returns:\n            List of batch samplers.\n        \"\"\"\n        base_batch_samplers = []\n        for k in range(self.num_buckets):\n            base_batch_samplers.append(\n                self.base_batch_sampler_class(\n                    self.bucket_element_indices[k],\n                    **self.base_batch_sampler_shared_kwargs,\n                    **self.base_batch_sampler_individual_kwargs[k],\n                )\n            )\n        return base_batch_samplers\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of batches.\n\n        Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n        Returns:\n            int: Number of batches\n        \"\"\"\n        num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n        return num_batches\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices of elements with sizes from each bucket range.\n\n        Yields:\n            List[int]: A batch of indices of elements with sizes from each bucket range.\n        \"\"\"\n        if self.shuffle:\n            for indices in self.bucket_element_indices:\n                idx = torch.randperm(len(indices), generator=self.generator)\n                indices[:] = torch.tensor(indices)[idx].tolist()\n\n        base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n        bucket_remaining_elements = self.bucket_sizes.clone()\n        total_remaining_elements = self.num_samples\n\n        while total_remaining_elements &gt; 0:\n            if self.shuffle:\n                bucket_idx = torch.multinomial(\n                    bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n                )\n            else:\n                bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n            try:\n                batch = next(base_batch_sampler_iters[bucket_idx])\n                bucket_remaining_elements[bucket_idx] -= len(batch)\n                total_remaining_elements -= len(batch)\n                yield batch\n            except StopIteration:\n                bucket_remaining_elements[bucket_idx] = 0\n                total_remaining_elements = torch.sum(bucket_remaining_elements)\n                continue\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__init__","title":"<code>__init__(sizes, bucket_boundaries, base_batch_sampler_class, base_batch_sampler_shared_kwargs=None, base_batch_sampler_individual_kwargs=None, shuffle=True, generator=None)</code>","text":"<p>Initializes the BucketBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the size of each element in the dataset.</p> required <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor of real numbers representing the boundaries of the bucket ranges. It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges. It should not contain any duplicate values.</p> required <code>base_batch_sampler_class</code> <code>Type[S]</code> <p>Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices, <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</p> required <code>base_batch_sampler_shared_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Shared keyword argument dictionary used to initialize all base batch samplers for all buckets. Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to  {}.</p> <code>None</code> <code>base_batch_sampler_individual_kwargs</code> <code>Optional[Dict[str, Iterable]]</code> <p>Keyword argument dictionary used to initialize each bucket batch sampler with the corresponding key value pairs. Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets). Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>. Default to  {}.</p> <code>None</code> <code>shuffle</code> <code>Optional[bool]</code> <p>A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</p> <code>True</code> <code>generator</code> <code>Optional[Generator]</code> <p>Generator used in sampling. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>sizes</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</p> <code>ValueError</code> <p>If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</p> <code>ValueError</code> <p>If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</p> <code>RuntimeError</code> <p>If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sizes: torch.Tensor,\n    bucket_boundaries: torch.Tensor,\n    base_batch_sampler_class: Type[S],\n    base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n    base_batch_sampler_individual_kwargs: Optional[Dict[str, Iterable]] = None,\n    shuffle: Optional[bool] = True,\n    generator: Optional[torch.Generator] = None,\n) -&gt; None:\n    \"\"\"Initializes the BucketBatchSampler.\n\n    Args:\n        sizes: A 1D tensor of real numbers representing the size of each element in the dataset.\n        bucket_boundaries: A 1D tensor of real numbers representing the boundaries of the bucket ranges.\n            It will be first sorted and used to create `len(bucket_boundaries) - 1` left-closed right-open intervals as bucket ranges.\n            It should not contain any duplicate values.\n        base_batch_sampler_class: Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,\n            `base_batch_sampler_shared_kwargs` and the corresponding `base_batch_sampler_individual_kwargs`.\n        base_batch_sampler_shared_kwargs: Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_individual_kwargs`. Default to  {}.\n        base_batch_sampler_individual_kwargs: Keyword argument dictionary used to initialize\n            each bucket batch sampler with the corresponding key value pairs.\n            Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).\n            Sufficient and valid arguments should be provided for `base_batch_sampler_class` with `base_batch_sampler_shared_kwargs`.\n            Default to  {}.\n        shuffle: A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.\n        generator: Generator used in sampling. Defaults to None.\n\n    Raises:\n        ValueError: If `sizes` is not a 1D tensor of real numbers.\n        ValueError: If `bucket_boundaries` is not a 1D tensor of real numbers.\n        ValueError: If `base_batch_sampler_individual_kwargs` or `base_batch_sampler_individual_kwargs` is not a keyword argument dictionary.\n        ValueError: If the length of values in the dict of `base_batch_sampler_individual_kwargs` must be equal to len(bucket_boundaries) - 1.\n        RuntimeError: If there is no elements with sizes inside the ranges specified by `bucket_boundaries`.\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if not torch.is_floating_point(sizes) and sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"sizes should contain only integers or floating point numbers, but got sizes.dtype={sizes.dtype}\"\n        )\n\n    if not torch.is_tensor(bucket_boundaries):\n        raise TypeError(\n            f\"bucket_boundaries should be a torch tensor, but got bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if bucket_boundaries.ndim != 1:\n        raise ValueError(\n            f\"bucket_boundaries should be a 2D tensor, but got bucket_boundaries with shape {bucket_boundaries.shape}\"\n        )\n\n    if len(bucket_boundaries) &lt; 2:\n        raise ValueError(\n            f\"bucket_boundaries should have at least 2 numbers, but got bucket_boundaries={bucket_boundaries.shape}\"\n        )\n\n    if not torch.is_floating_point(bucket_boundaries) and bucket_boundaries.dtype not in TorchIntegerDataTypes:\n        raise ValueError(\n            f\"bucket_boundaries should contain only integers or floating point numbers, but got bucket_boundaries.dtype={bucket_boundaries.dtype}\"\n        )\n\n    bucket_boundaries = torch.sort(bucket_boundaries)[0]\n\n    if torch.any(bucket_boundaries[:-1] &gt;= bucket_boundaries[1:]):\n        raise ValueError(\n            f\"bucket_boundaries should not have duplicate values, and should specify the lower endpoint of each interval smaller than the upper endpoint, but got sorted bucket_boundaries={bucket_boundaries}\"\n        )\n\n    if not isinstance(shuffle, bool):\n        raise TypeError(f\"shuffle should be a boolean value, but got shuffle={shuffle}\")\n\n    self.sizes = sizes\n    self.bucket_boundaries = bucket_boundaries\n    self.num_buckets = len(bucket_boundaries) - 1\n    self.shuffle = shuffle\n    self.generator = generator\n    if self.shuffle and self.generator is None:\n        self.generator = torch.Generator().manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n\n    if not issubclass(base_batch_sampler_class, Sampler):\n        raise TypeError(\n            f\"base_batch_sampler_class should be a batch sampler class inherited from torch.utils.data.Sampler, but got base_batch_sampler_class={base_batch_sampler_class}\"\n        )\n\n    if not isinstance(base_batch_sampler_shared_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should be a dictionary, but got base_batch_sampler_shared_kwargs={base_batch_sampler_shared_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_shared_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_shared_kwargs should have string keys, but got keys={list(base_batch_sampler_shared_kwargs.keys())}\"\n        )\n\n    if not isinstance(base_batch_sampler_individual_kwargs, dict):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should be a dictionary, but got base_batch_sampler_individual_kwargs={base_batch_sampler_individual_kwargs}\"\n        )\n\n    if not all(isinstance(key, str) for key in base_batch_sampler_individual_kwargs.keys()):\n        raise TypeError(\n            f\"base_batch_sampler_individual_kwargs should have string keys, but got keys={list(base_batch_sampler_individual_kwargs.keys())}\"\n        )\n\n    if not all(len(list(value)) == self.num_buckets for value in base_batch_sampler_individual_kwargs.values()):\n        raise ValueError(\n            f\"Each value in base_batch_sampler_individual_kwargs should have a length of {self.num_buckets}, \"\n            f\"but got lengths {[len(list(value)) for value in base_batch_sampler_individual_kwargs.values()]}\"\n        )\n\n    self.base_batch_sampler_class = base_batch_sampler_class\n    self.base_batch_sampler_shared_kwargs = (\n        {} if base_batch_sampler_shared_kwargs is None else base_batch_sampler_shared_kwargs\n    )\n    base_batch_sampler_individual_kwargs = (\n        {} if base_batch_sampler_individual_kwargs is None else base_batch_sampler_individual_kwargs\n    )\n    self.base_batch_sampler_individual_kwargs = [\n        {key: list(base_batch_sampler_individual_kwargs[key])[k] for key in base_batch_sampler_individual_kwargs}\n        for k in range(self.num_buckets)\n    ]\n\n    self.bucket_sizes: torch.Tensor  # number of elements in each bucket\n    self.bucket_element_indices: List[List[int]]  # List of elements' indices for each bucket\n\n    # bucket index for each element\n    element_bucket_indices = torch.bucketize(sizes, bucket_boundaries, right=True)\n\n    # element indices reordered for each bucket\n    reordered_element_indices = torch.argsort(element_bucket_indices, stable=True)\n\n    # bucket sizes, including the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    bucket_sizes = torch.bincount(element_bucket_indices, minlength=len(bucket_boundaries) + 1)\n\n    # bucket segments\n    bucket_segments = torch.cumsum(bucket_sizes, dim=0)[:-1]\n\n    self.bucket_element_indices = []\n    # exclude the buckets for &lt; bucket_boundaries[0] and &gt;= bucket_boundaries[-1]\n    for bucket_idx in range(self.num_buckets):\n        self.bucket_element_indices.append(\n            reordered_element_indices[bucket_segments[bucket_idx] : bucket_segments[bucket_idx + 1]].tolist()\n        )\n    self.bucket_sizes = bucket_sizes[1 : (self.num_buckets + 1)]\n\n    self.num_samples = torch.sum(self.bucket_sizes).item()\n    if self.num_samples == 0:\n        raise RuntimeError(\"The sizes of all elements in the dataset are outside the bucket ranges provided\")\n    if self.num_samples &lt; len(self.sizes):\n        warnings.warn(\n            f\"{len(self.sizes) - self.num_samples} elements are outside the buckets provided and will be skipped\"\n        )\n\n    self.base_batch_samplers: List[Sampler] = self._init_base_batch_samplers()\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>List[int]: A batch of indices of elements with sizes from each bucket range.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices of elements with sizes from each bucket range.\n\n    Yields:\n        List[int]: A batch of indices of elements with sizes from each bucket range.\n    \"\"\"\n    if self.shuffle:\n        for indices in self.bucket_element_indices:\n            idx = torch.randperm(len(indices), generator=self.generator)\n            indices[:] = torch.tensor(indices)[idx].tolist()\n\n    base_batch_sampler_iters = [iter(batch_sampler) for batch_sampler in self.base_batch_samplers]\n    bucket_remaining_elements = self.bucket_sizes.clone()\n    total_remaining_elements = self.num_samples\n\n    while total_remaining_elements &gt; 0:\n        if self.shuffle:\n            bucket_idx = torch.multinomial(\n                bucket_remaining_elements / total_remaining_elements, 1, generator=self.generator\n            )\n        else:\n            bucket_idx = torch.argmax((bucket_remaining_elements &gt; 0).to(int))  # type: ignore\n\n        try:\n            batch = next(base_batch_sampler_iters[bucket_idx])\n            bucket_remaining_elements[bucket_idx] -= len(batch)\n            total_remaining_elements -= len(batch)\n            yield batch\n        except StopIteration:\n            bucket_remaining_elements[bucket_idx] = 0\n            total_remaining_elements = torch.sum(bucket_remaining_elements)\n            continue\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.BucketBatchSampler.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of batches</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of batches.\n\n    Can only be called if the `base_batch_sampler_class` has __len__() implemented\n\n    Returns:\n        int: Number of batches\n    \"\"\"\n    num_batches = sum(len(sampler) for sampler in self.base_batch_samplers)  # type: ignore\n    return num_batches\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler","title":"<code>SizeAwareBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.</p> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>class SizeAwareBatchSampler(Sampler[List[int]]):\n    \"\"\"Varriying-size batching data sampler class that ensures batch size doesn't exceed maximum.\n\n    A sampler that batches elements of varying sizes while ensuring\n    that the total size of each batch does not exceed a specified maximum.\n\n    This is useful when dealing with datasets where each element has a\n    different size, such as graphs or sequences of varying lengths.\n    The sampler uses a provided `sizeof` function to determine the size\n    of each element in the dataset and ensures that the total size of\n    each batch does not exceed the specified `max_total_size`.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n    &gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n    &gt;&gt;&gt; def sizeof(index):\n    ...     return dataset[index].numel()\n\n\n    &gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n    &gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n    ...     sampler=torch.utils.data.SequentialSampler(dataset),\n    ...     sizeof=sizeof,\n    ...     max_total_size=4\n    ... )\n\n\n    &gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n    &gt;&gt;&gt; print(list(batch_sampler))\n        [[0, 1], [2, 3], [4]]\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        sampler: Union[Sampler[List[int]], Iterable[int]],\n        sizeof: Callable[[int], Real],\n        max_total_size: Real,\n        info_logger: Optional[Callable[[str], None]] = None,\n        warn_logger: Optional[Callable[[str], None]] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the SizeAwareBatchSampler.\n\n        Args:\n            sampler: The underlying sampler.\n            sizeof: A function that returns the size at each index. E.g., this can used to\n                determine how much memory an element consumes. Its return type must be\n                comparable with `max_total_size` and it must be addable (operator `+`).\n            max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n                is defined by the `sizeof` argument. The type of this value must be comparable\n                with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n            info_logger: A function to log info. Defaults to None.\n            warn_logger: A function to log warnings. Defaults None.\n\n        Raises:\n            TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n            ValueError: If max_total_size is not a positive number.\n\n        \"\"\"\n        if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n            raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n        if not isinstance(max_total_size, Real):\n            raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n        self._info_logger = info_logger\n        self._warn_logger = warn_logger\n\n        self._is_sizeof_callable = callable(sizeof)\n\n        if not self._is_sizeof_callable:\n            raise TypeError(\"sizeof must be a callable\")\n\n        self._sampler = sampler\n        self._sizeof = sizeof\n        self._max_total_size = max_total_size\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"Iterate over batches of indices.\n\n        This function yields batches of indices that do not exceed the maximum total size.\n\n        Yields:\n            A batch of indices that do not exceed the maximum total size.\n        \"\"\"\n        return size_aware_batching(\n            self._sampler,\n            self._sizeof,\n            self._max_total_size,\n            collate_fn=None,\n            info_logger=self._info_logger,\n            warn_logger=self._warn_logger,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__init__","title":"<code>__init__(sampler, sizeof, max_total_size, info_logger=None, warn_logger=None)</code>","text":"<p>Initializes the SizeAwareBatchSampler.</p> <p>Parameters:</p> Name Type Description Default <code>sampler</code> <code>Union[Sampler[List[int]], Iterable[int]]</code> <p>The underlying sampler.</p> required <code>sizeof</code> <code>Callable[[int], Real]</code> <p>A function that returns the size at each index. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total size of a mini-batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</p> <code>ValueError</code> <p>If max_total_size is not a positive number.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __init__(\n    self,\n    sampler: Union[Sampler[List[int]], Iterable[int]],\n    sizeof: Callable[[int], Real],\n    max_total_size: Real,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; None:\n    \"\"\"Initializes the SizeAwareBatchSampler.\n\n    Args:\n        sampler: The underlying sampler.\n        sizeof: A function that returns the size at each index. E.g., this can used to\n            determine how much memory an element consumes. Its return type must be\n            comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total size of a mini-batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults None.\n\n    Raises:\n        TypeError: If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.\n        ValueError: If max_total_size is not a positive number.\n\n    \"\"\"\n    if not (isinstance(sampler, Sampler) or (isinstance(sampler, Iterable) and not isinstance(sampler, str))):\n        raise TypeError(\"sampler should be an instance of torch.utils.data.Sampler or Iterable\")\n\n    if not isinstance(max_total_size, Real):\n        raise ValueError(f\"max_total_size should be int or float but got {type(max_total_size)}\")\n\n    self._info_logger = info_logger\n    self._warn_logger = warn_logger\n\n    self._is_sizeof_callable = callable(sizeof)\n\n    if not self._is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    self._sampler = sampler\n    self._sizeof = sizeof\n    self._max_total_size = max_total_size\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.SizeAwareBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> Type Description <code>List[int]</code> <p>A batch of indices that do not exceed the maximum total size.</p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"Iterate over batches of indices.\n\n    This function yields batches of indices that do not exceed the maximum total size.\n\n    Yields:\n        A batch of indices that do not exceed the maximum total size.\n    \"\"\"\n    return size_aware_batching(\n        self._sampler,\n        self._sizeof,\n        self._max_total_size,\n        collate_fn=None,\n        info_logger=self._info_logger,\n        warn_logger=self._warn_logger,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/sampler/#bionemo.size_aware_batching.sampler.size_aware_batching","title":"<code>size_aware_batching(dataset, sizeof, max_total_size, collate_fn=None, info_logger=None, warn_logger=None)</code>","text":"<p>Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.</p> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>The input iterable.</p> required <code>sizeof</code> <code>Callable[[Data], Real]</code> <p>A function or mapping that returns the \"size\" of each element in <code>dataset</code>. E.g., this can used to determine how much memory an element consumes. Its return type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</p> required <code>max_total_size</code> <code>Real</code> <p>The maximum total \"size\" of each batch. The semantics of \"size\" is defined by the <code>sizeof</code> argument. The type of this value must be comparable with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</p> required <code>collate_fn</code> <code>Optional[Callable[[Iterable[Data]], BatchCollated]]</code> <p>An optional function to collate batches. Defaults to None, in which case each batch is a list of elements from the input dataset</p> <code>None</code> <code>info_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log info. Defaults to None.</p> <code>None</code> <code>warn_logger</code> <code>Optional[Callable[[str], None]]</code> <p>A function to log warnings. Defaults to None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Union[List[Data], BatchCollated]</code> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions 1. Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,    by going over the data item one by one to build a batch and yield it as soon as the    addition of the next data item to the batch would exceed <code>max_total_size</code> or if the    batch is the last one (end of iteration) 2. Additive size measurement. For the general usage case of building mini-batches with    a threshold of the batch's memory consumption, it assumes that the size of the batch is    the sum of all elements in the batch (additive property). 3. Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values    must be compared with <code>max_total_size</code> to threshold the size of batches</p> <p>Caveat 1: The generated batch sizes may have large variance    - how to workaround: filter the output of this generator using a batch size threshold 2: The number of batches may vary a lot across different epochs.    - how to workaround: increase the number of steps that compose an epoch,      e.g., in the Lightning training/validation loop, which effectively increases the input      dataset size per epoch</p> <p>Example: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/sampler.py</code> <pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None,\n) -&gt; Iterator[Union[List[Data], BatchCollated]]:\n    \"\"\"Creates a batching iterator where each batch size varries (within a max limit) according to memory consumption.\n\n    A generator that batches elements from an iterable while ensuring that the\n    total size of each batch does not exceed a specified maximum. Here the size\n    can be a measurement of memory consumption of the elements in the batch.\n    This can be useful for both indexible data or non-indexible but iterable data.\n\n    Args:\n        dataset: The input iterable.\n        sizeof: A function or mapping that returns the \"size\" of each element in `dataset`.\n            E.g., this can used to determine how much memory an element consumes. Its return\n            type must be comparable with `max_total_size` and it must be addable (operator `+`).\n        max_total_size: The maximum total \"size\" of each batch. The semantics of \"size\"\n            is defined by the `sizeof` argument. The type of this value must be comparable\n            with the return type of sizeof, i.e., the operator `&lt;` and `==` must be meaningful.\n        collate_fn: An optional function to collate batches. Defaults to None, in which case\n            each batch is a list of elements from the input dataset\n        info_logger: A function to log info. Defaults to None.\n        warn_logger: A function to log warnings. Defaults to None.\n\n    Yields:\n        A generator that yields batches from `dataset`.\n\n    -----------\n    Assumptions\n    1. Linear complexity. This function consumes the given Iterable of data (`dataset`) once,\n       by going over the data item one by one to build a batch and yield it as soon as the\n       addition of the next data item to the batch would exceed `max_total_size` or if the\n       batch is the last one (end of iteration)\n    2. Additive size measurement. For the general usage case of building mini-batches with\n       a threshold of the batch's memory consumption, it assumes that the size of the batch is\n       the sum of all elements in the batch (additive property).\n    3. Comparable type of `max_total_size` and `sizeof`'s return. `sizeof`'s return values\n       must be compared with `max_total_size` to threshold the size of batches\n\n\n    ------\n    Caveat\n    1: The generated batch sizes may have large variance\n       - how to workaround: filter the output of this generator using a batch size threshold\n    2: The number of batches may vary a lot across different epochs.\n       - how to workaround: increase the number of steps that compose an epoch,\n         e.g., in the Lightning training/validation loop, which effectively increases the input\n         dataset size per epoch\n\n\n    -------\n\n    Example:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from torch.utils.data import default_collate\n    &gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n    &gt;&gt;&gt; # Define a sample dataset with torch.tensor\n    &gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n    ...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n    &gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n    &gt;&gt;&gt; def sizeof(x):\n    ...     return x.numel()\n\n    &gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n    &gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n    &gt;&gt;&gt; batches = list(gen)\n    &gt;&gt;&gt; print(batches)\n        [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n    ```\n\n    \"\"\"\n    is_sizeof_callable = callable(sizeof)\n    has_collate_fn = collate_fn is not None and callable(collate_fn)\n\n    if not is_sizeof_callable:\n        raise TypeError(\"sizeof must be a callable\")\n\n    batch_total_size = 0\n    batch = []\n    n_samples = 0\n    n_samples_batched = 0\n    n_batches = 0\n    for data in dataset:\n        n_samples += 1\n        try:\n            new_size = sizeof(data)\n        except Exception as e:\n            raise RuntimeError(f\"sizeof raises error at data={data}: {e}\") from e\n        if new_size &gt; max_total_size:\n            if warn_logger is not None:\n                warn_logger(\n                    f\"Size of element {data} exceeds max_total_size\" f\" ({new_size} &gt; {max_total_size}), skipping\"\n                )\n            continue\n        if new_size + batch_total_size &gt; max_total_size:\n            n_batches += 1\n            if has_collate_fn:\n                yield collate_fn(batch)\n            else:\n                yield batch\n            batch_total_size = 0\n            batch = []\n        batch.append(data)\n        n_samples_batched += 1\n        batch_total_size += new_size\n\n    # return the remaining batch if there is\n    if len(batch) &gt; 0:\n        n_batches += 1\n        if has_collate_fn:\n            yield collate_fn(batch)\n        else:\n            yield batch\n\n    if warn_logger is not None and n_samples_batched &lt; n_samples:\n        warn_logger(\n            f\"{n_samples_batched} samples were batched from {n_samples} \"\n            f\"of the input data. Missing samples are due to exceeding max_total_size={max_total_size})\"\n        )\n\n    if info_logger is not None:\n        info_logger(\n            f\"Batched {n_samples_batched} samples into {n_batches} batches. \"\n            f\"If this doesn't match the your expectation, consider adjusting \"\n            f\"max_total_size or the sizeof functor\"\n        )\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.Buckets","title":"<code>Buckets</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A container for storing bucket boundaries and sizes.</p> <p>Attributes:</p> Name Type Description <code>bucket_boundaries</code> <code>Tensor</code> <p>A 1D tensor with the boundaries of all the bucket.</p> <code>bucket_sizes</code> <code>Tensor</code> <p>The number of elements in each bucket.</p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>class Buckets(NamedTuple):\n    \"\"\"A container for storing bucket boundaries and sizes.\n\n    Attributes:\n        bucket_boundaries (torch.Tensor): A 1D tensor with the boundaries of all the bucket.\n        bucket_sizes (torch.Tensor): The number of elements in each bucket.\n    \"\"\"\n\n    bucket_boundaries: torch.Tensor\n    bucket_sizes: torch.Tensor\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.collect_cuda_peak_alloc","title":"<code>collect_cuda_peak_alloc(dataset, work, device, cleanup=None)</code>","text":"<p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Iterable[Data]</code> <p>An iterable containing the input data.</p> required <code>work</code> <code>Callable[[Data], Feature]</code> <p>A function that takes a data point and returns its corresponding feature. This is where the main computation happens and memory allocations are tracked.</p> required <code>device</code> <code>device</code> <p>The target Torch CUDA device.</p> required <code>cleanup</code> <code>Optional[Callable[[], None]]</code> <p>A function that is called after each iteration to perform any necessary cleanup.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[Feature], List[int]]</code> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided device is not a CUDA device.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None,\n) -&gt; Tuple[List[Feature], List[int]]:\n    \"\"\"Collects CUDA peak memory allocation statistics for a given workflow.\n\n    This function iterates through the provided dataset, applies the given feature function to each data point,\n    and records the peak CUDA memory allocation during this process. The features extracted from the data points\n    are collected along with their corresponding memory usage statistics.\n\n    Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized\n    data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.\n\n    Args:\n        dataset: An iterable containing the input data.\n        work: A function that takes a data point and returns its corresponding feature. This is where\n            the main computation happens and memory allocations are tracked.\n        device: The target Torch CUDA device.\n        cleanup: A function that is called after each iteration to perform any necessary cleanup.\n\n    Returns:\n        A tuple containing the collected features and their corresponding memory usage statistics.\n\n    Raises:\n        ValueError: If the provided device is not a CUDA device.\n\n    -------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n    &gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n    &gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n    &gt;&gt;&gt; dataset, model, optimizer = ...\n    &gt;&gt;&gt; # Set the target Torch CUDA device.\n    &gt;&gt;&gt; device = torch.device(\"cuda:0\")\n    &gt;&gt;&gt; model = model.to(device)\n\n    &gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n    &gt;&gt;&gt; # do a training step\n    &gt;&gt;&gt; def work(data):\n    ...     # example body of a training loop\n    ...     optimizer.zero_grad()\n    ...     output = model(data.to(device))\n    ...     loss = compute_loss(output)\n    ...     loss.backward()\n    ...     optimizer.step()\n    ...     # extract the feature for later to be modeled or analyzed\n    ...     return featurize(data)\n\n    &gt;&gt;&gt; # can optionally use a cleanup function to release the references\n    &gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n    &gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n    &gt;&gt;&gt; def cleanup():\n    ...     model.zero_grad(set_to_none=True)\n\n    &gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n    &gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n    ...     dataset=batches,\n    ...     work=work,\n    ...     device=device,\n    ...     cleanup=cleanup,\n    ... )\n\n\n    &gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n    &gt;&gt;&gt; # that can use these statistics to predict memory usage\n    &gt;&gt;&gt; memory_model = ...\n    &gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n    ```\n\n\n    \"\"\"\n    if device.type != \"cuda\":\n        raise ValueError(\"This function is intended for CUDA devices only.\")\n\n    features = []\n    alloc_peaks = []\n\n    for data in dataset:\n        try:\n            torch.cuda.reset_peak_memory_stats(device)\n            feature = work(data)\n            alloc_peak = torch.cuda.memory_stats(device)[\"allocated_bytes.all.peak\"]\n            alloc_peaks.append(alloc_peak)\n            features.append(feature)\n        except torch.cuda.OutOfMemoryError:\n            print(\"Encounter CUDA out-of-memory error. Skipping sample\", file=sys.stderr, flush=True)\n            continue\n        finally:\n            # ensures cleanup is done next round even in case of exception\n            del data\n            if \"feature\" in locals():\n                del feature\n            if cleanup is not None:\n                cleanup()\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.reset_peak_memory_stats(device)\n    return features, alloc_peaks\n</code></pre>"},{"location":"API_reference/bionemo/size_aware_batching/utils/#bionemo.size_aware_batching.utils.create_buckets","title":"<code>create_buckets(sizes, max_width, min_bucket_count)</code>","text":"<p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Tensor</code> <p>An 1D tensor of integers.</p> required <code>max_width</code> <code>int</code> <p>The maximum width of a bucket, should be a positive integer.</p> required <code>min_bucket_count</code> <code>int</code> <p>The minimum count of a bucket, should be a positive integer. Bucket size may be smaller than min_bucket_count if its width reaches max_width.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided sizes is empty, or not integers.</p> <code>ValueError</code> <p>If max_width is not a positive integer or min_bucket_count is not a positive integer.</p> <p>Returns:</p> Type Description <code>Buckets</code> <p>A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples: <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre></p> Source code in <code>bionemo/size_aware_batching/utils.py</code> <pre><code>def create_buckets(sizes: torch.Tensor, max_width: int, min_bucket_count: int) -&gt; Buckets:\n    \"\"\"Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.\n\n    It will return a named tuple containing the bucket boundaries and the actual bucket sizes.\n    e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements\n    and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.\n\n\n    Args:\n        sizes: An 1D tensor of integers.\n        max_width: The maximum width of a bucket, should be a positive integer.\n        min_bucket_count: The minimum count of a bucket, should be a positive integer.\n            Bucket size may be smaller than min_bucket_count if its width reaches max_width.\n\n    Raises:\n        ValueError: If the provided sizes is empty, or not integers.\n        ValueError: If max_width is not a positive integer or min_bucket_count is not a positive integer.\n\n    Returns:\n        A namedtuple containing bucket boundaries in ascending order and the number of elements in each bucket.\n\n    ---------\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n    &gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n    &gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 1,  6, 11, 16, 21, 23])\n\n    &gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([12,  0,  0,  0,  4])\n\n    &gt;&gt;&gt; sizes = torch.arange(20)\n    &gt;&gt;&gt; # min_bucket_count is used to control bucket size\n    &gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n    &gt;&gt;&gt; print(buckets.bucket_boundaries)\n    tensor([ 0,  5, 10, 15, 20])\n\n    &gt;&gt;&gt; print(buckets.bucket_sizes)\n    tensor([5, 5, 5, 5])\n    ```\n\n    \"\"\"\n    if not torch.is_tensor(sizes):\n        raise TypeError(f\"sizes should be a torch tensor, but got sizes={sizes}\")\n\n    if sizes.ndim != 1:\n        raise ValueError(f\"sizes should be a 1D tensor, but got sizes with shape {sizes.shape}\")\n\n    if sizes.dtype not in TorchIntegerDataTypes:\n        raise ValueError(f\"sizes should contain only integers, but got sizes.dtype={sizes.dtype}\")\n\n    if len(sizes) == 0:\n        raise ValueError(\"sizes should not be empty\")\n\n    if not isinstance(max_width, int) or max_width &lt;= 0:\n        raise ValueError(f\"max_width should be a positive integer but got max_width={max_width}\")\n\n    if not isinstance(min_bucket_count, int) or min_bucket_count &lt;= 0:\n        raise ValueError(f\"min_bucket_count should be a positive integer but got min_bucket_count={min_bucket_count}\")\n\n    unique_values, counts = torch.unique(sizes, return_counts=True, sorted=True)\n\n    bucket_boundaries = [unique_values[0]]\n    bucket_sizes = []\n    start = 0\n    end = 0\n    upper_bound = unique_values[0] + 1\n    bucket_count = 0\n\n    while start &lt; len(unique_values):\n        while (\n            end &lt; len(unique_values)\n            and bucket_count &lt; min_bucket_count\n            and unique_values[end] - bucket_boundaries[-1] &lt; max_width\n        ):\n            bucket_count += counts[end]\n            end += 1\n\n        bucket_sizes.append(sum(counts[start:end]))\n        if end == len(unique_values):\n            upper_bound = unique_values[-1] + 1\n        else:\n            upper_bound = unique_values[end]\n\n        # Adjust the end of the range to ensure that no width exceeds 'max_width'\n        n_empty_buckets = (upper_bound - bucket_boundaries[-1]) // max_width\n        if n_empty_buckets &gt; 0:\n            bucket_boundaries.extend(\n                list(\n                    range(\n                        bucket_boundaries[-1] + max_width,\n                        bucket_boundaries[-1] + max_width * (n_empty_buckets + 1),\n                        max_width,\n                    )\n                )\n            )\n            bucket_sizes.extend([0] * (n_empty_buckets - 1))\n        else:\n            bucket_boundaries.append(upper_bound)\n\n        start = end\n        end = start + 1\n        # index start may be out of bounds\n        bucket_count = counts[start:end].sum()\n\n    bucket_boundaries = torch.tensor(bucket_boundaries)\n    bucket_sizes = torch.tensor(bucket_sizes)\n\n    return Buckets(bucket_boundaries, bucket_sizes)\n</code></pre>"},{"location":"API_reference/bionemo/testing/callbacks/","title":"Callbacks","text":""},{"location":"API_reference/bionemo/testing/lightning/","title":"Lightning","text":""},{"location":"API_reference/bionemo/testing/lightning/#bionemo.testing.lightning.get_random_microbatch","title":"<code>get_random_microbatch(microbatch_size, max_sequence_length, vocab_size, seed)</code>","text":"<p>Generate random microbatches for testing.</p> <p>Note that this follows the convention that token_logits are s,b, while other fields are b,s.</p> Source code in <code>bionemo/testing/lightning.py</code> <pre><code>def get_random_microbatch(\n    microbatch_size: int, max_sequence_length: int, vocab_size: int, seed: int\n) -&gt; Dict[str, Dict[str, torch.Tensor]]:\n    \"\"\"Generate random microbatches for testing.\n\n    Note that this follows the convention that token_logits are s,b, while other fields are b,s.\n    \"\"\"\n    generator = torch.Generator(device=torch.cuda.current_device()).manual_seed(seed)\n    labels = torch.randint(\n        low=0,\n        high=vocab_size,\n        size=(microbatch_size, max_sequence_length),\n        generator=generator,\n        device=torch.cuda.current_device(),\n    )  # [b s]\n    loss_mask = torch.randint(\n        low=1,\n        high=1 + 1,\n        size=(microbatch_size, max_sequence_length),\n        dtype=torch.long,\n        device=torch.cuda.current_device(),\n        generator=generator,\n    )  # [b s]\n    token_logits = torch.rand(\n        max_sequence_length, microbatch_size, vocab_size, device=torch.cuda.current_device(), generator=generator\n    )  # [s b v]\n    labels[loss_mask == 0] = -100  # propagate masking to labels\n    microbatch_output = {\n        \"batch\": {\"labels\": labels, \"loss_mask\": loss_mask},\n        \"forward_out\": {\"token_logits\": token_logits},\n    }\n    return microbatch_output\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/","title":"Megatron dataset compatibility","text":""},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetDistributedNondeterministic","title":"<code>DatasetDistributedNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetDistributedNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.DatasetLocallyNondeterministic","title":"<code>DatasetLocallyNondeterministic</code>","text":"<p>               Bases: <code>AssertionError</code></p> <p>Datasets are not locally deterministic.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>class DatasetLocallyNondeterministic(AssertionError):\n    \"\"\"Datasets are not locally deterministic.\"\"\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_compatible_with_megatron","title":"<code>assert_dataset_compatible_with_megatron(dataset, index=0, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.</p> Constraints tested <ul> <li>dataset[i] returns the same element regardless of device</li> <li>dataset[i] doesn't make calls to known problematic randomization procedures (currently <code>torch.manual_seed</code>).</li> </ul> <p>As more constraints are discovered, they should be added to this test.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_compatible_with_megatron(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index: Index = 0,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Make sure that a dataset passes some basic sanity checks for megatron determinism constraints.\n\n    Constraints tested:\n        * dataset[i] returns the same element regardless of device\n        * dataset[i] doesn't make calls to known problematic randomization procedures (currently `torch.manual_seed`).\n\n    As more constraints are discovered, they should be added to this test.\n    \"\"\"\n    # 1. Make sure the dataset is deterministic when you ask for the same elements.\n    n_elements = len(dataset)  # type: ignore\n    assert n_elements &gt; 0, \"Need one element or more to test\"\n    try:\n        assert_elements_equal(dataset[index], dataset[index])\n    except AssertionError as e_0:\n        raise DatasetLocallyNondeterministic(e_0)\n    with (\n        patch(\"torch.manual_seed\") as mock_manual_seed,\n        patch(\"torch.cuda.manual_seed\") as mock_cuda_manual_seed,\n        patch(\"torch.cuda.manual_seed_all\") as mock_cuda_manual_seed_all,\n    ):\n        _ = dataset[index]\n    if mock_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed.call_count &gt; 0 or mock_cuda_manual_seed_all.call_count:\n        raise DatasetDistributedNondeterministic(\n            \"You cannot safely use torch.manual_seed in a cluster with model parallelism. Use torch.Generator directly.\"\n            \" See https://github.com/NVIDIA/Megatron-LM/blob/dddecd19/megatron/core/tensor_parallel/random.py#L198-L199\"\n        )\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dataset_elements_not_equal","title":"<code>assert_dataset_elements_not_equal(dataset, index_a=0, index_b=1, assert_elements_equal=assert_dict_tensors_approx_equal)</code>","text":"<p>Test the case where two indices return different elements on datasets that employ randomness, like masking.</p> <p>NOTE: if you have a dataset without any kinds of randomness, just use the <code>assert_dataset_compatible_with_megatron</code> test and skip this one. This test is for the case when you want to test that a dataset that applies a random transform to your elements as a function of index actually does so with two different indices that map to the same underlying object. This test also runs <code>assert_dataset_compatible_with_megatron</code> behind the scenes so if you do this you do not need to also do the other.</p> <p>With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect different masks to be used for each call, even though the underlying object is the same. Again this test only applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0), for example. We expect those to return different random features.</p> <p>The idea for using this test effectively is to identify cases where you have two indices that return the same underlying object, but where you expect different randomization to be applied to each by the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[TensorCollectionOrTensor]</code> <p>dataset object with randomness (eg masking) to test.</p> required <code>index_a</code> <code>Index</code> <p>index for some element. Defaults to 0.</p> <code>0</code> <code>index_b</code> <code>Index</code> <p>index for a different element. Defaults to 1.</p> <code>1</code> <code>assert_elements_equal</code> <code>Callable[[TensorCollectionOrTensor, TensorCollectionOrTensor], None]</code> <p>Function to compare two returned batch elements. Defaults to <code>assert_dict_tensors_approx_equal</code> which works for both tensors and dictionaries of tensors.</p> <code>assert_dict_tensors_approx_equal</code> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dataset_elements_not_equal(\n    dataset: torch.utils.data.Dataset[TensorCollectionOrTensor],\n    index_a: Index = 0,\n    index_b: Index = 1,\n    assert_elements_equal: Callable[\n        [TensorCollectionOrTensor, TensorCollectionOrTensor], None\n    ] = assert_dict_tensors_approx_equal,\n):\n    \"\"\"Test the case where two indices return different elements on datasets that employ randomness, like masking.\n\n    NOTE: if you have a dataset without any kinds of randomness, just use the `assert_dataset_compatible_with_megatron`\n    test and skip this one. This test is for the case when you want to test that a dataset that applies a random\n    transform to your elements as a function of index actually does so with two different indices that map to the same\n    underlying object. This test also runs `assert_dataset_compatible_with_megatron` behind the scenes so if you\n    do this you do not need to also do the other.\n\n    With epoch upsampling approaches, some underlying index, say index=0, will be called multiple times by some wrapping\n    dataset object. For example if you have a dataset of length 1, and you wrap it in an up-sampler that maps it to\n    length 2 by mapping index 0 to 0 and 1 to 0, then in that wrapper we apply randomness to the result and we expect\n    different masks to be used for each call, even though the underlying object is the same. Again this test only\n    applies to a dataset that employs randomness. Another approach some of our datasets take is to use a special index\n    that captures both the underlying index, and the epoch index. This tuple of indices is used internally to seed the\n    mask. If that kind of dataset is used, then index_a could be (epoch=0, idx=0) and index_b could be (epoch=1, idx=0),\n    for example. We expect those to return different random features.\n\n    The idea for using this test effectively is to identify cases where you have two indices that return the same\n    underlying object, but where you expect different randomization to be applied to each by the dataset.\n\n    Args:\n        dataset: dataset object with randomness (eg masking) to test.\n        index_a: index for some element. Defaults to 0.\n        index_b: index for a different element. Defaults to 1.\n        assert_elements_equal: Function to compare two returned batch elements. Defaults to\n            `assert_dict_tensors_approx_equal` which works for both tensors and dictionaries of tensors.\n    \"\"\"\n    # 0, first sanity check for determinism/compatibility on idx0 and idx1\n    assert_dataset_compatible_with_megatron(dataset, index=index_a, assert_elements_equal=assert_elements_equal)\n    assert_dataset_compatible_with_megatron(dataset, index=index_b, assert_elements_equal=assert_elements_equal)\n    # 1, now check that index_a != index_b\n    with pytest.raises(AssertionError):\n        assert_elements_equal(dataset[index_a], dataset[index_b])\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_dataset_compatibility/#bionemo.testing.megatron_dataset_compatibility.assert_dict_tensors_approx_equal","title":"<code>assert_dict_tensors_approx_equal(actual, expected)</code>","text":"<p>Assert that two tensors are equal.</p> Source code in <code>bionemo/testing/megatron_dataset_compatibility.py</code> <pre><code>def assert_dict_tensors_approx_equal(actual: TensorCollectionOrTensor, expected: TensorCollectionOrTensor) -&gt; None:\n    \"\"\"Assert that two tensors are equal.\"\"\"\n    if isinstance(actual, dict) and isinstance(expected, dict):\n        a_keys, b_keys = actual.keys(), expected.keys()\n        assert a_keys == b_keys\n        for key in a_keys:\n            torch.testing.assert_close(actual=actual[key], expected=expected[key])\n    else:\n        torch.testing.assert_close(actual=actual, expected=expected)\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/","title":"Megatron parallel state utils","text":"<p>This package contains utilities for managing the state of distributed model parallelism in Megatron and Apex.</p> <p>In general you should just use the context manager <code>distributed_model_parallel_state</code> to manage the state of your test. This context manager will handle the setup and teardown of the distributed model parallel state for you.</p> <p>Example usage: <pre><code>from bionemo.testing import megatron_parallel_state_utils\n\ndef my_test():\n    with megatron_parallel_state_utils.distributed_model_parallel_state():\n        # your test code that requires megatron/apex parallel state to be set up here\n</code></pre></p>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.clean_parallel_state_context","title":"<code>clean_parallel_state_context()</code>","text":"<p>Puts you into a clean parallel state, and again tears it down at the end.</p> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef clean_parallel_state_context() -&gt; Iterator[None]:\n    \"\"\"Puts you into a clean parallel state, and again tears it down at the end.\"\"\"\n    try:\n        _teardown_apex_megatron_cuda()\n        yield\n    except Exception as e:\n        # TODO (@skothenhill) verify this is a problem and that this is a solution. Had issues with keyboard interrupts being ignored inside context manager.\n        raise Exception from e\n    finally:\n        _teardown_apex_megatron_cuda()\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.distributed_model_parallel_state","title":"<code>distributed_model_parallel_state(seed=42, devices=1, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=0, context_parallel_size=1, interactive=False)</code>","text":"<p>Context manager for handling creating and cleaning up distributed model parallel state for tests. Use like: with distributed_model_parallel_state():     # your test code here</p>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.distributed_model_parallel_state--after-the-block-your-state-is-cleaned-up","title":"After the block your state is cleaned up.","text":"Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef distributed_model_parallel_state(\n    seed: Optional[int] = 42,\n    devices: int = 1,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    pipeline_model_parallel_split_rank: int = 0,\n    context_parallel_size: int = 1,\n    interactive: bool = False,\n) -&gt; Iterator[None]:\n    \"\"\"Context manager for handling creating and cleaning up distributed model parallel state for tests.\n    Use like:\n    with distributed_model_parallel_state():\n        # your test code here\n    # After the block your state is cleaned up.\n    \"\"\"  # noqa: D205\n    initial_states: Optional[Any] = None\n\n    try:\n        _teardown_apex_megatron_cuda()\n        _initialize_distributed_parallel_state(\n            devices=devices,\n            tensor_model_parallel_size=tensor_model_parallel_size,\n            pipeline_model_parallel_size=pipeline_model_parallel_size,\n            pipeline_model_parallel_split_rank=pipeline_model_parallel_split_rank,\n            context_parallel_size=context_parallel_size,\n            interactive=interactive,\n        )\n        # Our goal is to set required state on entry, and then restore current state on exit for the RNGs.\n        #  there are two possibilities that are handled below:\n        # 1. If the RNG state is not initialized, we need to set it up and then\n        #     unset it on exit to restore the current state. We track that this is the case when `initial_states` is `None`.\n        # 2. If the RNG state is initialized, we need to track this state and reset it on exit to be what it was on entry.\n        #    We track that this is the case when `initial_states` is not `None`.\n        if tp_random.get_cuda_rng_tracker().is_initialized():\n            initial_states = tp_random.get_cuda_rng_tracker().get_states()\n        if seed is not None:\n            # Set the seed if provided, this case is valid whether or not the RNG had state previously.\n            #  on exit the RNG state will be restored to what it was on entry.\n            tp_random.model_parallel_cuda_manual_seed(seed)\n        else:\n            # This is the case where the RNG state is not initialized and no seed was provided.\n            #  We need to raise an error in this case, as we cannot restore the RNG state on exit and we need a seed\n            #  to initialize the RNG state to. This only happens if the user overrides the default seed and sets it\n            #  to None, and additionally if the RNG state was not initialized externally, as there is a default seed of 42.\n            if initial_states is None:\n                raise ValueError(\n                    \"You must provide a seed if the initial parallel state is unset. \"\n                    \"Either provide a seed or leave the default seed (rather setting to None) \"\n                    \"or initialize the RNG state externally.\"\n                )\n        yield\n    finally:\n        if initial_states is not None:\n            tp_random.get_cuda_rng_tracker().set_states(initial_states)\n        else:\n            # Reset to the unset state\n            tp_random.get_cuda_rng_tracker().reset()\n        _teardown_apex_megatron_cuda()\n</code></pre>"},{"location":"API_reference/bionemo/testing/megatron_parallel_state_utils/#bionemo.testing.megatron_parallel_state_utils.mock_distributed_parallel_state","title":"<code>mock_distributed_parallel_state(world_size=8, rank=0, tensor_model_parallel_size=1, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, context_parallel_size=1, expert_model_parallel_size=1, seed=42)</code>","text":"<p>A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.</p> Key functions that are mocked <ul> <li><code>torch.distributed.new_group</code> when <code>backend=\"gloo\"</code> which doesn't support a <code>backend=\"fake\"</code></li> <li><code>torch.distributed.destroy_process_group</code> when <code>backend=\"gloo\"</code> since new \"gloo\" groups are not actually made</li> <li><code>torch._C._cuda_setDevice</code> which changes the current device behind the scenes. We assign devices round-robin     to support <code>world_size &gt; torch.cuda.device_count()</code>.</li> </ul> <p>Outside of this mocking, a fake cluster is initialized using <code>backend=\"fake\"</code> in <code>torch.distributed</code>. This sets up     enough global state and environment for megatron to think that it is initializing a larger cluster with some     settings where the current context has some user defined rank. You can then test the megatron state on a     hypothetical rank in some large world size.</p> <p>Parameters:</p> Name Type Description Default <code>world_size</code> <code>int</code> <p>The world size (cluster size). Defaults to 8.</p> <code>8</code> <code>rank</code> <code>int</code> <p>the GPU number globally in the cluster. Defaults to 0.</p> <code>0</code> <code>tensor_model_parallel_size</code> <code>int</code> <p>tensor model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>pipeline_model_parallel_size</code> <code>int</code> <p>pipeline model parallel setting for megatron. Defaults to 1.</p> <code>1</code> <code>virtual_pipeline_model_parallel_size</code> <code>Optional[int]</code> <p>virtual pipeline model parallel size for megatron. Defaults to None.</p> <code>None</code> <code>context_parallel_size</code> <code>int</code> <p>context parallel size. Defaults to 1.</p> <code>1</code> <code>expert_model_parallel_size</code> <code>int</code> <p>expert model parallel size. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int | None</code> <p>seed for RNG state. Defaults to 42.</p> <code>42</code> Source code in <code>bionemo/testing/megatron_parallel_state_utils.py</code> <pre><code>@contextmanager\ndef mock_distributed_parallel_state(\n    world_size: int = 8,\n    rank: int = 0,\n    tensor_model_parallel_size: int = 1,\n    pipeline_model_parallel_size: int = 1,\n    virtual_pipeline_model_parallel_size: Optional[int] = None,\n    context_parallel_size: int = 1,\n    expert_model_parallel_size: int = 1,\n    seed: int | None = 42,\n):\n    \"\"\"A context manager that facilitates easy mocking of torch.distributed for an arbitrary GPU in a simulated cluster.\n\n    Key functions that are mocked:\n        * `torch.distributed.new_group` when `backend=\"gloo\"` which doesn't support a `backend=\"fake\"`\n        * `torch.distributed.destroy_process_group` when `backend=\"gloo\"` since new \"gloo\" groups are not actually made\n        * `torch._C._cuda_setDevice` which changes the current device behind the scenes. We assign devices round-robin\n            to support `world_size &gt; torch.cuda.device_count()`.\n\n    Outside of this mocking, a fake cluster is initialized using `backend=\"fake\"` in `torch.distributed`. This sets up\n        enough global state and environment for megatron to think that it is initializing a larger cluster with some\n        settings where the current context has some user defined rank. You can then test the megatron state on a\n        hypothetical rank in some large world size.\n\n    Args:\n        world_size: The world size (cluster size). Defaults to 8.\n        rank: the GPU number globally in the cluster. Defaults to 0.\n        tensor_model_parallel_size: tensor model parallel setting for megatron. Defaults to 1.\n        pipeline_model_parallel_size: pipeline model parallel setting for megatron. Defaults to 1.\n        virtual_pipeline_model_parallel_size: virtual pipeline model parallel size for megatron. Defaults to None.\n        context_parallel_size: context parallel size. Defaults to 1.\n        expert_model_parallel_size: expert model parallel size. Defaults to 1.\n        seed: seed for RNG state. Defaults to 42.\n    \"\"\"\n    # First set up mocks for torch.distributed state/info\n    ori_device_count = torch.cuda.device_count()\n    # Conditionally mock torch.distributed.new_group based on backend argument\n    ori_dist_new_group = torch.distributed.new_group\n\n    def mock_new_group(*args, **kwargs):\n        if kwargs.get(\"backend\") == \"gloo\":\n            # Return a specific mock if backend is 'gloo'\n            return MagicMock(name=\"gloo_group\")\n        else:\n            # Return another mock or a different behavior for other backends\n            return ori_dist_new_group(*args, **kwargs)\n\n    ori_destroy_pg = torch.distributed.destroy_process_group\n\n    def mock_destroy_gloo_group(pg=None):\n        if isinstance(pg, MagicMock):\n            return None\n        ori_destroy_pg(pg)\n\n    # The next mock is required to \"set the device\" to one that is greater than the number of actual GPUs\n    #  the consequence of this mock is that the device is always dev 0\n    ori_set_device = torch._C._cuda_setDevice\n\n    def mock_set_device(device):\n        if ori_device_count &gt; 0:\n            ori_set_device(device % ori_device_count)  # wrap around the request\n\n    with (\n        mock.patch(\"torch.distributed.new_group\", side_effect=mock_new_group),\n        mock.patch(\"torch.distributed.destroy_process_group\", side_effect=mock_destroy_gloo_group),\n        mock.patch(\"torch._C._cuda_setDevice\", side_effect=mock_set_device),\n    ):\n        # Next set up state etc\n        state_util = _MockMegatronParallelStateSingleton()  # static singleton class\n        state_util.world_size = world_size\n        state_util.rank = rank\n        initial_states: Optional[Any] = None\n        try:\n            state_util.set_world_size(world_size=world_size, rank=rank)\n            state_util.initialize_model_parallel(\n                tensor_model_parallel_size=tensor_model_parallel_size,\n                pipeline_model_parallel_size=pipeline_model_parallel_size,\n                virtual_pipeline_model_parallel_size=virtual_pipeline_model_parallel_size,\n                context_parallel_size=context_parallel_size,\n                expert_model_parallel_size=expert_model_parallel_size,\n            )\n            # Our goal is to set required state on entry, and then restore current state on exit for the RNGs.\n            #  there are two possibilities that are handled below:\n            # 1. If the RNG state is not initialized, we need to set it up and then\n            #     unset it on exit to restore the current state. We track that this is the case when `initial_states` is `None`.\n            # 2. If the RNG state is initialized, we need to track this state and reset it on exit to be what it was on entry.\n            #    We track that this is the case when `initial_states` is not `None`.\n            if tp_random.get_cuda_rng_tracker().is_initialized():\n                initial_states = tp_random.get_cuda_rng_tracker().get_states()\n            if seed is not None:\n                # Set the seed if provided, this case is valid whether or not the RNG had state previously.\n                #  on exit the RNG state will be restored to what it was on entry.\n                tp_random.model_parallel_cuda_manual_seed(seed)\n            else:\n                # This is the case where the RNG state is not initialized and no seed was provided.\n                #  We need to raise an error in this case, as we cannot restore the RNG state on exit and we need a seed\n                #  to initialize the RNG state to. This only happens if the user overrides the default seed and sets it\n                #  to None, and additionally if the RNG state was not initialized externally, as there is a default seed of 42.\n                if initial_states is None:\n                    raise ValueError(\n                        \"You must provide a seed if the initial parallel state is unset. \"\n                        \"Either provide a seed or leave the default seed (rather setting to None) \"\n                        \"or initialize the RNG state externally.\"\n                    )\n            yield\n        finally:\n            if initial_states is not None:\n                tp_random.get_cuda_rng_tracker().set_states(initial_states)\n            else:\n                # Reset to the unset state\n                tp_random.get_cuda_rng_tracker().reset()\n            state_util.destroy_model_parallel()\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/","title":"Testing callbacks","text":""},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback","title":"<code>AbstractStopAndGoCallback</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseInterruptedVsContinuousCallback</code></p> <p>Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.</p> <p>This base class provides utility methods to help streamline stop and go comparison.</p> Provided methods <ul> <li>init: initializes the callback with the given mode.</li> <li>get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.</li> </ul> Default behaviors <ul> <li>in stop mode, metadata is gotten and compared on_validation_epoch_end.</li> <li>in go mode, metadata is gotten and saved on_train_epoch_start.</li> </ul> <p>Override these behaviors if necessary.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class AbstractStopAndGoCallback(ABC, BaseInterruptedVsContinuousCallback):\n    \"\"\"Abstract base class for stop-and-go callback to compare metadata before pausing and after resuming training.\n\n    This base class provides utility methods to help streamline stop and go comparison.\n\n    Provided methods:\n        - __init__: initializes the callback with the given mode.\n        - get_metadata: abstract method that should be overridden to get metadata from the trainer and pl_module.\n\n    Default behaviors:\n        - in stop mode, metadata is gotten and compared on_validation_epoch_end.\n        - in go mode, metadata is gotten and saved on_train_epoch_start.\n\n    Override these behaviors if necessary.\n    \"\"\"\n\n    def __init__(self, mode: Mode = Mode.STOP):\n        \"\"\"Initialize StopAndGoCallback.\n\n        Args:\n            mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n        Notes:\n            User must override get_metadata to get metadata from the trainer and pl_module.\n        \"\"\"\n        if mode not in [Mode.STOP, Mode.RESUME]:\n            raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n        self.mode = mode\n        super().__init__()\n\n    @abstractmethod\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get metadata from trainer and pl_module.\"\"\"\n        raise NotImplementedError\n\n    def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if self.mode == Mode.RESUME:\n            self.data = self.get_metadata(trainer, pl_module)\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if not trainer.sanity_checking and self.mode == Mode.STOP:\n            self.data = self.get_metadata(trainer, pl_module)\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.__init__","title":"<code>__init__(mode=Mode.STOP)</code>","text":"<p>Initialize StopAndGoCallback.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.</p> <code>STOP</code> Notes <p>User must override get_metadata to get metadata from the trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self, mode: Mode = Mode.STOP):\n    \"\"\"Initialize StopAndGoCallback.\n\n    Args:\n        mode (str, optional): Mode to run in. Must be either Mode.STOP or Mode.RESUME. Defaults to Mode.STOP.\n\n    Notes:\n        User must override get_metadata to get metadata from the trainer and pl_module.\n    \"\"\"\n    if mode not in [Mode.STOP, Mode.RESUME]:\n        raise ValueError(f\"mode must be 'stop' or 'go', got {mode}\")\n    self.mode = mode\n    super().__init__()\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.AbstractStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>  <code>abstractmethod</code>","text":"<p>Get metadata from trainer and pl_module.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@abstractmethod\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get metadata from trainer and pl_module.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback","title":"<code>BaseInterruptedVsContinuousCallback</code>","text":"<p>               Bases: <code>Callback</code>, <code>CallbackMethods</code>, <code>IOMixin</code></p> <p>Base class for serializable stop-and-go callback to compare continuous to interrupted training.</p> <p>This class is used by extending a callback and collecting data into the <code>self.data</code> attribute. This data is then compared between continuous and interrupted training.</p> <p>See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class BaseInterruptedVsContinuousCallback(Callback, CallbackMethods, io.IOMixin):\n    \"\"\"Base class for serializable stop-and-go callback to compare continuous to interrupted training.\n\n    This class is used by extending a callback and collecting data into the `self.data` attribute. This data is then\n    compared between continuous and interrupted training.\n\n    See nemo.lightning.megatron_parallel.CallbackMethods for the available callback methods.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the callback.\"\"\"\n        self.data = []\n\n    def __deepcopy__(self, memo):\n        \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n        ...\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Don't actually attempt to copy this data when this callback is being serialized.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __deepcopy__(self, memo):\n    \"\"\"Don't actually attempt to copy this data when this callback is being serialized.\"\"\"\n    ...\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.BaseInterruptedVsContinuousCallback.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the callback.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the callback.\"\"\"\n    self.data = []\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback","title":"<code>ConsumedSamplesCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ConsumedSamplesCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            data_sampler = step.trainer.datamodule.data_sampler\n            consumed_samples = data_sampler.compute_consumed_samples(\n                step.trainer.global_step - step.trainer.datamodule.init_global_step\n            )\n            self.data.append(np.array(consumed_samples))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ConsumedSamplesCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        data_sampler = step.trainer.datamodule.data_sampler\n        consumed_samples = data_sampler.compute_consumed_samples(\n            step.trainer.global_step - step.trainer.datamodule.init_global_step\n        )\n        self.data.append(np.array(consumed_samples))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback","title":"<code>GlobalStepStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for global_step before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class GlobalStepStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for global_step before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.global_step))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.GlobalStepStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.global_step))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback","title":"<code>LearningRateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback for learning rate before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class LearningRateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback for learning rate before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get learning rate as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.LearningRateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get learning rate as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get learning rate as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(np.array(step.trainer.optimizers[0].param_groups[0][\"lr\"]))\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback","title":"<code>OptimizerStateCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Stop-and-go callback to check optimizer states before pausing and after resuming training.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class OptimizerStateCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Stop-and-go callback to check optimizer states before pausing and after resuming training.\"\"\"\n\n    def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n        \"\"\"Get optimizer states as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(\n                recursive_detach(\n                    [\n                        optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                        for optimizer in step.trainer.optimizers\n                    ]\n                )\n            )\n        return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.OptimizerStateCallback.on_megatron_step_start","title":"<code>on_megatron_step_start(step)</code>","text":"<p>Get optimizer states as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_start(self, step: MegatronStep) -&gt; MegatronStep:\n    \"\"\"Get optimizer states as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(\n            recursive_detach(\n                [\n                    optimizer.mcore_optimizer.optimizer.state_dict()[\"state\"]\n                    for optimizer in step.trainer.optimizers\n                ]\n            )\n        )\n    return step\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.RaiseAfterMetadataCallback","title":"<code>RaiseAfterMetadataCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>A callback that raises a StopAndGoException after the validation epoch.</p> <p>Use this callback for pytest based Stop and go tests.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class RaiseAfterMetadataCallback(Callback):\n    \"\"\"A callback that raises a StopAndGoException after the validation epoch.\n\n    Use this callback for pytest based Stop and go tests.\n    \"\"\"\n\n    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n        if trainer.sanity_checking:\n            return\n        raise StopAndGoException()\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback","title":"<code>TrainInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback","title":"<code>TrainLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback","title":"<code>TrainOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.training:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.training:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback","title":"<code>TrainValInitConsumedSamplesStopAndGoCallback</code>","text":"<p>               Bases: <code>AbstractStopAndGoCallback</code></p> <p>Stop-and-go callback to check consumed samples before pausing and after resuming training.</p> <p>This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint resumption.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class TrainValInitConsumedSamplesStopAndGoCallback(AbstractStopAndGoCallback):\n    \"\"\"Stop-and-go callback to check consumed samples before pausing and after resuming training.\n\n    This is currently the only callback that doesn't fit with the new pattern of directly comparing continuous and\n    interrupted training, since the dataloaders don't track their consumed_samples before and after checkpoint\n    resumption.\n    \"\"\"\n\n    @override\n    def get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n        train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n        val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n        return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback.get_metadata","title":"<code>get_metadata(trainer, pl_module)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>@override\ndef get_metadata(self, trainer: Trainer, pl_module: LightningModule) -&gt; Any:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    # return trainer.datamodule.state_dict()[\"consumed_samples\"]  # TODO why state_dict can be empty despite working lines below\n    train_data_sampler: MegatronPretrainingSampler = trainer.train_dataloader.batch_sampler\n    val_data_sampler: MegatronPretrainingSampler = trainer.val_dataloaders.batch_sampler\n    return train_data_sampler.consumed_samples, val_data_sampler.consumed_samples\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback","title":"<code>ValidInputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation input samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidInputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation input samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidInputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(batch))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback","title":"<code>ValidLossCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect training loss samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidLossCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect training loss samples for comparison.\"\"\"\n\n    def on_megatron_step_end(\n        self,\n        step: MegatronStep,\n        microbatch_outputs: List[Any],\n        reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidLossCallback.on_megatron_step_end","title":"<code>on_megatron_step_end(step, microbatch_outputs, reduced=None)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_step_end(\n    self,\n    step: MegatronStep,\n    microbatch_outputs: List[Any],\n    reduced: Optional[Union[torch.Tensor, Dict[str, torch.Tensor]]] = None,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(reduced))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback","title":"<code>ValidOutputCallback</code>","text":"<p>               Bases: <code>BaseInterruptedVsContinuousCallback</code></p> <p>Collect validation output samples for comparison.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>class ValidOutputCallback(BaseInterruptedVsContinuousCallback):\n    \"\"\"Collect validation output samples for comparison.\"\"\"\n\n    def on_megatron_microbatch_end(\n        self,\n        step: MegatronStep,\n        batch: DataT,\n        forward_callback: \"MegatronLossReduction\",\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Get consumed samples as metadata.\"\"\"\n        if step.trainer.validating:\n            self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/testing_callbacks/#bionemo.testing.testing_callbacks.ValidOutputCallback.on_megatron_microbatch_end","title":"<code>on_megatron_microbatch_end(step, batch, forward_callback, output)</code>","text":"<p>Get consumed samples as metadata.</p> Source code in <code>bionemo/testing/testing_callbacks.py</code> <pre><code>def on_megatron_microbatch_end(\n    self,\n    step: MegatronStep,\n    batch: DataT,\n    forward_callback: \"MegatronLossReduction\",\n    output: Any,\n) -&gt; None:\n    \"\"\"Get consumed samples as metadata.\"\"\"\n    if step.trainer.validating:\n        self.data.append(recursive_detach(output))\n</code></pre>"},{"location":"API_reference/bionemo/testing/torch/","title":"Torch","text":""},{"location":"API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_assert_approx_equal","title":"<code>recursive_assert_approx_equal(x, y, atol=0.0001, rtol=0.0001)</code>","text":"<p>Assert that all tensors in a nested structure are approximately equal.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_assert_approx_equal(x, y, atol=1e-4, rtol=1e-4):\n    \"\"\"Assert that all tensors in a nested structure are approximately equal.\"\"\"\n    if isinstance(x, torch.Tensor):\n        torch.testing.assert_close(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, np.ndarray):\n        np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n    elif isinstance(x, (list, tuple)):\n        assert len(x) == len(y), f\"Length mismatch: {len(x)} vs {len(y)}\"\n        for x_item, y_item in zip(x, y):\n            recursive_assert_approx_equal(x_item, y_item, atol=atol, rtol=rtol)\n    elif isinstance(x, dict):\n        assert x.keys() == y.keys()\n        for key in x:\n            recursive_assert_approx_equal(x[key], y[key], atol=atol, rtol=rtol)\n    else:\n        assert x == y\n</code></pre>"},{"location":"API_reference/bionemo/testing/torch/#bionemo.testing.torch.recursive_detach","title":"<code>recursive_detach(x)</code>","text":"<p>Detach all tensors in a nested structure.</p> Source code in <code>bionemo/testing/torch.py</code> <pre><code>def recursive_detach(x):\n    \"\"\"Detach all tensors in a nested structure.\"\"\"\n    if isinstance(x, torch.Tensor):\n        return x.detach().cpu()\n    elif isinstance(x, (list, tuple)):\n        return type(x)(recursive_detach(item) for item in x)\n    elif isinstance(x, dict):\n        return {key: recursive_detach(value) for key, value in x.items()}\n    else:\n        return x\n</code></pre>"},{"location":"API_reference/bionemo/testing/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_correlation_above_value","title":"<code>assert_matrix_correlation_above_value(actual, expected, mask=None, min_correlation=0.95, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_correlation_above_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    min_correlation: float = 0.95,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    corr = torch.corrcoef(torch.stack([masked_actual, masked_expected]))[0, 1]\n    if corr &lt; min_correlation:\n        raise AssertionError(f\"Correlation below threshold: {corr} &lt; {min_correlation}. {msg}\")\n</code></pre>"},{"location":"API_reference/bionemo/testing/utils/#bionemo.testing.utils.assert_matrix_mape_below_value","title":"<code>assert_matrix_mape_below_value(actual, expected, mask=None, max_mape=0.1, eps=0.001, msg='')</code>","text":"<p>Assert that two tensors are close with a root mean squared error (RMSE)     relative to the scaled root mean square values for each matrix. This tells     you if the RMSE implies that the two matrices are more similar to eachother     as-is than would be the case if values were randomly permuted.</p> <p>Parameters:</p> Name Type Description Default <code>actual</code> <code>Tensor</code> <p>The actual tensor.</p> required <code>expected</code> <code>Tensor</code> <p>The expected tensor.</p> required <code>mask</code> <code>Optional[Tensor]</code> <p>If there are only some values you want to compare, apply this mask and RMSE will be computed on the unmasked items only.</p> <code>None</code> <code>min_relative_rmse</code> <p>The relative tolerance parameter.</p> required Source code in <code>bionemo/testing/utils.py</code> <pre><code>def assert_matrix_mape_below_value(  # noqa: D417\n    actual: torch.Tensor,\n    expected: torch.Tensor,\n    mask: Optional[torch.Tensor] = None,\n    max_mape: float = 0.1,\n    eps: float = 1e-3,\n    msg: str = \"\",\n) -&gt; None:\n    \"\"\"Assert that two tensors are close with a root mean squared error (RMSE)\n        relative to the scaled root mean square values for each matrix. This tells\n        you if the RMSE implies that the two matrices are more similar to eachother\n        as-is than would be the case if values were randomly permuted.\n\n    Args:\n        actual: The actual tensor.\n        expected: The expected tensor.\n        mask: If there are only some values you want to compare,\n            apply this mask and RMSE will be computed on the unmasked items only.\n        min_relative_rmse: The relative tolerance parameter.\n    \"\"\"  # noqa: D205\n    if mask is None:\n        mask = torch.ones_like(actual)\n    else:\n        if len(mask.shape) &lt; len(actual.shape):\n            mask = mask[..., None]\n    masked_actual = actual[mask.expand_as(actual).to(bool)]\n    masked_expected = expected[mask.expand_as(expected).to(bool)]\n    mape = (\n        torch.mean(\n            torch.abs(masked_actual - masked_expected)\n            / torch.maximum(torch.abs(masked_expected), torch.zeros_like(masked_expected) + eps)\n        )\n        * 100.0\n    )\n    if mape &gt; max_mape:\n        raise AssertionError(f\"MAPE below threshold: {mape} &gt; {max_mape}. {msg}\")\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/","title":"BioNeMo test data management","text":"<p>This library manages the downloading and caching of large or binary data files used in the documentation or test suite. These files should not be committed directly to the repo, and instead should be loaded at test-time when they are needed.</p> <p>We currently support two locations for test data or saved models:</p> SwiftStack <p>SwiftStack or <code>pbss</code> is an NVIDIA-internal, s3-compatible object store that allows for very large data and fast, parallel read/writes. Most critically, <code>pbss</code> can be uploaded to without legal approvals for dataset redistribution. These files will not be accessible by external collaborators.</p> NGC <p>NGC hosts containers, models, and resources, some of which require authentication and others that are generally available. This library uses the model and resource types to save test data and reference model weights. These items are accessible by external collaborators, but require legal approval before re-distributing test data.</p>"},{"location":"API_reference/bionemo/testing/data/#loading-test-or-example-data","title":"Loading test or example data","text":"<p>Test data are specified via yaml files in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code>. As an example, in <code>esm2.yaml</code>:</p> <pre><code>- tag: nv_650m:1.0\n  ngc: \"nvidia/clara/esm2nv650m:1.0\"\n  ngc_registry: model\n  pbss: \"s3://bionemo-ci/models/esm2nv_650M_converted.nemo\"\n  sha256: 1e38063cafa808306329428dd17ea6df78c9e5d6b3d2caf04237c555a1f131b7\n  owner: Farhad Ramezanghorbani &lt;farhadr@nvidia.com&gt;\n  description: &gt;\n    A pretrained 650M parameter ESM-2 model.\n    See https://ngc.nvidia.com/catalog/models/nvidia:clara:esm2nv650m.\n</code></pre> <p>To load these model weights during a test, use the load function with the filename and tag of the desired asset, which returns a path a the specified file:</p> <pre><code>path_to_my_checkpoint = load(\"esm2/nv_650m:1.0\")\nconfig = ESM2Config(nemo1_ckpt_path=path_to_my_checkpoint)\n</code></pre> <p>If this function is called without the data available on the local machine, it will be fetched from the default source (currently <code>pbss</code>.) Otherwise, it will return the cached directory. To download with NGC, pass <code>source=\"ngc\"</code> to load.</p>"},{"location":"API_reference/bionemo/testing/data/#file-unpacking-andor-decompression","title":"File unpacking and/or decompression","text":"<p>All test artifacts are individual files. If a zip or tar archive is specified, it will be unpacked automatically, and the path to the directory will be returned via load. Compressed files ('gzip', 'bz2', or 'xz') are automatically decompressed before they are returned. The file's compression and/or archive format is determined based on the filename specified in the <code>pbss</code> URL.</p> <p>Files in NGC resources</p> <p>NGC resources are folders, i.e., they may contain multiple files per resource. load will only download the filename matching the stem of the <code>pbss</code> url. The same NGC resource can therefore be used to host multiple test assets that are used independently.</p>"},{"location":"API_reference/bionemo/testing/data/#adding-new-test-assets","title":"Adding new test assets","text":"<p>To add new data, first ensure that the data is available from either NGC or <code>pbss</code>. Next, extend or create a new yaml file in <code>sub-packages/bionemo-testing/src/bionemo/testing/data/resources</code> with the required information. Owner emails must be provided for all assets. The description and <code>ngc</code> fields are currently optional. If the <code>sha256</code> is left unspecified, <code>pooch</code> will report the downloaded file's sha when loaded.</p> <p>Warning</p> <p>SHAs should be provided for all files to ensure the download completes correctly, and to invalidate caches if the files change.</p>"},{"location":"API_reference/bionemo/testing/data/esm2/","title":"Esm2","text":""},{"location":"API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_parquet_train_val_inputs","title":"<code>create_mock_parquet_train_val_inputs(tmp_path)</code>","text":"<p>Create a mock protein train and val cluster parquet.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_parquet_train_val_inputs(tmp_path):\n    \"\"\"Create a mock protein train and val cluster parquet.\"\"\"\n    train_cluster_path = tmp_path / \"train_clusters.parquet\"\n    train_clusters = pd.DataFrame(\n        {\n            \"ur90_id\": [[\"UniRef90_A\"], [\"UniRef90_B\", \"UniRef90_C\"]],\n        }\n    )\n    train_clusters.to_parquet(train_cluster_path)\n\n    valid_cluster_path = tmp_path / \"valid_clusters.parquet\"\n    valid_clusters = pd.DataFrame(\n        {\n            \"ur50_id\": [\"UniRef50_A\", \"UniRef50_B\", \"UniRef90_A\", \"UniRef90_B\"],\n        }\n    )\n    valid_clusters.to_parquet(valid_cluster_path)\n    return train_cluster_path, valid_cluster_path\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/esm2/#bionemo.testing.data.esm2.create_mock_protein_dataset","title":"<code>create_mock_protein_dataset(tmp_path)</code>","text":"<p>Create a mock protein dataset.</p> Source code in <code>bionemo/testing/data/esm2.py</code> <pre><code>def create_mock_protein_dataset(tmp_path):\n    \"\"\"Create a mock protein dataset.\"\"\"\n    db_file = tmp_path / \"protein_dataset.db\"\n    conn = sqlite3.connect(str(db_file))\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"\"\"\n        CREATE TABLE protein (\n            id TEXT PRIMARY KEY,\n            sequence TEXT\n        )\n    \"\"\"\n    )\n\n    proteins = [\n        (\"UniRef90_A\", \"ACDEFGHIKLMNPQRSTVWY\"),\n        (\"UniRef90_B\", \"DEFGHIKLMNPQRSTVWYAC\"),\n        (\"UniRef90_C\", \"MGHIKLMNPQRSTVWYACDE\"),\n        (\"UniRef50_A\", \"MKTVRQERLKSIVRI\"),\n        (\"UniRef50_B\", \"MRILERSKEPVSGAQLA\"),\n    ]\n    cursor.executemany(\"INSERT INTO protein VALUES (?, ?)\", proteins)\n\n    conn.commit()\n    conn.close()\n\n    return db_file\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/","title":"Load","text":""},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.NGCDownloader","title":"<code>NGCDownloader</code>  <code>dataclass</code>","text":"<p>A class to download files from NGC in a Pooch-compatible way.</p> <p>NGC downloads are typically structured as directories, while pooch expects a single file. This class downloads a single file from an NGC directory and moves it to the desired location.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>@dataclass\nclass NGCDownloader:\n    \"\"\"A class to download files from NGC in a Pooch-compatible way.\n\n    NGC downloads are typically structured as directories, while pooch expects a single file. This class\n    downloads a single file from an NGC directory and moves it to the desired location.\n    \"\"\"\n\n    filename: str\n    ngc_registry: Literal[\"model\", \"resource\"]\n\n    def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n        \"\"\"Download a file from NGC.\"\"\"\n        client = default_ngc_client()\n\n        download_fns = {\n            \"model\": client.registry.model.download_version,\n            \"resource\": client.registry.resource.download_version,\n        }\n\n        output_file = Path(output_file)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n\n        # NGC seems to always download to a specific directory that we can't specify ourselves.\n        ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n        with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n            download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n            shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.NGCDownloader.__call__","title":"<code>__call__(url, output_file, _)</code>","text":"<p>Download a file from NGC.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def __call__(self, url: str, output_file: str | Path, _: pooch.Pooch) -&gt; None:\n    \"\"\"Download a file from NGC.\"\"\"\n    client = default_ngc_client()\n\n    download_fns = {\n        \"model\": client.registry.model.download_version,\n        \"resource\": client.registry.resource.download_version,\n    }\n\n    output_file = Path(output_file)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n\n    # NGC seems to always download to a specific directory that we can't specify ourselves.\n    ngc_dirname = Path(url).name.replace(\":\", \"_v\")\n\n    with tempfile.TemporaryDirectory(dir=output_file.parent) as temp_dir:\n        download_fns[self.ngc_registry](url, temp_dir, file_patterns=[self.filename])\n        shutil.move(Path(temp_dir) / ngc_dirname / self.filename, output_file)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_ngc_client","title":"<code>default_ngc_client()</code>","text":"<p>Create a default NGC client.</p> <p>This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def default_ngc_client() -&gt; ngcsdk.Client:\n    \"\"\"Create a default NGC client.\n\n    This should load the NGC API key from ~/.ngc/config, or from environment variables passed to the docker container.\n    \"\"\"\n    return ngcsdk.Client()\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.default_pbss_client","title":"<code>default_pbss_client()</code>","text":"<p>Create a default S3 client for PBSS.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def default_pbss_client():\n    \"\"\"Create a default S3 client for PBSS.\"\"\"\n    retry_config = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})\n    return boto3.client(\"s3\", endpoint_url=\"https://pbss.s8k.io\", config=retry_config)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.entrypoint","title":"<code>entrypoint()</code>","text":"<p>Allows a user to get a specific artifact from the command line.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def entrypoint():\n    \"\"\"Allows a user to get a specific artifact from the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Retrieve the local path to the requested artifact name or list resources.\"\n    )\n\n    # Create mutually exclusive group\n    group = parser.add_mutually_exclusive_group(required=True)\n\n    # Add the argument for artifact name, which is required if --list-resources is not used\n    group.add_argument(\"artifact_name\", type=str, nargs=\"?\", help=\"Name of the artifact\")\n\n    # Add the --list-resources option\n    group.add_argument(\n        \"--list-resources\", action=\"store_true\", default=False, help=\"List all available artifacts and then exit.\"\n    )\n\n    # Add the --source option\n    parser.add_argument(\n        \"--source\",\n        type=str,\n        choices=[\"pbss\", \"ngc\"],\n        default=\"ngc\",\n        help='Backend to use, Internal NVIDIA users can set this to \"pbss\".',\n    )\n\n    parser.add_argument(\n        \"--all\",\n        action=\"store_true\",\n        default=False,\n        help=\"Download all resources. Ignores all other options.\",\n    )\n    args = parser.parse_args()\n    maybe_error = main(\n        download_all=args.all,\n        list_resources=args.list_resources,\n        artifact_name=args.artifact_name,\n        source=args.source,\n    )\n    if maybe_error is not None:\n        parser.error(maybe_error)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.load","title":"<code>load(model_or_data_tag, source='pbss', resources=None, cache_dir=None)</code>","text":"<p>Download a resource from PBSS or NGC.</p> <p>Parameters:</p> Name Type Description Default <code>model_or_data_tag</code> <code>str</code> <p>A pointer to the desired resource. Must be a key in the resources dictionary.</p> required <code>source</code> <code>Literal['ngc', 'pbss']</code> <p>Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".</p> <code>'pbss'</code> <code>resources</code> <code>dict[str, Resource] | None</code> <p>A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the desired tag was not found, or if an NGC url was requested but not provided.</p> <p>Returns:</p> Type Description <code>Path</code> <p>A Path object pointing either at the downloaded file, or at a decompressed folder containing the</p> <code>Path</code> <p>file(s).</p> <p>Examples:</p> <p>For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:</p> <pre><code>&gt;&gt;&gt; load(\"filename/tag\")\nPosixPath(/tmp/bionemo/downloaded-file-name)\n</code></pre> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def load(\n    model_or_data_tag: str,\n    source: Literal[\"ngc\", \"pbss\"] = \"pbss\",\n    resources: dict[str, Resource] | None = None,\n    cache_dir: Path | None = None,\n) -&gt; Path:\n    \"\"\"Download a resource from PBSS or NGC.\n\n    Args:\n        model_or_data_tag: A pointer to the desired resource. Must be a key in the resources dictionary.\n        source: Either \"pbss\" (NVIDIA-internal download) or \"ngc\" (NVIDIA GPU Cloud). Defaults to \"pbss\".\n        resources: A custom dictionary of resources. If None, the default resources will be used. (Mostly for testing.)\n        cache_dir: The directory to store downloaded files. Defaults to BIONEMO_CACHE_DIR. (Mostly for testing.)\n\n    Raises:\n        ValueError: If the desired tag was not found, or if an NGC url was requested but not provided.\n\n    Returns:\n        A Path object pointing either at the downloaded file, or at a decompressed folder containing the\n        file(s).\n\n    Examples:\n        For a resource specified in 'filename.yaml' with tag 'tag', the following will download the file:\n        &gt;&gt;&gt; load(\"filename/tag\")\n        PosixPath(/tmp/bionemo/downloaded-file-name)\n    \"\"\"\n    if resources is None:\n        resources = get_all_resources()\n\n    if cache_dir is None:\n        cache_dir = BIONEMO_CACHE_DIR\n\n    if model_or_data_tag not in resources:\n        raise ValueError(f\"Resource '{model_or_data_tag}' not found.\")\n\n    if source == \"ngc\" and resources[model_or_data_tag].ngc is None:\n        raise ValueError(f\"Resource '{model_or_data_tag}' does not have an NGC URL.\")\n\n    resource = resources[model_or_data_tag]\n    filename = str(resource.pbss).split(\"/\")[-1]\n\n    extension = \"\".join(Path(filename).suffixes)\n    processor = _get_processor(extension, resource.unpack, resource.decompress)\n\n    if source == \"pbss\":\n        download_fn = _s3_download\n        url = resource.pbss\n\n    elif source == \"ngc\":\n        assert resource.ngc_registry is not None\n        download_fn = NGCDownloader(filename=filename, ngc_registry=resource.ngc_registry)\n        url = resource.ngc\n\n    else:\n        raise ValueError(f\"Source '{source}' not supported.\")\n\n    download = pooch.retrieve(\n        url=str(url),\n        known_hash=resource.sha256,\n        path=cache_dir,\n        downloader=download_fn,\n        processor=processor,\n    )\n\n    # Pooch by default returns a list of unpacked files if they unpack a zipped or tarred directory. Instead of that, we\n    # just want the unpacked, parent folder.\n    if isinstance(download, list):\n        return Path(processor.extract_dir)  # type: ignore\n\n    else:\n        return Path(download)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.main","title":"<code>main(download_all, list_resources, artifact_name, source)</code>","text":"<p>Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def main(\n    download_all: bool, list_resources: bool, artifact_name: str, source: Literal[\"pbss\", \"ngc\"]\n) -&gt; Optional[str]:\n    \"\"\"Main download script logic: parameters are 1:1 with CLI flags. Returns string describing error on failure.\"\"\"\n    if download_all:\n        print(\"Downloading all resources:\", file=sys.stderr)\n        print_resources(output_source=sys.stderr)\n        print(\"-\" * 80, file=sys.stderr)\n\n        resource_to_local: dict[str, Path] = {}\n        for resource_name in tqdm(\n            sorted(get_all_resources()),\n            desc=\"Downloading Resources\",\n        ):\n            with contextlib.redirect_stdout(sys.stderr):\n                local_path = load(resource_name, source=source)\n            resource_to_local[resource_name] = local_path\n\n        print(\"-\" * 80, file=sys.stderr)\n        print(\"All resources downloaded:\", file=sys.stderr)\n        for resource_name, local_path in sorted(resource_to_local.items()):\n            print(f\"  {resource_name}: {str(local_path.absolute())}\", file=sys.stderr)\n\n    elif list_resources:\n        print_resources(output_source=sys.stdout)\n\n    elif artifact_name is not None and len(artifact_name) &gt; 0:\n        # Get the local path for the provided artifact name\n        with contextlib.redirect_stdout(sys.stderr):\n            local_path = load(artifact_name, source=source)\n\n        # Print the result =&gt; CLI use assumes that we can get the single downloaded resource's path on STDOUT\n        print(str(local_path.absolute()))\n\n    else:\n        return \"You must provide an artifact name if --list-resources or --all is not set!\"\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/load/#bionemo.testing.data.load.print_resources","title":"<code>print_resources(*, output_source=sys.stdout)</code>","text":"<p>Prints all available downloadable resources &amp; their sources to STDOUT.</p> Source code in <code>bionemo/testing/data/load.py</code> <pre><code>def print_resources(*, output_source: TextIO = sys.stdout) -&gt; None:\n    \"\"\"Prints all available downloadable resources &amp; their sources to STDOUT.\"\"\"\n    print(\"#resource_name\\tsource_options\", file=output_source)\n    for resource_name, resource in sorted(get_all_resources().items()):\n        sources = []\n        if resource.ngc is not None:\n            sources.append(\"ngc\")\n        if resource.pbss is not None:\n            sources.append(\"pbss\")\n        print(f\"{resource_name}\\t{','.join(sources)}\", file=output_source)\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/resource/","title":"Resource","text":""},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource","title":"<code>Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class that represents a remote resource for downloading and caching test data.</p> Source code in <code>bionemo/testing/data/resource.py</code> <pre><code>class Resource(pydantic.BaseModel):\n    \"\"\"Class that represents a remote resource for downloading and caching test data.\"\"\"\n\n    model_config = pydantic.ConfigDict(use_attribute_docstrings=True)\n\n    tag: Annotated[str, pydantic.StringConstraints(pattern=r\"^[^/]*/[^/]*$\")]  # Only slash between filename and tag.\n    \"\"\"A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").\"\"\"\n\n    ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None\n    \"\"\"The NGC URL for the resource.\n\n    Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.\n    \"\"\"\n\n    ngc_registry: Literal[\"model\", \"resource\"] | None = None\n    \"\"\"The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.\"\"\"\n\n    pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[\"s3\"])]\n    \"\"\"The PBSS (NVIDIA-internal) URL of the resource.\"\"\"\n\n    sha256: str | None\n    \"\"\"The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).\"\"\"\n\n    owner: pydantic.NameEmail\n    \"\"\"The owner or primary point of contact for the resource, in the format \"Name &lt;email&gt;\".\"\"\"\n\n    description: str | None = None\n    \"\"\"A description of the file(s).\"\"\"\n\n    unpack: Literal[False, None] = None\n    \"\"\"Whether the resource should be unpacked after download. If None, will defer to the file extension.\"\"\"\n\n    decompress: Literal[False, None] = None\n    \"\"\"Whether the resource should be decompressed after download. If None, will defer to the file extension.\"\"\"\n\n    @pydantic.model_validator(mode=\"after\")\n    def _validate_ngc_registry(self):\n        if self.ngc and not self.ngc_registry:\n            raise ValueError(f\"ngc_registry must be provided if ngc is not None: {self.tag}\")\n        return self\n</code></pre>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.decompress","title":"<code>decompress: Literal[False, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be decompressed after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.description","title":"<code>description: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A description of the file(s).</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc","title":"<code>ngc: Annotated[str, pydantic.AfterValidator(_validate_ngc_resource)] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC URL for the resource.</p> <p>Should be in format [org/[team/]]name[:version]. If None, the resource is not available on NGC.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.ngc_registry","title":"<code>ngc_registry: Literal['model', 'resource'] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The NGC resource type (model or resource) for the data. Must be provided if ngc is not None.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.owner","title":"<code>owner: pydantic.NameEmail</code>  <code>instance-attribute</code>","text":"<p>The owner or primary point of contact for the resource, in the format \"Name \"."},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.pbss","title":"<code>pbss: Annotated[pydantic.AnyUrl, pydantic.UrlConstraints(allowed_schemes=[s3])]</code>  <code>instance-attribute</code>","text":"<p>The PBSS (NVIDIA-internal) URL of the resource.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.sha256","title":"<code>sha256: str | None</code>  <code>instance-attribute</code>","text":"<p>The SHA256 checksum of the resource. If None, the SHA will not be checked on download (not recommended).</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.tag","title":"<code>tag: Annotated[str, pydantic.StringConstraints(pattern='^[^/]*/[^/]*$')]</code>  <code>instance-attribute</code>","text":"<p>A unique identifier for the resource. The file(s) will be accessible via load(\"filename/tag\").</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.Resource.unpack","title":"<code>unpack: Literal[False, None] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the resource should be unpacked after download. If None, will defer to the file extension.</p>"},{"location":"API_reference/bionemo/testing/data/resource/#bionemo.testing.data.resource.get_all_resources","title":"<code>get_all_resources(resource_path=None)</code>  <code>cached</code>","text":"<p>Return a dictionary of all resources.</p> Source code in <code>bionemo/testing/data/resource.py</code> <pre><code>@functools.cache\ndef get_all_resources(resource_path: Path | None = None) -&gt; dict[str, Resource]:\n    \"\"\"Return a dictionary of all resources.\"\"\"\n    if not resource_path:\n        resource_path = Path(files(\"bionemo.testing.data\").joinpath(\"resources\"))  # type: ignore\n\n    resources_files = itertools.chain(resource_path.glob(\"*.yaml\"), resource_path.glob(\"*.yml\"))\n\n    all_resources = [resource for file in resources_files for resource in _parse_resource_file(file)]\n\n    resource_list = pydantic.TypeAdapter(list[Resource]).validate_python(all_resources)\n    resource_dict = {resource.tag: resource for resource in resource_list}\n\n    if len(resource_dict) != len(resource_list):\n        # Show the # of and which ones are duplicated so that a user can begin debugging and resolve the issue.\n        tag_counts = Counter([resource.tag for resource in resource_list])\n        raise ValueError(f\"Duplicate resource tags found!: {[tag for tag, count in tag_counts.items() if count &gt; 1]}\")\n\n    return resource_dict\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/mode/","title":"Mode","text":""},{"location":"API_reference/bionemo/testing/harnesses/mode/#bionemo.testing.harnesses.mode.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Mode for stop-go testing.</p> Source code in <code>bionemo/testing/harnesses/mode.py</code> <pre><code>class Mode(Enum):\n    \"\"\"Mode for stop-go testing.\"\"\"\n\n    STOP = auto()\n    RESUME = auto()\n    CONTINUOUS = auto()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/","title":"Stop and go","text":""},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness","title":"<code>StopAndGoHarness</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for testing consistency between interrupted and continuous training.</p> <p>Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and continuous cases.</p> <p>By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are compared. Users can add additional metrics by adding new callbacks to <code>cls.callbacks</code> and associated test functions.</p> Stop and go tests act as follows <ul> <li>setup a clean model for a brief training run, set callbacks to track.</li> <li>interrupt training via the StopAndGoException in the callback Raise.</li> <li>train the model resumed from the checkpoint with the same set of callbacks.</li> <li>train the model continuously without interruption with a new set of the same callbacks.</li> <li>compare each pair of interrupted and continuous callbacks to check for equality.</li> </ul> Considerations when implementing this class <ul> <li>The derived test name should start with <code>Test</code>, and test methods should start with <code>test_</code> to enable pytest   discovery.</li> <li>devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain     datasets expect a known global batch size, which depends on the number of devices and conditional tensor     model parallel/ pipeline model parallel settings. By default, we are testing only on single device without     parallelism.</li> <li>'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an     example, it may be useful to implement a test that stops and resumes.<ul> <li>changing callbacks to test metadata integrity (core feature of stop-and-go tests).</li> <li>changing the model construction to use different hyperparameters.</li> <li>... etc Each of the above tests cases may be useful for automated testing of various expected behavior.</li> </ul> </li> <li>stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual   tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.</li> </ul> <p>Attributes:</p> Name Type Description <code>root_dir</code> <p>The root directory.</p> <code>val_check_interval</code> <code>int</code> <p>The validation check interval. Stored as an attribute to ensure consistency.</p> <code>exp_name</code> <code>str</code> <p>The experiment name.</p> <code>extra_metrics_dict</code> <code>str</code> <p>A dictionary of metrics and their corresponding functions.</p> <p>See Also: bionemo.testing.callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>class StopAndGoHarness(ABC):\n    \"\"\"Abstract base class for testing consistency between interrupted and continuous training.\n\n    Users should override cls.setup_model and update cls.setup_class to customize the downstream test cases. Metadata\n    are collected through callbacks and users can add new unit tests by comparing the metadata for the interrupted and\n    continuous cases.\n\n    By default, learning rate, global step, optimizer state, consumed samples, input and output tensors, and loss are\n    compared. Users can add additional metrics by adding new callbacks to `cls.callbacks` and associated test functions.\n\n    Stop and go tests act as follows:\n        - setup a clean model for a brief training run, set callbacks to track.\n        - interrupt training via the StopAndGoException in the callback Raise.\n        - train the model resumed from the checkpoint with the same set of callbacks.\n        - train the model continuously without interruption with a new set of the same callbacks.\n        - compare each pair of interrupted and continuous callbacks to check for equality.\n\n    Considerations when implementing this class:\n        - The derived test name should start with `Test`, and test methods should start with `test_` to enable pytest\n          discovery.\n        - devices, pipeline_model_parallel, and tensor_model_parallel may impact the setup of DataModule. Certain\n            datasets expect a known global batch size, which depends on the number of devices and conditional tensor\n            model parallel/ pipeline model parallel settings. By default, we are testing only on single device without\n            parallelism.\n        - 'mode' is useful in some cases, but not in all cases. Implement conditions based on these when useful. As an\n            example, it may be useful to implement a test that stops and resumes.\n            - changing callbacks to test metadata integrity (core feature of stop-and-go tests).\n            - changing the model construction to use different hyperparameters.\n            - ... etc\n            Each of the above tests cases may be useful for automated testing of various expected behavior.\n        - stop(), resume(), continuous() or collectively run_stop_and_go() are provided methods which execute the actual\n          tests, leveraging the conditions in the various setup methods, respecting 'mode' where necessary.\n\n    Attributes:\n        root_dir: The root directory.\n        val_check_interval: The validation check interval. Stored as an attribute to ensure consistency.\n        exp_name: The experiment name.\n        extra_metrics_dict: A dictionary of metrics and their corresponding functions.\n\n    See Also: bionemo.testing.callbacks.\n    \"\"\"\n\n    # class variables that need to be overridden\n    num_steps: int\n    val_check_interval: int\n    limit_val_batches: int\n    lr: float = 1e-4\n    precision: Literal[\"16-mixed\", \"bf16-mixed\", \"32\"]\n\n    # class variables that will be setup in setUpClass\n    tempdir: tempfile.TemporaryDirectory\n    metadata_dir: pathlib.Path\n    exp_name: str\n    callbacks: CallbackDict\n    nemo_logger: NeMoLogger\n\n    @classmethod\n    def setup_class(cls) -&gt; None:\n        \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n        cls.tempdir = tempfile.TemporaryDirectory()\n        cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n        cls.exp_name = cls.__name__\n\n        cls.callbacks = cls.get_default_callbacks()\n\n        cls.nemo_logger = NeMoLogger(\n            log_dir=cls.tempdir.name,\n            name=cls.exp_name,\n            use_datetime_version=False,\n            version=None,\n            tensorboard=None,\n            wandb=None,\n            ckpt=None,\n        )\n\n    @classmethod\n    def teardown_class(cls) -&gt; None:\n        \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n        cls.tempdir.cleanup()\n\n    @classmethod\n    @abstractmethod\n    def setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n        \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n        Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n        to use the same code path for both.\n\n        Args:\n            mode: The mode indicating whether to stop or go.\n\n        Returns:\n            tuple: A tuple containing the model, data, and optimizer.\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def setup_trainer(\n        cls,\n        mode: Mode,\n    ) -&gt; nl.Trainer:\n        \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n        Args:\n            mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n        Returns:\n            (nl.Trainer): NeMo Lightning trainer object.\n        \"\"\"\n        strategy = MegatronStrategy(\n            ddp=\"megatron\",\n            find_unused_parameters=True,\n            ckpt_include_optimizer=True,\n        )\n\n        trainer = nl.Trainer(\n            devices=1,\n            max_steps=cls.num_steps,\n            accelerator=\"gpu\",\n            strategy=strategy,\n            limit_val_batches=cls.limit_val_batches,\n            val_check_interval=cls.val_check_interval,\n            log_every_n_steps=cls.val_check_interval,\n            num_nodes=1,\n            callbacks=list(cls.callbacks[mode].values()),\n            plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n        )\n        return trainer\n\n    @classmethod\n    def get_default_callbacks(cls) -&gt; CallbackDict:\n        \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n        To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n        ```python\n        callbacks = super().get_callbacks()\n        callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n        return callbacks\n        ```\n\n        Returns:\n            A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n            object.\n        \"\"\"\n        callbacks: CallbackDict = {}\n\n        def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n            return {\n                testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n                testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n                testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n                testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n                testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n                testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n                testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n                testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n                testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n                testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n            }\n\n        interrupted_callbacks = make_callbacks()\n        callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n        for mode in [Mode.STOP, Mode.RESUME]:\n            consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n            callbacks[mode] = {\n                consumed_samples_cls: consumed_samples_cls(mode=mode),\n                **interrupted_callbacks,\n            }\n\n        callbacks[Mode.STOP].update(\n            {\n                testing_callbacks.RaiseAfterMetadataCallback: testing_callbacks.RaiseAfterMetadataCallback(),\n                nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                    save_last=True,\n                    monitor=\"reduced_train_loss\",\n                    save_top_k=2,\n                    every_n_train_steps=cls.val_check_interval,\n                    always_save_context=True,\n                ),\n            }\n        )\n\n        return callbacks\n\n    # stop() and resume() are provided methods and run the requisite methods with the appropriate mode.\n    @classmethod\n    def stop(cls) -&gt; None:\n        \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n        This method sets up the model, data, and optimizer for the Mode.STOP mode.\n        It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n        The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n        If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n        Raises:\n            testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n        \"\"\"\n        logging.info(\"Running stop()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.STOP)\n        trainer = cls.setup_trainer(Mode.STOP)\n        with distributed_model_parallel_state():\n            try:\n                llm.train(\n                    model=model,\n                    data=data,\n                    trainer=trainer,\n                    log=cls.nemo_logger,\n                    optim=opt,\n                    resume=resume.AutoResume(\n                        resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                        resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n                    ),\n                )\n            except testing_callbacks.StopAndGoException:\n                return\n\n    @classmethod\n    def resume(cls) -&gt; None:\n        \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n        logging.info(\"Running resume()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.RESUME)\n        trainer = cls.setup_trainer(Mode.RESUME)\n        with distributed_model_parallel_state():\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n\n    @classmethod\n    def continuous(cls) -&gt; None:\n        \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n        logging.info(\"Running continuous()...\")\n\n        model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n        trainer = cls.setup_trainer(Mode.CONTINUOUS)\n        with distributed_model_parallel_state():\n            llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n\n    @classmethod\n    def run_stop_and_go(cls):\n        \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n        # Interrupted model training\n        cls.stop()\n        cls.resume()\n\n        # Continuous model training.\n        cls.continuous()\n\n    @pytest.mark.parametrize(\n        \"callback_type\",\n        [\n            testing_callbacks.LearningRateCallback,\n            testing_callbacks.GlobalStepStateCallback,\n            testing_callbacks.ConsumedSamplesCallback,\n            testing_callbacks.OptimizerStateCallback,\n            testing_callbacks.TrainInputCallback,\n            testing_callbacks.TrainOutputCallback,\n            testing_callbacks.TrainLossCallback,\n        ],\n    )\n    def test_stop_and_go_consistency(self, callback_type):\n        \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n        interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n        continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n        assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n        if callback_type == testing_callbacks.TrainOutputCallback:\n            atol = 1e-3\n        else:\n            atol = 1e-4\n\n        recursive_assert_approx_equal(interrupted_callback.data, continuous_callback.data, atol=atol)\n\n    def test_train_val_init_consumed_samples(self):\n        \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n        train_consumed_stop, val_consumed_stop = get_callback(\n            self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n        train_consumed_go, val_consumed_go = get_callback(\n            self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        ).data\n\n        assert val_consumed_stop == 0\n        assert val_consumed_go == 0\n        assert train_consumed_stop == 0\n        assert train_consumed_go &gt; 0\n\n    # TODO: For some reason, validation in NeMo runs an extra batch in the case when the training is stopped and\n    # resumed. Hopefully we can fix this upstream and remove the indexing based on the length of the continuous\n    # validation batches.\n    @pytest.mark.xfail(reason=\"Validation runs an extra batch in the case when training is stopped and resumed.\")\n    def test_identical_number_of_validation_batches(self):\n        \"\"\"Ensures that the input tensors for training are identical for the interrupted and continuous tests.\"\"\"\n        callback_type = testing_callbacks.ValidInputCallback\n        interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n        continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n        assert interrupted_callback.data, f\"No data found for {callback_type}\"\n        recursive_assert_approx_equal(interrupted_callback.data, continuous_callback.data)\n        assert len(interrupted_callback.data) == len(continuous_callback.data)\n\n    @pytest.mark.parametrize(\n        \"callback_type\",\n        [\n            testing_callbacks.ValidInputCallback,\n            testing_callbacks.ValidOutputCallback,\n            testing_callbacks.ValidLossCallback,\n        ],\n    )\n    def test_stop_and_go_consistency_with_uneven_validation_sizes(self, callback_type):\n        \"\"\"Ensures that the input tensors for training are identical for the interrupted and continuous tests.\"\"\"\n        interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n        continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n        assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n        # Hack: Validation seems to run an extra batch in the case when training is stopped and resumed, but we can\n        # still test the rest of the data to ensure consistency.\n        interrupted_data = interrupted_callback.data[-len(continuous_callback.data) :]\n\n        if callback_type == testing_callbacks.ValidOutputCallback:\n            atol = 1e-3\n        else:\n            atol = 1e-4\n\n        recursive_assert_approx_equal(interrupted_data, continuous_callback.data, atol=atol)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.continuous","title":"<code>continuous()</code>  <code>classmethod</code>","text":"<p>Trains the model in one continuous path without stopping.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef continuous(cls) -&gt; None:\n    \"\"\"Trains the model in one continuous path without stopping.\"\"\"\n    logging.info(\"Running continuous()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.CONTINUOUS)\n    trainer = cls.setup_trainer(Mode.CONTINUOUS)\n    with distributed_model_parallel_state():\n        llm.train(model=model, data=data, trainer=trainer, log=cls.nemo_logger, optim=opt)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.get_default_callbacks","title":"<code>get_default_callbacks()</code>  <code>classmethod</code>","text":"<p>Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.</p> <p>To extend this method, call the super and append to the callbacks, depending on which mode you are in:</p> <pre><code>callbacks = super().get_callbacks()\ncallbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\nreturn callbacks\n</code></pre> <p>Returns:</p> Type Description <code>CallbackDict</code> <p>A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback</p> <code>CallbackDict</code> <p>object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef get_default_callbacks(cls) -&gt; CallbackDict:\n    \"\"\"Returns a list of callbacks based on the specified mode. Base implementation provides reasonable defaults.\n\n    To extend this method, call the super and append to the callbacks, depending on which mode you are in:\n\n    ```python\n    callbacks = super().get_callbacks()\n    callbacks[mode][\"MyCustomCallback\"] = MyCustomCallback()\n    return callbacks\n    ```\n\n    Returns:\n        A dictionary of callbacks based on the specified mode, each of which maps a callback name to a callback\n        object.\n    \"\"\"\n    callbacks: CallbackDict = {}\n\n    def make_callbacks() -&gt; Dict[Type[pl.Callback], pl.Callback]:\n        return {\n            testing_callbacks.LearningRateCallback: testing_callbacks.LearningRateCallback(),\n            testing_callbacks.GlobalStepStateCallback: testing_callbacks.GlobalStepStateCallback(),\n            testing_callbacks.ConsumedSamplesCallback: testing_callbacks.ConsumedSamplesCallback(),\n            testing_callbacks.OptimizerStateCallback: testing_callbacks.OptimizerStateCallback(),\n            testing_callbacks.TrainInputCallback: testing_callbacks.TrainInputCallback(),\n            testing_callbacks.TrainOutputCallback: testing_callbacks.TrainOutputCallback(),\n            testing_callbacks.TrainLossCallback: testing_callbacks.TrainLossCallback(),\n            testing_callbacks.ValidInputCallback: testing_callbacks.ValidInputCallback(),\n            testing_callbacks.ValidOutputCallback: testing_callbacks.ValidOutputCallback(),\n            testing_callbacks.ValidLossCallback: testing_callbacks.ValidLossCallback(),\n        }\n\n    interrupted_callbacks = make_callbacks()\n    callbacks[Mode.CONTINUOUS] = make_callbacks()\n\n    for mode in [Mode.STOP, Mode.RESUME]:\n        consumed_samples_cls = testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n        callbacks[mode] = {\n            consumed_samples_cls: consumed_samples_cls(mode=mode),\n            **interrupted_callbacks,\n        }\n\n    callbacks[Mode.STOP].update(\n        {\n            testing_callbacks.RaiseAfterMetadataCallback: testing_callbacks.RaiseAfterMetadataCallback(),\n            nl_callbacks.ModelCheckpoint: nl_callbacks.ModelCheckpoint(\n                save_last=True,\n                monitor=\"reduced_train_loss\",\n                save_top_k=2,\n                every_n_train_steps=cls.val_check_interval,\n                always_save_context=True,\n            ),\n        }\n    )\n\n    return callbacks\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.resume","title":"<code>resume()</code>  <code>classmethod</code>","text":"<p>Resumes the model from the checkpoint saved at the end of <code>stop()</code> and verifies the metadata integrity.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef resume(cls) -&gt; None:\n    \"\"\"Resumes the model from the checkpoint saved at the end of `stop()` and verifies the metadata integrity.\"\"\"\n    logging.info(\"Running resume()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.RESUME)\n    trainer = cls.setup_trainer(Mode.RESUME)\n    with distributed_model_parallel_state():\n        llm.train(\n            model=model,\n            data=data,\n            trainer=trainer,\n            log=cls.nemo_logger,\n            optim=opt,\n            resume=resume.AutoResume(\n                resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n                resume_ignore_no_checkpoint=False,  # When false this will throw an error with no existing checkpoint.\n            ),\n        )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.run_stop_and_go","title":"<code>run_stop_and_go()</code>  <code>classmethod</code>","text":"<p>Executes training both continuously and with a checkpoint interruption.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef run_stop_and_go(cls):\n    \"\"\"Executes training both continuously and with a checkpoint interruption.\"\"\"\n    # Interrupted model training\n    cls.stop()\n    cls.resume()\n\n    # Continuous model training.\n    cls.continuous()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_class","title":"<code>setup_class()</code>  <code>classmethod</code>","text":"<p>Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_class(cls) -&gt; None:\n    \"\"\"Sets up the class by creating a temporary directory, metadata_dir, exp_name and callbacks.\"\"\"\n    cls.tempdir = tempfile.TemporaryDirectory()\n    cls.metadata_dir = pathlib.Path(cls.tempdir.name) / \"metadata\"\n    cls.exp_name = cls.__name__\n\n    cls.callbacks = cls.get_default_callbacks()\n\n    cls.nemo_logger = NeMoLogger(\n        log_dir=cls.tempdir.name,\n        name=cls.exp_name,\n        use_datetime_version=False,\n        version=None,\n        tensorboard=None,\n        wandb=None,\n        ckpt=None,\n    )\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_model","title":"<code>setup_model(mode)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs the model, data, and optimizer for the test harness.</p> <p>Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged to use the same code path for both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[LightningModule, LightningDataModule, MegatronOptimizerModule]</code> <p>A tuple containing the model, data, and optimizer.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\n@abstractmethod\ndef setup_model(cls, mode: Mode) -&gt; tuple[pl.LightningModule, pl.LightningDataModule, nl.MegatronOptimizerModule]:\n    \"\"\"Constructs the model, data, and optimizer for the test harness.\n\n    Optionally supports separate code paths for 'stop'/'resume'/'continuous', although implementors are encouraged\n    to use the same code path for both.\n\n    Args:\n        mode: The mode indicating whether to stop or go.\n\n    Returns:\n        tuple: A tuple containing the model, data, and optimizer.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.setup_trainer","title":"<code>setup_trainer(mode)</code>  <code>classmethod</code>","text":"<p>Setup trainer by passing stop, resume, or continuous callbacks according to mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop, resume, or train continuously.</p> required <p>Returns:</p> Type Description <code>Trainer</code> <p>NeMo Lightning trainer object.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef setup_trainer(\n    cls,\n    mode: Mode,\n) -&gt; nl.Trainer:\n    \"\"\"Setup trainer by passing stop, resume, or continuous callbacks according to mode.\n\n    Args:\n        mode (Mode): The mode indicating whether to stop, resume, or train continuously.\n\n    Returns:\n        (nl.Trainer): NeMo Lightning trainer object.\n    \"\"\"\n    strategy = MegatronStrategy(\n        ddp=\"megatron\",\n        find_unused_parameters=True,\n        ckpt_include_optimizer=True,\n    )\n\n    trainer = nl.Trainer(\n        devices=1,\n        max_steps=cls.num_steps,\n        accelerator=\"gpu\",\n        strategy=strategy,\n        limit_val_batches=cls.limit_val_batches,\n        val_check_interval=cls.val_check_interval,\n        log_every_n_steps=cls.val_check_interval,\n        num_nodes=1,\n        callbacks=list(cls.callbacks[mode].values()),\n        plugins=nl.MegatronMixedPrecision(precision=cls.precision),\n    )\n    return trainer\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.stop","title":"<code>stop()</code>  <code>classmethod</code>","text":"<p>Runs pre-training and 'stops' after the first checkpoint is saved.</p> <p>This method sets up the model, data, and optimizer for the Mode.STOP mode. It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics. The training process is executed using the <code>llm.train</code> function, passing the model, data, trainer, logger, optimizer, and resume options. If a <code>testing_callbacks.StopAndGoException</code> is raised during training, it is caught and no action is taken.</p> <p>Raises:</p> Type Description <code>StopAndGoException</code> <p>If a stop and go exception occurs during training.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef stop(cls) -&gt; None:\n    \"\"\"Runs pre-training and 'stops' after the first checkpoint is saved.\n\n    This method sets up the model, data, and optimizer for the Mode.STOP mode.\n    It then sets up the trainer and strategy for the Mode.STOP mode with the given metrics.\n    The training process is executed using the `llm.train` function, passing the model, data, trainer, logger, optimizer, and resume options.\n    If a `testing_callbacks.StopAndGoException` is raised during training, it is caught and no action is taken.\n\n    Raises:\n        testing_callbacks.StopAndGoException: If a stop and go exception occurs during training.\n    \"\"\"\n    logging.info(\"Running stop()...\")\n\n    model, data, opt = cls.setup_model(mode=Mode.STOP)\n    trainer = cls.setup_trainer(Mode.STOP)\n    with distributed_model_parallel_state():\n        try:\n            llm.train(\n                model=model,\n                data=data,\n                trainer=trainer,\n                log=cls.nemo_logger,\n                optim=opt,\n                resume=resume.AutoResume(\n                    resume_if_exists=False,  # Looks for the -last checkpoint to continue training.\n                    resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n                ),\n            )\n        except testing_callbacks.StopAndGoException:\n            return\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.teardown_class","title":"<code>teardown_class()</code>  <code>classmethod</code>","text":"<p>Tears down the class by cleaning up the temporary directory.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@classmethod\ndef teardown_class(cls) -&gt; None:\n    \"\"\"Tears down the class by cleaning up the temporary directory.\"\"\"\n    cls.tempdir.cleanup()\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_identical_number_of_validation_batches","title":"<code>test_identical_number_of_validation_batches()</code>","text":"<p>Ensures that the input tensors for training are identical for the interrupted and continuous tests.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@pytest.mark.xfail(reason=\"Validation runs an extra batch in the case when training is stopped and resumed.\")\ndef test_identical_number_of_validation_batches(self):\n    \"\"\"Ensures that the input tensors for training are identical for the interrupted and continuous tests.\"\"\"\n    callback_type = testing_callbacks.ValidInputCallback\n    interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n    continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n    assert interrupted_callback.data, f\"No data found for {callback_type}\"\n    recursive_assert_approx_equal(interrupted_callback.data, continuous_callback.data)\n    assert len(interrupted_callback.data) == len(continuous_callback.data)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_stop_and_go_consistency","title":"<code>test_stop_and_go_consistency(callback_type)</code>","text":"<p>Tests the consistency of the callback data between the interrupted and continuous checks.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@pytest.mark.parametrize(\n    \"callback_type\",\n    [\n        testing_callbacks.LearningRateCallback,\n        testing_callbacks.GlobalStepStateCallback,\n        testing_callbacks.ConsumedSamplesCallback,\n        testing_callbacks.OptimizerStateCallback,\n        testing_callbacks.TrainInputCallback,\n        testing_callbacks.TrainOutputCallback,\n        testing_callbacks.TrainLossCallback,\n    ],\n)\ndef test_stop_and_go_consistency(self, callback_type):\n    \"\"\"Tests the consistency of the callback data between the interrupted and continuous checks.\"\"\"\n    interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n    continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n    assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n    if callback_type == testing_callbacks.TrainOutputCallback:\n        atol = 1e-3\n    else:\n        atol = 1e-4\n\n    recursive_assert_approx_equal(interrupted_callback.data, continuous_callback.data, atol=atol)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_stop_and_go_consistency_with_uneven_validation_sizes","title":"<code>test_stop_and_go_consistency_with_uneven_validation_sizes(callback_type)</code>","text":"<p>Ensures that the input tensors for training are identical for the interrupted and continuous tests.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>@pytest.mark.parametrize(\n    \"callback_type\",\n    [\n        testing_callbacks.ValidInputCallback,\n        testing_callbacks.ValidOutputCallback,\n        testing_callbacks.ValidLossCallback,\n    ],\n)\ndef test_stop_and_go_consistency_with_uneven_validation_sizes(self, callback_type):\n    \"\"\"Ensures that the input tensors for training are identical for the interrupted and continuous tests.\"\"\"\n    interrupted_callback = get_callback(self.callbacks, Mode.RESUME, callback_type)\n    continuous_callback = get_callback(self.callbacks, Mode.CONTINUOUS, callback_type)\n    assert interrupted_callback.data, f\"No data found for {callback_type}\"\n\n    # Hack: Validation seems to run an extra batch in the case when training is stopped and resumed, but we can\n    # still test the rest of the data to ensure consistency.\n    interrupted_data = interrupted_callback.data[-len(continuous_callback.data) :]\n\n    if callback_type == testing_callbacks.ValidOutputCallback:\n        atol = 1e-3\n    else:\n        atol = 1e-4\n\n    recursive_assert_approx_equal(interrupted_data, continuous_callback.data, atol=atol)\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.StopAndGoHarness.test_train_val_init_consumed_samples","title":"<code>test_train_val_init_consumed_samples()</code>","text":"<p>Tests the initial consumed samples in stop-and-go scenario.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def test_train_val_init_consumed_samples(self):\n    \"\"\"Tests the initial consumed samples in stop-and-go scenario.\"\"\"\n    train_consumed_stop, val_consumed_stop = get_callback(\n        self.callbacks, Mode.STOP, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n    train_consumed_go, val_consumed_go = get_callback(\n        self.callbacks, Mode.RESUME, testing_callbacks.TrainValInitConsumedSamplesStopAndGoCallback\n    ).data\n\n    assert val_consumed_stop == 0\n    assert val_consumed_go == 0\n    assert train_consumed_stop == 0\n    assert train_consumed_go &gt; 0\n</code></pre>"},{"location":"API_reference/bionemo/testing/harnesses/stop_and_go/#bionemo.testing.harnesses.stop_and_go.get_callback","title":"<code>get_callback(callbacks, mode, callback_type)</code>","text":"<p>Returns the callback with the given name and mode.</p> <p>Convenience function to make type hinting easier.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>CallbackDict</code> <p>The dictionary of callbacks.</p> required <code>mode</code> <code>Mode</code> <p>The mode indicating whether to stop or go.</p> required <code>callback_type</code> <code>Type[Callback]</code> <p>The type of the callback.</p> required <p>Returns:</p> Type Description <code>Callback</code> <p>pl.Callback: The callback with the given name and mode.</p> Source code in <code>bionemo/testing/harnesses/stop_and_go.py</code> <pre><code>def get_callback(callbacks: CallbackDict, mode: Mode, callback_type: Type[Callback]) -&gt; Callback:\n    \"\"\"Returns the callback with the given name and mode.\n\n    Convenience function to make type hinting easier.\n\n    Args:\n        callbacks: The dictionary of callbacks.\n        mode: The mode indicating whether to stop or go.\n        callback_type: The type of the callback.\n\n    Returns:\n        pl.Callback: The callback with the given name and mode.\n    \"\"\"\n    return callbacks[mode][callback_type]  # type: ignore\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/","title":"Datamodule","text":""},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS","title":"<code>PickledDataWDS</code>","text":"<p>               Bases: <code>WebDataModule</code></p> <p>A LightningDataModule to process pickled data into webdataset tar files.</p> <p><code>PickledDataWDS</code> is a LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS--examples","title":"Examples:","text":"<ol> <li>create the data module with a directory of pickle files and the file name prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = {\n        Split.train : \"/path/to/output/tars/dir-train\",\n        Split.val : \"/path/to/output/tars/dir-val\",\n        Split.test : \"/path/to/output/tars/dir-test\",\n    }\n\n&gt;&gt;&gt; # see the `WebDataModule` API doc for the definition of global_batch_size\n&gt;&gt;&gt; global_batch_size = 16\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n&gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n&gt;&gt;&gt;     global_batch_size, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class PickledDataWDS(WebDataModule):\n    \"\"\"A LightningDataModule to process pickled data into webdataset tar files.\n\n    `PickledDataWDS` is a LightningDataModule to process pickled data into webdataset tar files\n    and setup dataset and dataloader. This inherits the webdataset setup from its parent module\n    `WebDataModule`. This data module takes a directory of pickled data files, data filename\n    prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files\n    by globbing the specific pickle data files `{dir_pickles}/{name_subset[split]}.{suffix_pickles}`\n    and outputing to webdataset tar file with the dict structure:\n    ```\n        {\"__key__\" : name.replace(\".\", \"-\"),\n         suffix_pickles : pickled.dumps(data) }\n    ```\n    NOTE: this assumes only one pickled file is processed for each sample. In\n    its setup() function, it creates the webdataset object chaining up the input\n    `pipeline_wds` workflow. In its train/val/test_dataloader(), it creates the\n    WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with a directory of pickle files and the file name\n    prefix thereof for different splits to used by `Lightning.Trainer.fit()`\n\n    ```\n    &gt;&gt;&gt; from bionemo.core.data.datamodule import Split, PickledDataWDS\n\n    &gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n    &gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n    &gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n    &gt;&gt;&gt; # validation dataset\n\n    &gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n    &gt;&gt;&gt; names_subset = {\n    &gt;&gt;&gt;     Split.train: [sample1, sample2],\n    &gt;&gt;&gt;     Split.val: [sample4, sample5],\n    &gt;&gt;&gt; }\n\n    &gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n    &gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n    &gt;&gt;&gt; n_tars_wds = 5\n    &gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n    &gt;&gt;&gt; output_dir_tar_files = {\n            Split.train : \"/path/to/output/tars/dir-train\",\n            Split.val : \"/path/to/output/tars/dir-val\",\n            Split.test : \"/path/to/output/tars/dir-test\",\n        }\n\n    &gt;&gt;&gt; # see the `WebDataModule` API doc for the definition of global_batch_size\n    &gt;&gt;&gt; global_batch_size = 16\n\n    &gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n    &gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n    &gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n    &gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n    &gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n    &gt;&gt;&gt; # create the data module\n    &gt;&gt;&gt; data_module = PickledDataWDS(\n    &gt;&gt;&gt;     dir_pickles,\n    &gt;&gt;&gt;     names_subset,\n    &gt;&gt;&gt;     suffix_pickles, # `WebDataModule` args\n    &gt;&gt;&gt;     output_dir_tar_files, # `WebDataModule` args\n    &gt;&gt;&gt;     global_batch_size, # `WebDataModule` args\n    &gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n    &gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n    &gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n    &gt;&gt;&gt; )\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        dir_pickles: str,\n        names_subset: Dict[Split, List[str]],\n        *args,\n        n_tars_wds: Optional[int] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Args:\n            dir_pickles: input directory of pickled data files\n            names_subset: list of filename prefix of\n                the data samples to be loaded in the dataset and dataloader for\n                each of the split\n            *args: arguments passed to the parent WebDataModule after its\n            `n_samples` args (where `n_samples` is deduced from the length of\n            `names_subset` arg of this class)\n            n_tars_wds: attempt to create at least this number of\n                webdataset shards\n            **kwargs: arguments passed to the parent WebDataModule\n        \"\"\"\n        super().__init__(\n            {split: len(names_subset[split]) for split in names_subset.keys()},\n            *args,\n            **kwargs,\n        )\n\n        self._dir_pickles = dir_pickles\n\n        self._names_subset = names_subset\n\n        self._n_tars_wds = n_tars_wds\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. The nesting\n        `pickles_to_tars` function goes through the data name prefixes in the\n        different splits, read the corresponding pickled file and output a\n        webdataset tar archive with the dict structure: {\"__key__\" :\n        name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n        \"\"\"\n        for split in self._names_subset.keys():\n            # create wds shards (tar files) for train set\n            pickles_to_tars(\n                self._dir_pickles,\n                self._names_subset[split],\n                self._suffix_keys_wds,\n                self._dirs_tars_wds[split],\n                self._prefix_tars_wds,\n                min_num_shards=self._n_tars_wds,\n            )\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.__init__","title":"<code>__init__(dir_pickles, names_subset, *args, n_tars_wds=None, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dir_pickles</code> <code>str</code> <p>input directory of pickled data files</p> required <code>names_subset</code> <code>Dict[Split, List[str]]</code> <p>list of filename prefix of the data samples to be loaded in the dataset and dataloader for each of the split</p> required <code>*args</code> <p>arguments passed to the parent WebDataModule after its</p> <code>()</code> <code>n_tars_wds</code> <code>Optional[int]</code> <p>attempt to create at least this number of webdataset shards</p> <code>None</code> <code>**kwargs</code> <p>arguments passed to the parent WebDataModule</p> <code>{}</code> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    dir_pickles: str,\n    names_subset: Dict[Split, List[str]],\n    *args,\n    n_tars_wds: Optional[int] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Args:\n        dir_pickles: input directory of pickled data files\n        names_subset: list of filename prefix of\n            the data samples to be loaded in the dataset and dataloader for\n            each of the split\n        *args: arguments passed to the parent WebDataModule after its\n        `n_samples` args (where `n_samples` is deduced from the length of\n        `names_subset` arg of this class)\n        n_tars_wds: attempt to create at least this number of\n            webdataset shards\n        **kwargs: arguments passed to the parent WebDataModule\n    \"\"\"\n    super().__init__(\n        {split: len(names_subset[split]) for split in names_subset.keys()},\n        *args,\n        **kwargs,\n    )\n\n    self._dir_pickles = dir_pickles\n\n    self._names_subset = names_subset\n\n    self._n_tars_wds = n_tars_wds\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.PickledDataWDS.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. The nesting\n    `pickles_to_tars` function goes through the data name prefixes in the\n    different splits, read the corresponding pickled file and output a\n    webdataset tar archive with the dict structure: {\"__key__\" :\n    name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.\n    \"\"\"\n    for split in self._names_subset.keys():\n        # create wds shards (tar files) for train set\n        pickles_to_tars(\n            self._dir_pickles,\n            self._names_subset[split],\n            self._suffix_keys_wds,\n            self._dirs_tars_wds[split],\n            self._prefix_tars_wds,\n            min_num_shards=self._n_tars_wds,\n        )\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.Split","title":"<code>Split</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Names for each data split.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class Split(Enum):\n    \"\"\"Names for each data split.\"\"\"\n\n    train = auto()\n    val = auto()\n    test = auto()\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule","title":"<code>WebDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for using webdataset tar files.</p> <p><code>WebDataModule</code> is a <code>LightningDataModule</code> for using webdataset tar files to setup PyTorch datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule--examples","title":"Examples:","text":"<ol> <li> <p>create the data module with input directory to webdataset tar files. Depending on which of the downstream Lightning.Trainer methods are called, e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>: <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # see the API doc for the definition of global_batch_size\n&gt;&gt;&gt; global_batch_size = 16\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(n_samples, suffix_keys_wds,\n                                dirs_of_tar_files, global_batch_size,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld)\n</code></pre></p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>class WebDataModule(L.LightningDataModule):\n    \"\"\"A LightningDataModule for using webdataset tar files.\n\n    `WebDataModule` is a `LightningDataModule` for using webdataset tar files to setup PyTorch\n    datasets and dataloaders. This data module takes as input a dictionary: Split -&gt; tar file\n    directory and vaiours webdataset config settings. In its setup() function, it creates the\n    webdataset object chaining up the input `pipeline_wds` workflow. In its train/val/test_dataloader(),\n    it creates the WebLoader object chaining up the `pipeline_prebatch_wld` workflow.\n\n    Examples:\n    --------\n    1. create the data module with input directory to webdataset tar files.\n    Depending on which of the downstream Lightning.Trainer methods are called,\n    e.g., `Trainer.fit()`, `Trainer.validate()`, `Trainer.test()` or\n    `Trainer.predict()`, only a subset of the train, val and test splits need to\n    be specified in the various input options to the data module:\n\n    - `Trainer.fit()` requires the `train` and `val` splits\n    - `Trainer.validate()` requires the `val` split\n    - `Trainer.test()` requires the `test` splits\n    - `Trainer.predict()` requires the `test` splits\n\n    Here is an example of constructing the data module for `Trainer.fit()`:\n    ```\n    &gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; tar_file_prefix = \"shards\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; dirs_of_tar_files = {\n    &gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n    &gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; n_samples {\n    &gt;&gt;&gt;     Split.train: 1000,\n    &gt;&gt;&gt;     Split.val: 100,\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n    &gt;&gt;&gt; # webdataset file (see\n    &gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n    &gt;&gt;&gt; # for details)\n    &gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # see the API doc for the definition of global_batch_size\n    &gt;&gt;&gt; global_batch_size = 16\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; seed = 27193781\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n    &gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n    &gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n    &gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n    &gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n    &gt;&gt;&gt; # for details.\n    &gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n    &gt;&gt;&gt; # file parsing rule.\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; from webdatast import shuffle\n    &gt;&gt;&gt; pipeline_wds = {\n    &gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n    &gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n    &gt;&gt;&gt;     Split.val: untuple\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n    &gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n    &gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n    &gt;&gt;&gt; # user can customize their batching routines here\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                        list_samples : torch.vstack(list_samples))\n    &gt;&gt;&gt; pipeline_prebatch_wld = {\n            Split.train: [shuffle(n_samples[Split.train],\n                                  rng=random.Random(seed_rng_shfl)), batch],\n            Split.val : batch,\n            Split.test : batch\n        }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n    &gt;&gt;&gt; # WebLoader\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wds = {\n    &gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n    &gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n    &gt;&gt;&gt;              'seed' : seed_rng_shfl}\n    &gt;&gt;&gt;     for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; kwargs_wld = {\n    &gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n    &gt;&gt;&gt;     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # construct the data module\n    &gt;&gt;&gt; data_module = WebDataModule(n_samples, suffix_keys_wds,\n                                    dirs_of_tar_files, global_batch_size,\n                                    prefix_tars_wds=tar_file_prefix,\n                                    pipeline_wds=pipeline_wds,\n                                    pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                    kwargs_wds=kwargs_wds,\n                                    kwargs_wld=kwargs_wld)\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        n_samples: Dict[Split, int],\n        suffix_keys_wds: Union[str, Iterable[str]],\n        dirs_tars_wds: Dict[Split, str],\n        global_batch_size: int,\n        prefix_tars_wds: str = \"wdshards\",\n        pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n        kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n        kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n    ):\n        \"\"\"Constructor.\n\n        Args:\n            n_samples: input dictionary: Split -&gt; number of data samples for each split\n            suffix_keys_wds: a set of keys each\n                corresponding to a data object in the webdataset tar file\n                dictionary. The data objects of these keys will be extracted and\n                tupled for each sample in the tar files\n            dirs_tars_wds: input dictionary: Split -&gt; tar file\n                directory that contains the webdataset tar files for each split\n            global_batch_size: size of batch summing across nodes in Data\n                Distributed Parallel, i.e., local_batch_size * n_nodes. NOTE:\n                this data module doesn't rely on the input `global_batch_size`\n                for batching the samples. The batching is supposed to be done as\n                a part of the input `pipeline_prebatch_wld`. `global_batch_size`\n                is only used to compute a (pseudo-) epoch length for the data\n                loader so that the loader yield approximately n_samples //\n                global_batch_size batches\n        Kwargs:\n            prefix_tars_wds: name prefix of the input webdataset tar\n                files. The input tar files are globbed by\n                \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n            pipeline_wds: a dictionary of webdatast composable, i.e.,\n                functor that maps a iterator to another iterator that\n                transforms the data sample yield from the dataset object, for\n                different splits, or an iterable to such a sequence of such\n                iterators. For example, this can be used to transform the\n                sample in the worker before sending it to the main process of\n                the dataloader\n            pipeline_prebatch_wld: a dictionary\n                of webloader composable, i.e., functor that maps a iterator to\n                another iterator that transforms the data sample yield from the\n                WebLoader object, for different splits, or an iterable to a\n                seuqnence of such iterators. For example, this can be used for\n                batching the samples. NOTE: this is applied before batching is\n                yield from the WebLoader\n            kwargs_wds: kwargs for the WebDataset.__init__()\n            kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n        \"\"\"\n        super().__init__()\n\n        self._dirs_tars_wds = dirs_tars_wds\n\n        keys_subset = self._dirs_tars_wds.keys()\n\n        if n_samples.keys() != keys_subset:\n            raise RuntimeError(\n                f\"Input n_samples has different keys than \" f\"dirs_tars_wds: {n_samples.keys()} vs \" f\"{keys_subset}\"\n            )\n\n        self._n_samples = n_samples\n\n        self._global_batch_size = global_batch_size\n\n        if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n            raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n        self._suffix_keys_wds = suffix_keys_wds\n\n        self._prefix_tars_wds = prefix_tars_wds\n        self._pipeline_wds = pipeline_wds\n        self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n        self._kwargs_wld = kwargs_wld\n\n        self._kwargs_wds = kwargs_wds\n\n        # to be created later in setup\n        self._dataset = {}\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"This is called only by the main process by the Lightning workflow.\n\n        Do not rely on this data module object's state update here as there is no\n        way to communicate the state update to other subprocesses. Is a **no-op**.\n        \"\"\"\n        pass\n\n    def _setup_wds(self, split: Split) -&gt; wds.WebDataset:\n        \"\"\"Setup webdataset and webloader. This is called by setup().\n\n        Args:\n            split (Split): train, val or test split\n\n        Returns:\n            WebDataset\n\n        \"\"\"\n        if split not in self._dirs_tars_wds.keys():\n            raise RuntimeError(f\"_setup_wds() is called with {split} \" f\"split that doesn't have the input tar dir\")\n        urls = sorted(glob.glob(f\"{self._dirs_tars_wds[split]}/{self._prefix_tars_wds}-*.tar\"))\n        kwargs = self._kwargs_wds[split] if self._kwargs_wds is not None else None\n        dataset = wds.WebDataset(urls, **(kwargs if kwargs is not None else {})).decode()\n        if isinstance(self._suffix_keys_wds, str):\n            dataset = dataset.extract_keys(f\"*.{self._suffix_keys_wds}\")\n        else:\n            dataset = dataset.extract_keys(*[f\"*.{key}\" for key in self._suffix_keys_wds])\n\n        if self._pipeline_wds is not None and self._pipeline_wds[split] is not None:\n            if isinstance(self._pipeline_wds[split], Iterable):\n                dataset = dataset.compose(*self._pipeline_wds[split])\n            else:\n                dataset = dataset.compose(self._pipeline_wds[split])\n        return dataset\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n        Args:\n            stage: \"fit\", \"test\" or \"predict\"\n        \"\"\"\n        if stage == \"fit\":\n            self._dataset[Split.train] = self._setup_wds(Split.train)\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"validate\":\n            self._dataset[Split.val] = self._setup_wds(Split.val)\n        elif stage == \"test\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        elif stage == \"predict\":\n            self._dataset[Split.test] = self._setup_wds(Split.test)\n        else:\n            raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n\n    def _setup_dataloader(self, split: Split) -&gt; wds.WebLoader:\n        \"\"\"Setup the dataloader for the input dataset split.\n\n        Args:\n            split (Split): input split type\n\n        Returns:\n             WebLoader object\n\n        Raises:\n            ValueError if `split` doesn't correspond to a known dataset.\n        \"\"\"\n        if self._dataset[split] is None:\n            raise ValueError(\n                f\"_setup_dataloader() is called with {split} split without setting up the corresponding dataset.\"\n            )\n        dataset = self._dataset[split]\n        n_samples = self._n_samples[split]\n        n_batches = (n_samples + self._global_batch_size - 1) // self._global_batch_size\n        kwargs = self._kwargs_wld[split] if self._kwargs_wld is not None else None\n        loader = wds.WebLoader(dataset, batch_size=None, **(kwargs if kwargs is not None else {}))\n\n        if self._pipeline_prebatch_wld is not None and self._pipeline_prebatch_wld[split] is not None:\n            if isinstance(self._pipeline_prebatch_wld[split], Iterable):\n                loader = loader.compose(*self._pipeline_prebatch_wld[split])\n            else:\n                loader = loader.compose(self._pipeline_prebatch_wld[split])\n\n        loader = loader.with_epoch(n_batches)\n\n        return loader\n\n    def train_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the training data.\"\"\"\n        return self._setup_dataloader(Split.train)\n\n    def val_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the validation data.\"\"\"\n        return self._setup_dataloader(Split.val)\n\n    def test_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Webdataset for the test data.\"\"\"\n        return self._setup_dataloader(Split.test)\n\n    def predict_dataloader(self) -&gt; wds.WebLoader:\n        \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n        return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.__init__","title":"<code>__init__(n_samples, suffix_keys_wds, dirs_tars_wds, global_batch_size, prefix_tars_wds='wdshards', pipeline_wds=None, pipeline_prebatch_wld=None, kwargs_wds=None, kwargs_wld=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>Dict[Split, int]</code> <p>input dictionary: Split -&gt; number of data samples for each split</p> required <code>suffix_keys_wds</code> <code>Union[str, Iterable[str]]</code> <p>a set of keys each corresponding to a data object in the webdataset tar file dictionary. The data objects of these keys will be extracted and tupled for each sample in the tar files</p> required <code>dirs_tars_wds</code> <code>Dict[Split, str]</code> <p>input dictionary: Split -&gt; tar file directory that contains the webdataset tar files for each split</p> required <code>global_batch_size</code> <code>int</code> <p>size of batch summing across nodes in Data Distributed Parallel, i.e., local_batch_size * n_nodes. NOTE: this data module doesn't rely on the input <code>global_batch_size</code> for batching the samples. The batching is supposed to be done as a part of the input <code>pipeline_prebatch_wld</code>. <code>global_batch_size</code> is only used to compute a (pseudo-) epoch length for the data loader so that the loader yield approximately n_samples // global_batch_size batches</p> required <p>Kwargs:     prefix_tars_wds: name prefix of the input webdataset tar         files. The input tar files are globbed by         \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"     pipeline_wds: a dictionary of webdatast composable, i.e.,         functor that maps a iterator to another iterator that         transforms the data sample yield from the dataset object, for         different splits, or an iterable to such a sequence of such         iterators. For example, this can be used to transform the         sample in the worker before sending it to the main process of         the dataloader     pipeline_prebatch_wld: a dictionary         of webloader composable, i.e., functor that maps a iterator to         another iterator that transforms the data sample yield from the         WebLoader object, for different splits, or an iterable to a         seuqnence of such iterators. For example, this can be used for         batching the samples. NOTE: this is applied before batching is         yield from the WebLoader     kwargs_wds: kwargs for the WebDataset.init()     kwargs_wld : kwargs for the WebLoader.init(), e.g., num_workers, of each split</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def __init__(\n    self,\n    n_samples: Dict[Split, int],\n    suffix_keys_wds: Union[str, Iterable[str]],\n    dirs_tars_wds: Dict[Split, str],\n    global_batch_size: int,\n    prefix_tars_wds: str = \"wdshards\",\n    pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    pipeline_prebatch_wld: Optional[Dict[Split, Union[Iterable[Iterable[Any]], Iterable[Any]]]] = None,\n    kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n    kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None,\n):\n    \"\"\"Constructor.\n\n    Args:\n        n_samples: input dictionary: Split -&gt; number of data samples for each split\n        suffix_keys_wds: a set of keys each\n            corresponding to a data object in the webdataset tar file\n            dictionary. The data objects of these keys will be extracted and\n            tupled for each sample in the tar files\n        dirs_tars_wds: input dictionary: Split -&gt; tar file\n            directory that contains the webdataset tar files for each split\n        global_batch_size: size of batch summing across nodes in Data\n            Distributed Parallel, i.e., local_batch_size * n_nodes. NOTE:\n            this data module doesn't rely on the input `global_batch_size`\n            for batching the samples. The batching is supposed to be done as\n            a part of the input `pipeline_prebatch_wld`. `global_batch_size`\n            is only used to compute a (pseudo-) epoch length for the data\n            loader so that the loader yield approximately n_samples //\n            global_batch_size batches\n    Kwargs:\n        prefix_tars_wds: name prefix of the input webdataset tar\n            files. The input tar files are globbed by\n            \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"\n        pipeline_wds: a dictionary of webdatast composable, i.e.,\n            functor that maps a iterator to another iterator that\n            transforms the data sample yield from the dataset object, for\n            different splits, or an iterable to such a sequence of such\n            iterators. For example, this can be used to transform the\n            sample in the worker before sending it to the main process of\n            the dataloader\n        pipeline_prebatch_wld: a dictionary\n            of webloader composable, i.e., functor that maps a iterator to\n            another iterator that transforms the data sample yield from the\n            WebLoader object, for different splits, or an iterable to a\n            seuqnence of such iterators. For example, this can be used for\n            batching the samples. NOTE: this is applied before batching is\n            yield from the WebLoader\n        kwargs_wds: kwargs for the WebDataset.__init__()\n        kwargs_wld : kwargs for the WebLoader.__init__(), e.g., num_workers, of each split\n    \"\"\"\n    super().__init__()\n\n    self._dirs_tars_wds = dirs_tars_wds\n\n    keys_subset = self._dirs_tars_wds.keys()\n\n    if n_samples.keys() != keys_subset:\n        raise RuntimeError(\n            f\"Input n_samples has different keys than \" f\"dirs_tars_wds: {n_samples.keys()} vs \" f\"{keys_subset}\"\n        )\n\n    self._n_samples = n_samples\n\n    self._global_batch_size = global_batch_size\n\n    if not isinstance(suffix_keys_wds, get_args(Union[str, Iterable])):\n        raise TypeError(\"suffix_keys_wds can only be str or Iterable[str]\")\n\n    self._suffix_keys_wds = suffix_keys_wds\n\n    self._prefix_tars_wds = prefix_tars_wds\n    self._pipeline_wds = pipeline_wds\n    self._pipeline_prebatch_wld = pipeline_prebatch_wld\n\n    self._kwargs_wld = kwargs_wld\n\n    self._kwargs_wds = kwargs_wds\n\n    # to be created later in setup\n    self._dataset = {}\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Alias for :func:<code>test_dataloader</code>.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Alias for :func:`test_dataloader`.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>This is called only by the main process by the Lightning workflow.</p> <p>Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. Is a no-op.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"This is called only by the main process by the Lightning workflow.\n\n    Do not rely on this data module object's state update here as there is no\n    way to communicate the state update to other subprocesses. Is a **no-op**.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>This is called on all Lightning-managed nodes in a multi-node training session.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>\"fit\", \"test\" or \"predict\"</p> required Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"This is called on all Lightning-managed nodes in a multi-node training session.\n\n    Args:\n        stage: \"fit\", \"test\" or \"predict\"\n    \"\"\"\n    if stage == \"fit\":\n        self._dataset[Split.train] = self._setup_wds(Split.train)\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"validate\":\n        self._dataset[Split.val] = self._setup_wds(Split.val)\n    elif stage == \"test\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    elif stage == \"predict\":\n        self._dataset[Split.test] = self._setup_wds(Split.test)\n    else:\n        raise NotImplementedError(f\"Data setup with {stage=} is not implemented.\")\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Webdataset for the test data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the test data.\"\"\"\n    return self._setup_dataloader(Split.test)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Webdataset for the training data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the training data.\"\"\"\n    return self._setup_dataloader(Split.train)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/datamodule/#bionemo.webdatamodule.datamodule.WebDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Webdataset for the validation data.</p> Source code in <code>bionemo/webdatamodule/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; wds.WebLoader:\n    \"\"\"Webdataset for the validation data.\"\"\"\n    return self._setup_dataloader(Split.val)\n</code></pre>"},{"location":"API_reference/bionemo/webdatamodule/utils/","title":"Utils","text":""},{"location":"API_reference/bionemo/webdatamodule/utils/#bionemo.webdatamodule.utils.pickles_to_tars","title":"<code>pickles_to_tars(dir_input, input_prefix_subset, input_suffix, dir_output, output_prefix, func_output_data=lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}, min_num_shards=None)</code>","text":"<p>Convert a subset of pickle files from a directory to Webdataset tar files.</p> <p>Input path and name pattern for sample 0: f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\" Input path and name pattern for sample 1: f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\" f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\" ... Output path and name pattern: f\"{dir_output}/{output_prefix}-%06d.tar\".</p> <p>The webdataset tar archive is specified by the dictionary: {     \"key\" : sample_filename_preifx,     sample_filename_suffix_1 : data_1,     sample_filename_suffix_2 : data_2,     ... } so that parsing the tar archive is equivalent of reading {sample_filename_preifx}.{sample_filename_suffix_1} etc.</p> <p>Here, each sample data get its name prefix from one element of <code>input_prefix_subset</code> and its name suffixes from the list <code>input_suffix</code>. Per the webdataset file format specification, the <code>sample_filename_preifx</code> can't contain dots '.' so this function removes it for the user by calling .replace(\".\", \"-\") on the elements of <code>input_prefix_subset</code></p> <p>Parameters:</p> Name Type Description Default <code>dir_input</code> <code>str</code> <p>Input directory</p> required <code>input_prefix_subset</code> <code>List[str]</code> <p>Input subset of pickle files' prefix</p> required <code>input_suffix</code> <code>Union[str, Iterable[str]]</code> <p>Input pickle file name suffixes, each for one type of data object, for all the samples</p> required <code>dir_output</code> <code>str</code> <p>Output directory</p> required <code>output_prefix</code> <code>str</code> <p>Output tar file name prefix</p> required <code>func_output_data</code> <code>Callable[[str, Dict[str, Any]], Dict[str, Any]]</code> <p>function that maps the name prefix, name suffix and data object to a webdataset tar archive dictionary. Refer to the webdataset github repo for the archive file format specification.</p> <code>lambda prefix, suffix_to_data: {'__key__': prefix, None: suffix_to_data}</code> <code>min_num_shards</code> <p>create at least this number of tar files. WebDataset has bugs when reading small number of tar files in a multi-node lightening + DDP setting so this option can be used to guarantee the tar file counts</p> <code>None</code> Source code in <code>bionemo/webdatamodule/utils.py</code> <pre><code>def pickles_to_tars(\n    dir_input: str,\n    input_prefix_subset: List[str],\n    input_suffix: Union[str, Iterable[str]],\n    dir_output: str,\n    output_prefix: str,\n    func_output_data: Callable[[str, Dict[str, Any]], Dict[str, Any]] = lambda prefix, suffix_to_data: {\n        \"__key__\": prefix,\n        **suffix_to_data,\n    },\n    min_num_shards: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Convert a subset of pickle files from a directory to Webdataset tar files.\n\n    Input path and name pattern for sample 0:\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[0]}.{input_suffix[1]}\"\n    Input path and name pattern for sample 1:\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[0]}\"\n    f\"{dir_input}/{input_prefix_subset[1]}.{input_suffix[1]}\"\n    ...\n    Output path and name pattern:\n    f\"{dir_output}/{output_prefix}-%06d.tar\".\n\n    The webdataset tar archive is specified by the dictionary:\n    {\n        \"__key__\" : sample_filename_preifx,\n        sample_filename_suffix_1 : data_1,\n        sample_filename_suffix_2 : data_2,\n        ...\n    }\n    so that parsing the tar archive is equivalent of reading\n    {sample_filename_preifx}.{sample_filename_suffix_1} etc.\n\n    Here, each sample data get its name prefix from one element of\n    `input_prefix_subset` and its name suffixes from the list `input_suffix`.\n    Per the webdataset file format specification, the `sample_filename_preifx`\n    can't contain dots '.' so this function removes it for the user by calling\n    .replace(\".\", \"-\") on the elements of `input_prefix_subset`\n\n    Args:\n        dir_input: Input directory\n        input_prefix_subset: Input subset of pickle files' prefix\n        input_suffix: Input pickle file name\n            suffixes, each for one type of data object, for all the samples\n        dir_output: Output directory\n        output_prefix: Output tar file name prefix\n        func_output_data: function that maps the name prefix, name suffix and\n            data object to a webdataset tar archive dictionary. Refer to the webdataset\n            github repo for the archive file format specification.\n        min_num_shards : create at least this number of tar files.\n            WebDataset has bugs when reading small number of tar files in a\n            multi-node lightening + DDP setting so this option can be used to\n            guarantee the tar file counts\n    \"\"\"\n    if not isinstance(input_suffix, get_args(Union[str, Iterable])):\n        raise TypeError(\"input_suffix can only be str or Iterable[str]\")\n    os.makedirs(dir_output, exist_ok=True)\n    wd_subset_pattern = os.path.join(dir_output, f\"{output_prefix}-%06d.tar\")\n    n_samples_per_shard_max = 100000\n    if min_num_shards is not None:\n        if min_num_shards &lt;= 0:\n            raise ValueError(f\"Invalid min_num_shards = {min_num_shards} &lt;= 0\")\n        n_samples_per_shard_max = len(input_prefix_subset) // min_num_shards\n    with wds.ShardWriter(\n        wd_subset_pattern,\n        encoder=False,\n        maxcount=n_samples_per_shard_max,\n        compress=False,\n        mode=0o777,\n    ) as sink:\n        for name in input_prefix_subset:\n            try:\n                if isinstance(input_suffix, str):\n                    suffix_to_data = {\n                        input_suffix: pickle.dumps(\n                            pickle.loads((Path(dir_input) / f\"{name}.{input_suffix}\").read_bytes())\n                        )\n                    }\n                else:\n                    suffix_to_data = {\n                        suffix: pickle.dumps(pickle.loads((Path(dir_input) / f\"{name}.{suffix}\").read_bytes()))\n                        for suffix in input_suffix\n                    }\n                # the prefix name shouldn't contain any \".\" per webdataset's\n                # specification\n                sample = func_output_data(name.replace(\".\", \"-\"), suffix_to_data)\n                sink.write(sample)\n            except ModuleNotFoundError as e:\n                raise RuntimeError(\n                    \"Can't process pickle file due to\\\n                                   missing dependencies\"\n                ) from e\n            except Exception as e:\n                raise RuntimeError(f\"Failed to write {name} into tar files.\") from e\n</code></pre>"},{"location":"datasets/","title":"BioNeMo Framework: Available Datasets","text":"<p>The BioNeMo Framework provides access to a variety of high-quality datasets for bioinformatics and cheminformatics research. These datasets cover a range of biological and chemical modalities, supporting various research applications. The following table lists the currently available datasets:</p> Dataset Modality Uses CELLxGENE Single Cell Single-Cell Gene Expression UniProt Protein Protein Sequence and Function Analysis <p>For more information about the datasets included in the BioNeMo Framework, refer to the Dataset Cards linked in the table above or the original sources referenced in the respective dataset descriptions.</p>"},{"location":"datasets/CELLxGENE/","title":"CELLxGENE","text":""},{"location":"datasets/CELLxGENE/#description","title":"Description","text":"<p>CELLxGENE is an aggregation of publicly available single-cell datasets collected by CZI.</p>"},{"location":"datasets/CELLxGENE/#dataset-attributes-of-version-2023-12-15","title":"Dataset attributes of version 2023-12-15","text":"<p>Data was downloaded using the CELLxGENE Discover Census version <code>2023-12-15</code>. We first downloaded cellxgene census version 2023-12-15 using the <code>cellxgene_census</code> python API. We limited cell data to <code>organism=\u201dHomo sapiens\u201d</code>, with a non \u201cna\u201d <code>suspension_type</code>, <code>is_primary_data=True</code>, and <code>disease=\u201dnormal\u201d</code> to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \u201cassay\u201d, \u201csex\u201d, \u201cdevelopment_stage\u201d, \u201ctissue_general\u201d, \u201cdataset_id\u201d and \u201cself_reported_ethnicity\u201d. The metadata \u201cassay\u201d, \u201ctissue_general\u201d, and \u201cdataset_id\u201d were used to construct dataset splits into train, validation, and test sets. The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets. In this training split, we made sure that all \u201cassay\u201d and \u201ctissue_general\u201d labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases. Finally the 1% hold-out set was split further into a validation and test set. This final split was mostly done randomly by cell, however we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>These parameters resulted in 23.87 Million single cells collected from a variety of public datasets, all hosted by CZI cell x gene census. After the splitting procedure we had:</p> <ul> <li>23.64 Million cells in the training split</li> <li>0.13 Million cells in the validation split</li> <li>0.11 Million cells in the test split</li> </ul>"},{"location":"datasets/CELLxGENE/#distributions-of-donor-covariates","title":"Distributions of donor covariates","text":"<p>There are various biases apparent in this dataset.</p>"},{"location":"datasets/CELLxGENE/#tissue-distribution","title":"Tissue distribution","text":"<p>At a high level tissues were heavily biased toward the nervous system, which made up nearly 40 percent of the data.</p> <p></p>"},{"location":"datasets/CELLxGENE/#assay-distribution","title":"Assay distribution","text":"<p>Assays were also imbalanced in this dataset. As the 10x machine is fairly high throughput and currently popular, it makes sense that the majority of cells present would be from this instrument. Various versions of the 10x instrument made up 18M of the 24M cells while the next largest category was <code>sci-RNA-seq</code>. </p>"},{"location":"datasets/CELLxGENE/#sex-distribution","title":"Sex distribution","text":"<p>A bias exists in this dataset for sex. Most of the donor's cells were male-derived at 52%, while female donor's cell contribution made up 42%, and the remaining 6% were not annotated. .</p>"},{"location":"datasets/CELLxGENE/#reported-ethnicity-distribution","title":"Reported ethnicity distribution","text":"<p>The dataset has a heavy bias toward cells derived from donors with european ethnicity at 40%, while the next largest category, asian, made up 8%. When considering that nearly 50% were unknown, we might expect that as much as 75% of this dataset is made up of cells extracted from donors of self reported european ethnicity. </p>"},{"location":"datasets/CELLxGENE/#age-distribution","title":"Age distribution","text":"<p>This dataset is very heavily balanced toward younger donors. Many of the cells are derived from donors that are under a year of age (over 25%). After that the remaining 75% of cells are dispersed roughly under a normal distribution with a mode of 51-60 other than an additional peak in the 21-30 range. Donors over 61 years old make up approximately 15% of the data.</p> <p></p>"},{"location":"datasets/CELLxGENE/#assay-size-distribution","title":"Assay size distribution","text":"<p>Different assays have different ranges of reported gene measurements. On the low end <code>BD Rapsody Targetted mRNA</code> has only a few genes reported, while 10x instruments tend to report on 30,000 genes.</p> <p></p>"},{"location":"datasets/CELLxGENE/#dataset-distribution","title":"Dataset distribution","text":"<p>Dataset (eg a publication that produces data and uploads to cellxgene) leads to known batch effects due to different handling proceedures, collection procedures, etc. We stratify our training vs hold-out split by this covariate for this reason. Exploring the breakdown of datasets we see that the top 10 datsets represent approximately 10 million cells of the full cellxgene datset. The largest dataset alone has 4 million cells.</p> <p></p> <p>Looking at the makeup of these top datasets, we see that most represent single tissue categories predominately. Most of these tend to be nervous system datsets with the exception of one which is balanced between many cell types. </p>"},{"location":"datasets/CELLxGENE/#references","title":"References","text":"<ul> <li>CZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al. bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174</li> </ul>"},{"location":"datasets/CELLxGENE/#data-license","title":"Data License","text":"<p>The data in CELLxGENE are made available by the study authors and Chan Zuckerberg Initiative under the creative commons CC BY 4.0 license. Study authors agree prior to submission that their data is not identifiable, lacking any direct personal identifiers in the metadata. More information may be found in the CELLxGENE Data Submission Policy. Our training, validation and test data, including subsets made available for testing and demonstration purposes, was contributed to CELLxGENE through one or more of the following sources:</p> <ul> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/01fee550-877c-4a13-97b2-96bb43e5a2a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6b701826-37bb-4356-9792-ff41fc4c3161</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/9959402b-2e69-4aeb-ba39-efdfa5e0de1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/10bf5c50-8d85-4c5f-94b4-22c1363d9f31</li> <li>Publication Reference: ; Dataset Version: https://datasets.cellxgene.cziscience.com/bc484ee8-b3cc-47a3-8f4f-f95aa1fec803.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a98b828a-622a-483a-80e0-15703678befd</li> <li>Publication Reference: Ahern et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.01.012 Dataset Version: https://datasets.cellxgene.cziscience.com/e72d0170-3399-4aa0-8c56-da7b4f0ced6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8f126edf-5405-4731-8374-b5ce11f53e82</li> <li>Publication Reference: Andrews et al. (2022) Hepatology Communications; Publication: https://doi.org/10.1002/hep4.1854 Dataset Version: https://datasets.cellxgene.cziscience.com/19b364f7-db0c-430b-bc16-9b31cbd45a58.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/44531dd9-1388-4416-a117-af0a99de2294</li> <li>Publication Reference: Arutyunyan et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05869-0 Dataset Version: https://datasets.cellxgene.cziscience.com/5720f13d-fc15-4859-90df-447637fb37c4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e2c257e7-6f79-487c-b81c-39451cd4ab3c</li> <li>Publication Reference: Bhaduri et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03910-8 Dataset Version: https://datasets.cellxgene.cziscience.com/677082ca-48d3-44b8-b5c4-84dffafbba23.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c8565c6a-01a1-435b-a549-f11b452a83a8</li> <li>Publication Reference: Bhat-Nakshatri et al. (2021) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2021.100219 Dataset Version: https://datasets.cellxgene.cziscience.com/f72aae6e-c997-484c-bffd-6d09e41ef9a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c9706a92-0e5f-46c1-96d8-20e42467f287</li> <li>Publication Reference: Bondoc et al. (2021) Commun Biol; Publication: https://doi.org/10.1038/s42003-021-02562-8 Dataset Version: https://datasets.cellxgene.cziscience.com/20d54624-2098-4ed5-89f8-6da2bb460c3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a261413d-835b-4f1e-ab0c-dada55ea6afd</li> <li>Publication Reference: Burclaff et al. (2022) Cellular and Molecular Gastroenterology and Hepatology; Publication: https://doi.org/10.1016/j.jcmgh.2022.02.007 Dataset Version: https://datasets.cellxgene.cziscience.com/e00e3a74-038a-46fd-8931-f6dc8c90fd13.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/64b24fda-6591-4ce1-89e7-33eb6c43ad7b</li> <li>Publication Reference: Calandrelli et al. (2020) Nat Commun; Publication: https://doi.org/10.1038/s41467-020-18957-w Dataset Version: https://datasets.cellxgene.cziscience.com/c3189372-fceb-493d-98be-23abe1947253.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/db468083-041c-41ca-8f6f-bf991a070adf</li> <li>Publication Reference: Cao et al. (2020) Science; Publication: https://doi.org/10.1126/science.aba7721 Dataset Version: https://datasets.cellxgene.cziscience.com/8f6296d0-5b29-4dca-8061-b97147df5fcc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c114c20f-1ef4-49a5-9c2e-d965787fb90c</li> <li>Publication Reference: Chan Zuckerberg Initiative Single-Cell COVID-19 Consortia et al. (2020) medRxiv; Publication: https://doi.org/10.1101/2020.11.20.20227355 Dataset Version: https://datasets.cellxgene.cziscience.com/6c779b98-f437-4cbf-9b81-a3e6be637419.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0434a9d4-85fd-4554-b8e3-cf6c582bb2fa</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/1ba7d495-c1a8-4809-b56d-548fbea77c8a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Chan et al. (2021) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2021.09.008 Dataset Version: https://datasets.cellxgene.cziscience.com/c40911a4-47de-460e-be86-52e39800654c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62e8f058-9c37-48bc-9200-e767f318a8ec</li> <li>Publication Reference: Cheng et al. (2018) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2018.09.006 Dataset Version: https://datasets.cellxgene.cziscience.com/912d943b-9060-4fd3-a12c-ad641a89f0e4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43d4bb39-21af-4d05-b973-4c1fed7b916c</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/b1989183-5808-46ab-87f5-978febb2d26e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Cowan et al. (2020) Cell; Publication: https://doi.org/10.1016/j.cell.2020.08.013 Dataset Version: https://datasets.cellxgene.cziscience.com/c0d3867e-1a7b-4e57-af62-c563f1934226.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2f4c738f-e2f3-4553-9db2-0582a38ea4dc</li> <li>Publication Reference: Dom\\u00ednguez Conde et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl5197 Dataset Version: https://datasets.cellxgene.cziscience.com/08f58b32-a01b-4300-8ebc-2b93c18f26f7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/62ef75e4-cbea-454e-a0ce-998ec40223d3</li> <li>Publication Reference: Easter et al. (2024) Nat Commun; Publication: https://doi.org/10.1038/s41467-024-49037-y Dataset Version: https://datasets.cellxgene.cziscience.com/221dff56-a47d-4563-90ed-51b60e2f16d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/71f4bccf-53d4-4c12-9e80-e73bfb89e398</li> <li>Publication Reference: Egozi et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01586-1 Dataset Version: https://datasets.cellxgene.cziscience.com/e3a84fef-b6df-49b2-b0ca-ecaf444773ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7651ac1a-f947-463a-9223-a9e408a41989</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/3aedefc0-401a-4ee8-a1b5-a0ffc20e1ff2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2020) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2020.11.010 Dataset Version: https://datasets.cellxgene.cziscience.com/5d27ffd6-1769-4564-961f-9bb32d9ca3a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/17481d16-ee44-49e5-bcf0-28c0780d8c4a</li> <li>Publication Reference: Elmentaite et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03852-1 Dataset Version: https://datasets.cellxgene.cziscience.com/c463b937-dbdc-48ff-8037-dd191ea4e41e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e33ffcd3-7cbf-4b8c-b0f4-85587ad5019a</li> <li>Publication Reference: Eraslan et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4290 Dataset Version: https://datasets.cellxgene.cziscience.com/355ed159-f7d7-45e9-bc55-95639f0ab8b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a3ffde6c-7ad2-498a-903c-d58e732f7470</li> <li>Publication Reference: Fan et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-11036-9 Dataset Version: https://datasets.cellxgene.cziscience.com/9b2536db-4576-4906-ae9b-a01a623462f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2902f08c-f83c-470e-a541-e463e25e5058</li> <li>Publication Reference: Fasolino et al. (2022) Nat Metab; Publication: https://doi.org/10.1038/s42255-022-00531-x Dataset Version: https://datasets.cellxgene.cziscience.com/d39144df-fa59-4b63-b07b-9b34613b5c84.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/51544e44-293b-4c2b-8c26-560678423380</li> <li>Publication Reference: Fawkner-Corbett et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2020.12.016 Dataset Version: https://datasets.cellxgene.cziscience.com/e8473c46-eada-43b7-bd1c-e0fed1c4c913.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/60358420-6055-411d-ba4f-e8ac80682a2e</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/291ce735-8d18-4a2f-a6bc-98f75f8d6bc0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Gabitto et al. (2023) bioRxiv; Publication: https://doi.org/10.1101/2023.05.08.539485 Dataset Version: https://datasets.cellxgene.cziscience.com/e9bffe1d-9f07-4467-9230-c080b362e737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1ca90a2d-2943-483d-b678-b809bf464c30</li> <li>Publication Reference: Garcia-Alonso et al. (2021) Nat Genet; Publication: https://doi.org/10.1038/s41588-021-00972-2 Dataset Version: https://datasets.cellxgene.cziscience.com/15f77a91-aadc-4e63-81c7-a8614e9ad33d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/32f2fd23-ec74-486f-9544-e5b2f41725f5</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/366847dc-8fc4-42c3-9c27-9704929c6792.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/4b894b0d-6b27-4ab4-a48a-0205e4aaf348.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Garcia-Alonso et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-04918-4 Dataset Version: https://datasets.cellxgene.cziscience.com/c74c3be0-1a80-4af9-8241-d560afc67886.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/661a402a-2a5a-4c71-9b05-b346c57bc451</li> <li>Publication Reference: Gray et al. (2022) Developmental Cell; Publication: https://doi.org/10.1016/j.devcel.2022.05.003 Dataset Version: https://datasets.cellxgene.cziscience.com/9fecd056-d8c8-4ec6-8522-8d40f19c90a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/99f1515b-46a2-4bc4-94c3-f62659dc1eb4</li> <li>Publication Reference: Guilliams et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2021.12.018 Dataset Version: https://datasets.cellxgene.cziscience.com/5f2d618d-2a5f-4c31-8750-982342d7dd04.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/74e10dc4-cbb2-4605-a189-8a1cd8e44d8c</li> <li>Publication Reference: Han et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2157-4 Dataset Version: https://datasets.cellxgene.cziscience.com/7a455e3b-dd79-499b-95c9-8b1b2dde5339.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/38833785-fac5-48fd-944a-0f62a4c23ed1</li> <li>Publication Reference: Han et al. (2022) Blood Cancer Discovery; Publication: https://doi.org/10.1158/2643-3230.BCD-21-0075 Dataset Version: https://datasets.cellxgene.cziscience.com/f905e484-1162-467e-b8c5-835dcfe9bd5c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/968834a0-1895-40df-8720-666029b3bbac</li> <li>Publication Reference: Hao et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.048 Dataset Version: https://datasets.cellxgene.cziscience.com/55c120dc-6a20-4caf-9513-f5970b24b1be.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0cf0afa-ec40-4d65-b570-ed4ceacc6813</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/256d6049-b499-47a9-9c9a-20b92f9c6ba6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/25dbd3da-8cb0-4b9d-814c-85ae0a710353.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/2b55f5c0-aa82-41e8-9d84-915b1d5a797b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: He et al. (2022) Cell; Publication: https://doi.org/10.1016/j.cell.2022.11.005 Dataset Version: https://datasets.cellxgene.cziscience.com/c4122ff1-79d9-405b-92f1-c1c27234a125.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2d2e2acd-dade-489f-a2da-6c11aa654028</li> <li>Publication Reference: James et al. (2020) Nat Immunol; Publication: https://doi.org/10.1038/s41590-020-0602-z Dataset Version: https://datasets.cellxgene.cziscience.com/6e2ab5f9-bb51-459a-9fd7-605d29661823.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7681c7d7-0168-4892-a547-6f02a6430ace</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/7c452616-bc00-4499-b511-bfd5de1b7cd6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/8ef64f32-461d-4d50-81b6-5718b506a7a8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jardine et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03929-x Dataset Version: https://datasets.cellxgene.cziscience.com/f402b857-7871-45f4-9ad1-ff943552285a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/793fdaec-5067-428a-a9db-ecefe135c945</li> <li>Publication Reference: Jin et al. (2021) iScience; Publication: https://doi.org/10.1016/j.isci.2021.103115 Dataset Version: https://datasets.cellxgene.cziscience.com/753caa81-61c5-4126-a61c-df2c546b16d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b9fc3d70-5a72-4479-a046-c2cc1ab19efc</li> <li>Publication Reference: Jorstad et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf6812 Dataset Version: https://datasets.cellxgene.cziscience.com/97a035e0-b9d3-4e7b-adc2-b318316da7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d17249d2-0e6e-4500-abb8-e6c93fa1ac6f</li> <li>Publication Reference: Joseph et al. (2021) J. Pathol.; Publication: https://doi.org/10.1002/path.5751 Dataset Version: https://datasets.cellxgene.cziscience.com/fb812806-ae3a-45de-9102-0b2f801424e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4b54248f-2165-477c-a027-dd55082e8818</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/4936be1a-c766-46e2-815f-1c994aed7a4f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kamath et al. (2022) Nat Neurosci; Publication: https://doi.org/10.1038/s41593-022-01061-1 Dataset Version: https://datasets.cellxgene.cziscience.com/dd206caf-13ca-4598-86af-339189adff0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b0f0b447-ac37-45b0-b1bf-5c0b7d871120</li> <li>Publication Reference: Kanemaru et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06311-1 Dataset Version: https://datasets.cellxgene.cziscience.com/1a7a9fb0-aee1-437f-8a7c-9d132253a4db.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3116d060-0a8e-4767-99bb-e866badea1ed</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/02675fa7-5f13-4d89-a07d-9d0ff7996f0d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: King et al. (2021) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abe6291 Dataset Version: https://datasets.cellxgene.cziscience.com/6c8a5c10-2617-4cb1-8ed6-20a5e25065ef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3a2af25b-2338-4266-aad3-aa8d07473f50</li> <li>Publication Reference: Knight-Schrijver et al. (2022) Nat Cardiovasc Res; Publication: https://doi.org/10.1038/s44161-022-00183-w Dataset Version: https://datasets.cellxgene.cziscience.com/c1e3c998-4961-46c9-929d-d011900964e8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/43b45a20-a969-49ac-a8e8-8c84b211bd01</li> <li>Publication Reference: Kock et al. (2024) bioRxiv; Publication: https://doi.org/10.1101/2024.06.30.601119 Dataset Version: https://datasets.cellxgene.cziscience.com/a3850101-1e15-4ae2-82f7-bb9289e911d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ced320a1-29f3-47c1-a735-513c7084d508</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/30d6b4f5-1475-4b86-a47c-48609d6706c2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/41bdc5da-b436-485a-a753-9ae297057ee6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/485d1aee-db62-4373-854c-12a34237e97b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/8db9570a-3f80-41c9-b927-ae3eb115ba1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/dedfeef9-bfb8-4c2f-a196-1afea9a846d8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kong et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.002 Dataset Version: https://datasets.cellxgene.cziscience.com/ef7d48a5-c56b-4f13-9903-fb3327924445.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5c868b6f-62c5-4532-9d7f-a346ad4b50a7</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/24ee53d1-e5ed-47ae-8b8e-7a0d62d91513.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kumar et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-06252-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b8b5be07-061b-4390-af0a-f9ced877a068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4195ab4c-20bd-4cd3-8b3d-65601277e731</li> <li>Publication Reference: Kuppe et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-022-05060-x Dataset Version: https://datasets.cellxgene.cziscience.com/c1f6034b-7973-45e1-85e7-16933d0550bc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/8191c283-0816-424b-9b61-c3e1d6258a77</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/1cbe52c1-0567-4188-9c18-9d7271c56055.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lake et al. (2023) Nature; Publication: https://doi.org/10.1038/s41586-023-05769-3 Dataset Version: https://datasets.cellxgene.cziscience.com/d0ddf40e-dc4b-4134-8439-bfb8bf7a81f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bcb61471-2a44-4d00-a0af-ff085512674c</li> <li>Publication Reference: Lavaert et al. (2020) Immunity; Publication: https://doi.org/10.1016/j.immuni.2020.03.019 Dataset Version: https://datasets.cellxgene.cziscience.com/22e3ee6e-e47b-4502-9b2c-21b4c30a455f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/83ed3be8-4cb9-43e6-9aaa-3fbbf5d1bd3a</li> <li>Publication Reference: Ledergor et al. (2018) Nat Med; Publication: https://doi.org/10.1038/s41591-018-0269-2 Dataset Version: https://datasets.cellxgene.cziscience.com/680c8801-ccdd-4018-a14a-cefda2da3848.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/2a0b02c0-fea6-47bd-92b9-9b03f5d2580c</li> <li>Publication Reference: Lee et al. (2020) Sci. Immunol.; Publication: https://doi.org/10.1126/sciimmunol.abd1554 Dataset Version: https://datasets.cellxgene.cziscience.com/b0a99b60-7480-4b5c-93c5-11020c36adb2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4f889ffc-d4bc-4748-905b-8eb9db47a2ed</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/da638059-73e0-4a3b-a6fc-5fb8e47d4bff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Lengyel et al. (2022) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2022.111838 Dataset Version: https://datasets.cellxgene.cziscience.com/de2a800c-249f-4072-8454-cde3d6bfb5b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/d36ca85c-3e8b-444c-ba3e-a645040c6185</li> <li>Publication Reference: Li et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.11.001 Dataset Version: https://datasets.cellxgene.cziscience.com/39196e03-e248-4724-a618-b3bef017d6b2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/f7cecffa-00b4-4560-a29a-8ad626b8ee08</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/0da7c2ec-246f-4ffb-9c25-1aa059870a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Liang et al. (2023) Cell Genomics; Publication: https://doi.org/10.1016/j.xgen.2023.100298 Dataset Version: https://datasets.cellxgene.cziscience.com/9a199269-5a65-4ade-aa85-07ab4cfb4c26.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/af893e86-8e9f-41f1-a474-ef05359b1fb7</li> <li>Publication Reference: Litvi\\u0148ukov\\u00e1 et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2797-4 Dataset Version: https://datasets.cellxgene.cziscience.com/a5618935-5bbd-494d-b300-9ecc2402d5b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b52eb423-5d0d-4645-b217-e1c6d38b2e72</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/504b4b53-f0fb-43a6-8b8d-6254e8d81e85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Liu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.02.018 Dataset Version: https://datasets.cellxgene.cziscience.com/6d14e6f5-9b9f-4e8e-8b31-cee9020e18a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ed9185e3-5b82-40c7-9824-b2141590c7f0</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/a6c47325-eae6-4111-9e47-89a550ef99af.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukassen et al. (2020) EMBO J; Publication: https://doi.org/10.15252/embj.20105114 Dataset Version: https://datasets.cellxgene.cziscience.com/b0c0edd1-a8ba-436c-bef4-667bdf3fc8f1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6ff3401b-d72c-4940-a00c-3f0792397082</li> <li>Publication Reference: Lukowski et al. (2019) EMBO J; Publication: https://doi.org/10.15252/embj.2018100811 Dataset Version: https://datasets.cellxgene.cziscience.com/9b910901-bcf8-4d51-a85b-268ccbd54544.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/3472f32d-4a33-48e2-aad5-666d4631bf4c</li> <li>Publication Reference: MacParland et al. (2018) Nat Commun; Publication: https://doi.org/10.1038/s41467-018-06318-7 Dataset Version: https://datasets.cellxgene.cziscience.com/f07d2b1d-2e04-4f30-92d2-7d55e22da909.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bd5230f4-cd76-4d35-9ee5-89b3e7475659</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/2c6ab5e2-09c9-410b-a472-7559e45e553a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/55abbbab-bb53-4f92-92cd-483d8f74a881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Madissoon et al. (2020) Genome Biol; Publication: https://doi.org/10.1186/s13059-019-1906-x Dataset Version: https://datasets.cellxgene.cziscience.com/ee76e44a-8b7a-4ddc-8e29-c8a355254033.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/4d74781b-8186-4c9a-b659-ff4dc4601d91</li> <li>Publication Reference: Melms et al. (2021) Nature; Publication: https://doi.org/10.1038/s41586-021-03569-1 Dataset Version: https://datasets.cellxgene.cziscience.com/739bae5d-2d6f-4664-a79f-d065610da5ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e4c9ed14-e560-4900-a3bf-b0f8d2ce6a10</li> <li>Publication Reference: Menon et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12780-8 Dataset Version: https://datasets.cellxgene.cziscience.com/f34734b0-5de7-4b48-a3ba-bd9bb8c48e73.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1a486c4c-c115-4721-8c9f-f9f096e10857</li> <li>Publication Reference: Muto et al. (2021) Nat Commun; Publication: https://doi.org/10.1038/s41467-021-22368-w Dataset Version: https://datasets.cellxgene.cziscience.com/3b84b0b5-3d0a-41a1-860a-bbadad3717bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9b02383a-9358-4f0f-9795-a891ec523bcc</li> <li>Publication Reference: Nowicki-Osuch et al. (2023) Cancer Discovery; Publication: https://doi.org/10.1158/2159-8290.cd-22-0824 Dataset Version: https://datasets.cellxgene.cziscience.com/074caa96-9205-4bc6-b5ba-eef787f131dc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a18474f4-ff1e-4864-af69-270b956cee5b</li> <li>Publication Reference: Orozco et al. (2020) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2019.12.082 Dataset Version: https://datasets.cellxgene.cziscience.com/56334044-9f3a-4541-be24-28bed94c2d4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/939769a8-d8d2-4d01-abfc-55699893fd49</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/70170717-45c4-4891-9b14-fb795ecc3d94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Otero-Garcia et al. (2022) Neuron; Publication: https://doi.org/10.1016/j.neuron.2022.06.021 Dataset Version: https://datasets.cellxgene.cziscience.com/b6ddb1cd-f058-450e-8e66-e81f6e8a3c4a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b953c942-f5d8-434f-9da7-e726ba7c1481</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/59d5b3c5-9a55-44ae-a7fa-c14567e02755.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Park et al. (2020) Science; Publication: https://doi.org/10.1126/science.aay3224 Dataset Version: https://datasets.cellxgene.cziscience.com/c6e08ab6-ab3b-41dc-8058-8e6442e081ec.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/de13e3e2-23b6-40ed-a413-e9e12d7d3910</li> <li>Publication Reference: Perez et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf1970 Dataset Version: https://datasets.cellxgene.cziscience.com/8f2e4a29-e397-4f33-bc9a-3181e06b64b0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/436154da-bcf1-4130-9c8b-120ff9a888f2</li> <li>Publication Reference: Reed et al. (2024) Nat Genet; Publication: https://doi.org/10.1038/s41588-024-01688-9 Dataset Version: https://datasets.cellxgene.cziscience.com/5a611776-aae0-41b9-9f2b-aaf5f83771a3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/48259aa8-f168-4bf5-b797-af8e88da6637</li> <li>Publication Reference: Reichart et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo1984 Dataset Version: https://datasets.cellxgene.cziscience.com/e0251a80-0058-4686-9ab8-b2e751313e18.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e75342a8-0f3b-4ec5-8ee1-245a23e0f7cb</li> <li>Publication Reference: Ren et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.01.053 Dataset Version: https://datasets.cellxgene.cziscience.com/ae18e694-6f45-4635-affa-63ac0e29323d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a839c4b-10d0-4d64-9272-684c49a2c8ba</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/0ac22902-dfaa-4ceb-9cad-03afbd96f6d4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/5b4b134c-42f6-415b-a4fd-7f2a60128ff1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Rodr\\u00edguez-Ubreva et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-29450-x Dataset Version: https://datasets.cellxgene.cziscience.com/d6e742c5-f6e5-42f4-8064-622783542f6b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bf325905-5e8e-42e3-933d-9a9053e9af80</li> <li>Publication Reference: Salcher et al. (2022) Cancer Cell; Publication: https://doi.org/10.1016/j.ccell.2022.10.008 Dataset Version: https://datasets.cellxgene.cziscience.com/99040b08-7e1a-4d81-911c-4d2fd2335757.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/edb893ee-4066-4128-9aec-5eb2b03f8287</li> <li>Publication Reference: Seeker et al. (2023) acta neuropathol commun; Publication: https://doi.org/10.1186/s40478-023-01568-z Dataset Version: https://datasets.cellxgene.cziscience.com/32c319ef-10e2-4948-8b40-093d2f9d7cb5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9d63fcf1-5ca0-4006-8d8f-872f3327dbe9</li> <li>Publication Reference: Sikkema et al. (2023) Nat Med; Publication: https://doi.org/10.1038/s41591-023-02327-2 Dataset Version: https://datasets.cellxgene.cziscience.com/8d84ba15-d367-4dce-979c-85da70b868a2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6f6d381a-7701-4781-935c-db10d30de293</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/005aba12-a5af-4fcd-9b80-e28d845885c8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/05c60502-d980-4b5f-b093-d27cdf1360cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/09df20c1-3c87-465d-b67c-73dc5f3d1bc8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0eb52059-c5a4-4b6a-aa95-e9bec179e13c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/0f509022-ab14-4151-a435-37bcf582e20d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/137f5623-e19a-4cb4-9466-05ba393c3552.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1a5d4cf1-e1a7-4081-918a-8d9cb441cd54.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1eef8184-4629-406d-b04e-13f1c34b1071.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/1faafbca-0062-4fc0-8b50-a3d32a111d39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/214485af-4c67-4fda-b430-995162804fbf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/21fd9d75-dfec-443c-b634-9803792a081c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/229ddec2-a177-4325-bb2c-0ec11092982a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25a92b24-76c5-4bdb-a431-3478a8b38a69.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/25cc85a9-8645-43cf-a552-d7487c04159e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2a8c8c02-e64b-4b39-9b1a-e46247031c1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/2baa0697-1757-4675-b1ae-51672b2d9bce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/34c82f0d-04a7-4b13-8d4f-ae06065a2b1c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/364a5dc5-2d78-4931-9eeb-477c56ba63e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/366254d0-6aef-4c49-a814-bc195a5bb6c6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3a64f908-b132-4d0c-aac6-28012cb6fd11.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f0713f8-2fd6-4b77-8ea2-010a0d9b51ae.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3f1a2741-7b58-4eef-91ad-becc1646b1a4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/3ff56875-32be-4008-be41-c9c6526fc730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/443f8b59-a15d-4d22-9b18-9ec721560c19.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/45d8a451-d313-465d-b66d-bdea50800ce8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/47a0e562-2745-4619-9623-d6fa431b4026.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4a65c6dd-cb77-4cc3-8d95-06957a668c1f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/4df9fbc5-871b-43be-b796-92cd3f592e32.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/54095aef-f5c6-4dfa-b2e3-00fa8bb8d86a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/544c6998-8d55-4c59-bd18-18c81a3a8b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/560a125a-9dd4-4fae-b58a-f24c932c5dd9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/587acfff-2fb3-46eb-8511-8a1cd2e65ad4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/58f6d3e6-aedd-479f-b2f5-9026fac2011c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/595f2363-dd4a-4184-a75a-c8b0fb62e3d1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5ae914f7-a02e-4f08-8bf5-45023b1a23d9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5dac55f7-6650-4d24-b8c0-bbb9672ac819.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/5e07340f-a505-4272-a688-de9e104de2d6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/60034ae4-27a7-46f3-a5aa-668eba808b6f.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/609942ad-2a18-4567-a9b7-8bf126cd1732.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/61ee7a9e-216b-4119-93fa-740fe41a8a0a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/63319798-c980-4fd6-866c-46d9fc1f6d28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/64f05ce7-291c-487e-9b00-498a2279291d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6918480f-892a-42bf-a5cd-5e988f6e46b9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6a65d707-2c28-4d47-8b77-12f8dd3d66e2.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6be7d5fc-9024-46a9-84aa-26a230ac6733.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/6cc6a9b9-d53a-4b25-9b3f-d1f00e092d85.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/702d58ff-08f2-48ef-85b7-6132e77efdc4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/705b76b8-68a5-44fb-89ef-fb5273d2ec88.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/717b249c-46e3-43b5-9b3e-93bbdf879874.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/73ec94b8-e1c8-4de8-966a-8dea2842b068.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/744be8ae-ddd2-4822-9537-50030b340bef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/74903ae9-7470-4795-9508-057a058b9447.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/777952cd-2f82-408b-8890-f084f5825d90.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/7d5e92ed-87fa-4e85-831c-0b98479f3ec6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/80869ae5-15a7-4ddc-9c2e-4f40dffb2aeb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81d55237-e3df-4212-84f8-5ed7a1b5b62e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/81eb9dd8-2b33-4ff5-aa3f-a8842b68ddca.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/821fa877-c2b1-488f-bc12-606e804f6f4b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8268fa76-4996-4a21-b687-ad67c5dc383d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82a043bf-b6c0-4746-9d69-39437796282e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/82faf671-658a-4c88-8a7d-618fc7f68fad.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/83b8522d-f362-4aca-8c64-2dfb125b87e1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/88bacb21-c9d7-4d59-a1d9-ac71d75628b3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/8faf8f5a-284a-4b2f-b8fe-4a71cb38a79a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9993afca-cfb9-4ae9-851b-07528edd8b20.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9b42ebb8-5ebe-4707-bb75-7d64a5fdcafb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f0e0ca6-43ed-4e52-9d85-93cbe24346a5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/9f90a216-3fac-44ff-b0ca-7b77cf53ef07.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a1446790-c439-4a23-904d-488028e4d34c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a4581a49-fe69-4039-97dd-a2c54e676159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a497b47b-c4a2-42fe-a7a4-8abc8aed4ec5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a561b8a5-9c26-4b52-bb0b-dca989c432cd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a737721b-e311-40da-9835-fc60d991be9a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/a7f64bb1-6198-406f-ae78-ddb884633937.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ab10decf-4e42-44ac-bb45-660771526f96.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ad40f328-75a3-4b5e-8212-308b7d734d45.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ae98b1bf-fd73-49a0-9732-41f74da1832b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/aeb99282-b851-469d-bb77-4c550ae71b37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b18522ea-2a30-4f9d-b5c1-aa93a9f5677d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b38e94df-9801-4cf1-a8e2-8b39f25f56b4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b47045e2-d8ad-4c72-9412-aa4824f532d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b893efbd-c660-404d-9055-6717b7eba280.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/b9ecd819-1a5b-4e8e-9f2d-833b1891af37.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bc0c001e-5c7f-43ef-9003-40f2e3015ff7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/bfbe13d3-d6b8-4fd9-aeb7-c35e80219e1d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c2b0f135-3723-4351-83e5-c4eef19401f4.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/c821776d-a9fd-4a9b-b0ef-65bee87c7dbc.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cb5f81ff-4676-4ead-be89-e7da582ddd94.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cc1379b7-5d6e-4944-85ab-6b3ddb602730.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ccef089c-a215-4607-9617-8e00f40ece67.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/cf701447-cfdd-40fd-b8fe-76d181dad7f9.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d693ce29-f45e-4871-83e5-c60c001c190d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/d8b0c448-6d8c-4286-aa8b-55372af3ad1a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/dcf1e7b7-6e47-4052-a096-2b2ef8deafce.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/ddf86877-360c-44d0-af09-e6645bd0432a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/de350331-df0e-4b8d-88dc-0bc12c4ca845.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/df9fb19c-3e91-484a-bee6-6db14d649e05.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e0763860-958c-4f2c-928e-9625f7a37a39.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e1669dcb-672e-4bca-bf68-6ff242618aa0.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e683f70e-10b0-4189-ae3f-7007d32ca57a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/e9f575c0-bf54-4252-9551-1d4c69eda1bf.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f143d458-e4d0-4b44-9e25-d13e8578c9a6.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f146e689-f045-4b3a-999d-ab0b287391bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/f867c94f-c15b-4e40-9ffe-b0c598b90881.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Siletti et al. (2023) Science; Publication: https://doi.org/10.1126/science.add7046 Dataset Version: https://datasets.cellxgene.cziscience.com/fdda2f51-9ed9-4959-9897-9581595c1bfb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/283d65eb-dd53-496d-adb7-7570c7caa443</li> <li>Publication Reference: Smillie et al. (2019) Cell; Publication: https://doi.org/10.1016/j.cell.2019.06.029 Dataset Version: https://datasets.cellxgene.cziscience.com/6c483976-30de-4835-97f0-2b9bc93614e7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/33d19f34-87f5-455b-8ca5-9023a2e5453d</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/bf50dbfb-9ca0-4f0d-8deb-a1a810a0e313.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Smith et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2023333118 Dataset Version: https://datasets.cellxgene.cziscience.com/ff7778bf-7a65-4d23-a9f4-b26c47926c28.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e02201d7-f49f-401f-baf0-1eb1406546c0</li> <li>Publication Reference: Sol\\u00e9-Boldo et al. (2020) Commun Biol; Publication: https://doi.org/10.1038/s42003-020-0922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/bc8d7152-3b69-4153-9314-7342ae58fbde.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/c353707f-09a4-4f12-92a0-cb741e57e5f0</li> <li>Publication Reference: Stephenson et al. (2021) Nat Med; Publication: https://doi.org/10.1038/s41591-021-01329-2 Dataset Version: https://datasets.cellxgene.cziscience.com/46586a98-b75d-4557-9cc4-839fc28e67d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/ddfad306-714d-4cc0-9985-d9072820c530</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/40ebb8e4-1a25-4a33-b8ff-02d1156e4e9b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Stewart et al. (2019) Science; Publication: https://doi.org/10.1126/science.aat5031 Dataset Version: https://datasets.cellxgene.cziscience.com/fe7e4408-7390-4f93-95aa-ffe472843421.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/120e86b4-1195-48c5-845b-b98054105eec</li> <li>Publication Reference: Strati et al. (2023) Cell Reports Medicine; Publication: https://doi.org/10.1016/j.xcrm.2023.101158 Dataset Version: https://datasets.cellxgene.cziscience.com/21d11474-fe9a-420d-9661-5b88ba407bc1.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/26b5b4f6-828c-4791-b4a3-abb19e3b1952</li> <li>Publication Reference: Suo et al. (2022) Science; Publication: https://doi.org/10.1126/science.abo0510 Dataset Version: https://datasets.cellxgene.cziscience.com/fe340fe0-f2e8-4e7b-8879-0161248129d3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b1a879f6-5638-48d3-8f64-f6592c1b1561</li> <li>Publication Reference: Szabo et al. (2019) Nat Commun; Publication: https://doi.org/10.1038/s41467-019-12464-3 Dataset Version: https://datasets.cellxgene.cziscience.com/71c5026d-9567-4d0f-a808-edf29440df43.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/24d42e5e-ce6d-45ff-a66b-a3b3b715deaf</li> <li>Publication Reference: The Tabula Sapiens Consortium* et al. (2022) Science; Publication: https://doi.org/10.1126/science.abl4896 Dataset Version: https://datasets.cellxgene.cziscience.com/981bcf57-30cb-4a85-b905-e04373432fef.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/e5f58829-1a66-40b5-a624-9046778e74f5</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/0fd9c007-8ba2-4d87-8568-c938d2631fba.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Travaglini et al. (2020) Nature; Publication: https://doi.org/10.1038/s41586-020-2922-4 Dataset Version: https://datasets.cellxgene.cziscience.com/6dde7580-6b89-4cf9-83b4-c778f06eda7c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/5d445965-6f1a-4b68-ba3a-b8f765155d3a</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/3967ab0f-a63c-4318-809e-73329341ba5e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/61f15353-e598-43b5-bb5a-80ac44a0cf0b.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Triana et al. (2021) Nat Immunol; Publication: https://doi.org/10.1038/s41590-021-01059-0 Dataset Version: https://datasets.cellxgene.cziscience.com/9397b7fa-a9b5-4b56-8570-c30cb09d9df8.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/93eebe82-d8c3-41bc-a906-63b5b5f24a9d</li> <li>Publication Reference: Tritschler et al. (2022) Molecular Metabolism; Publication: https://doi.org/10.1016/j.molmet.2022.101595 Dataset Version: https://datasets.cellxgene.cziscience.com/9abfdde4-1d63-4c9a-8ec5-2fff1bd6f387.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/0a77d4c0-d5d0-40f0-aa1a-5e1429bcbd7e</li> <li>Publication Reference: Ulrich et al. (2021) bioRxiv; Publication: https://doi.org/10.1101/2021.09.16.460628 Dataset Version: https://datasets.cellxgene.cziscience.com/7fa27624-7eda-454a-a066-4de49d5788bd.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/fc77d2ae-247d-44d7-aa24-3f4859254c2c</li> <li>Publication Reference: Velmeshev et al. (2023) Science; Publication: https://doi.org/10.1126/science.adf0834 Dataset Version: https://datasets.cellxgene.cziscience.com/b126d3e9-a1a2-419a-a049-b4a27d3ce3ab.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/bacccb91-066d-4453-b70e-59de0b4598cd</li> <li>Publication Reference: Vento-Tormo et al. (2018) Nature; Publication: https://doi.org/10.1038/s41586-018-0698-6 Dataset Version: https://datasets.cellxgene.cziscience.com/7a3bdf11-fbb1-4966-8742-38b574c47317.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a9254216-6cd8-4186-b32c-349363777584</li> <li>Publication Reference: Wang et al. (2020) eLife; Publication: https://doi.org/10.7554/eLife.62522 Dataset Version: https://datasets.cellxgene.cziscience.com/b0afc679-2ec0-4c8d-9e3e-19cd693f8462.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/625f6bf4-2f33-4942-962e-35243d284837</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/07d4feab-33de-4bb2-8a5d-452044bc066d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wang et al. (2023) Immunity; Publication: https://doi.org/10.1016/j.immuni.2023.01.032 Dataset Version: https://datasets.cellxgene.cziscience.com/20f29b78-5b74-4e6d-bc88-679116733988.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03cdc7f4-bd08-49d0-a395-4487c0e5a168</li> <li>Publication Reference: Wiedemann et al. (2023) Cell Reports; Publication: https://doi.org/10.1016/j.celrep.2023.111994 Dataset Version: https://datasets.cellxgene.cziscience.com/1ffe8f40-d258-4c05-885e-46375c483fc7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/6d203948-a779-4b69-9b3f-1ee1dadc3980</li> <li>Publication Reference: Wilk et al. (2020) Nat Med; Publication: https://doi.org/10.1038/s41591-020-0944-y Dataset Version: https://datasets.cellxgene.cziscience.com/419da3c2-9141-4654-817f-ee6472df4be3.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/a72afd53-ab92-4511-88da-252fb0e26b9a</li> <li>Publication Reference: Wilson et al. (2022) Nat Commun; Publication: https://doi.org/10.1038/s41467-022-32972-z Dataset Version: https://datasets.cellxgene.cziscience.com/14e77ad6-38ce-4aad-8385-68b21aff0737.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/b3e2c6e3-9b05-4da9-8f42-da38a664b45b</li> <li>Publication Reference: Xiang et al. (2020) Front. Cardiovasc. Med.; Publication: https://doi.org/10.3389/fcvm.2020.00052 Dataset Version: https://datasets.cellxgene.cziscience.com/4dc06a70-6d39-4da6-aa8d-2f3fcdbbc1ff.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/9c8808ce-1138-4dbe-818c-171cff10e650</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/60eec096-34b9-45c2-b433-3caffb84f955.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/6d75c1db-1fe2-4a0e-b55f-680c6df9c99e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/8de7d30b-caeb-4def-aad2-592c39cb3f3c.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/96c7866c-7111-4687-b2c4-fbd445842a30.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/a9d71d53-66a1-4681-ba97-9671c05dff6d.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yan et al. (2020) Sci Rep; Publication: https://doi.org/10.1038/s41598-020-66092-9 Dataset Version: https://datasets.cellxgene.cziscience.com/b67dcfb1-ccae-404c-b0d4-d681ac227858.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1d1c7275-476a-49e2-9022-ad1b1c793594</li> <li>Publication Reference: Yazar et al. (2022) Science; Publication: https://doi.org/10.1126/science.abf3041 Dataset Version: https://datasets.cellxgene.cziscience.com/b3e8792e-a31b-404b-a866-250c43bc06d5.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dde06e0f-ab3b-46be-96a2-a8082383c4a1</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/69be948c-03f4-46e8-896e-530c79080808.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yoshida et al. (2022) Nature; Publication: https://doi.org/10.1038/s41586-021-04345-x Dataset Version: https://datasets.cellxgene.cziscience.com/d911082e-b5e2-40e6-8f08-fb53c7894622.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/c7bec699-47c3-44ee-a311-ae8507adf6bb.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Yu et al. (2021) Cell; Publication: https://doi.org/10.1016/j.cell.2021.04.028 Dataset Version: https://datasets.cellxgene.cziscience.com/da7ca6a3-079c-428f-8453-9b21ecce87a7.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/dfc09a93-bce0-4c77-893d-e153d1b7f9fa</li> <li>Publication Reference: Zhang et al. (2021) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2103240118 Dataset Version: https://datasets.cellxgene.cziscience.com/b6a3566e-a7bf-4fb3-b20c-858c6330e380.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/1df8c90d-d299-4b2e-a54d-a5a80f36e780</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/47573dc2-1dfc-4ca8-8c6e-b705a6656159.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/9fa63ab8-6316-4460-8283-af290fec1654.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/a76ab167-d254-4128-b5ca-86960560074a.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/e999edf8-f5e6-4af7-975c-1d8f0a5e8d3e.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van Zyl et al. (2022) Proc. Natl. Acad. Sci. U.S.A.; Publication: https://doi.org/10.1073/pnas.2200914119 Dataset Version: https://datasets.cellxgene.cziscience.com/f5f99f11-5a99-4a72-bd0d-0a87c5b6b406.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/63d03351-06be-478e-a0db-f7a653b6b19b</li> <li>Publication Reference: van der Wijst et al. (2021) Sci. Transl. Med.; Publication: https://doi.org/10.1126/scitranslmed.abh2624 Dataset Version: https://datasets.cellxgene.cziscience.com/02a1eee1-e290-47d1-8d9d-bc7f51c13670.h5ad curated and distributed by CZ CELLxGENE Discover in Collection: https://cellxgene.cziscience.com/collections/7d7cabfd-1d1f-40af-96b7-26a0825a306d</li> </ul>"},{"location":"datasets/uniprot/","title":"UniProt Dataset","text":"<p>The UniProt Knowledgebase (UniProtKB) is an open database of protein sequences curated from translated genomic data [1]. The UniProt Reference Cluster (UniRef) databases provide clustered sets of sequences from UniProtKB [2], which have been used in previous large language model training studies to improve diversity in protein training data. UniRef clusters proteins hierarchically. At the highest level, UniRef100 groups proteins with identical primary sequences from the UniProt Archive (UniParc). UniRef90 clusters these unique sequences into buckets with 90% sequence similarity, selecting a single sequence from within each cluster as the representative sequence. UniRef50 is then built by clustering these UniRef90 representative sequences into groups with 50% sequence similarity.</p>"},{"location":"datasets/uniprot/#data-used-for-esm-2-pre-training","title":"Data Used for ESM-2 Pre-training","text":"<p>Since the original train/test splits from ESM-2 were not available [3], we replicated the ESM-2 pre-training experiments with UniProt's 2024_03 release. Following the approach described by the ESM-2 authors, we removed artificial sequences and reserved 0.5% of UniRef50 clusters for validation. From the 65,672,139 UniRef50 clusters, this resulted in 328,360 validation sequences. We then ran MMSeqs to further ensure no contamination of the training set with sequences similar to the validation set. This resulted in 65,182,365 training UniRef50 clusters comprising 187,382,018 UniRef90 sequences.</p> <p>Pretraining batches were formed by uniformly sampling each UniRef50 cluster from the training database, taking a randomly chosen UniRef90 sequence from each.</p>"},{"location":"datasets/uniprot/#data-availability","title":"Data Availability","text":"<p>Two versions of the dataset are distributed, a full training dataset (~80Gb) and a 10,000 UniRef50 cluster random slice (~150Mb). To load and use the sanity dataset, the bionemo.testing.data.load function can be used to materialize the sanity dataset in the BioNeMo2 cache directory:</p> <pre><code>from bionemo.testing.data.load import load\n\nsanity_data_dir = load(\"esm2/testdata_esm2_pretrain:2.0\")\n</code></pre>"},{"location":"datasets/uniprot/#ngc-resource-links","title":"NGC Resource Links","text":"<ul> <li>Sanity Dataset</li> <li>[Full Dataset]</li> </ul>"},{"location":"datasets/uniprot/#reference","title":"Reference","text":"<ol> <li> <p>UniProt Consortium. (2023). UniProt: The universal protein knowledgebase in 2023. Nucleic Acids Research, 51(D1),    D523\u2013D531. doi:10.1093/nar/gkac1052</p> </li> <li> <p>Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu, C. H., &amp; UniProt Consortium. (2015). UniRef clusters: a    comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics (Oxford, England),    31(6), 926\u2013932. doi:10.1093/bioinformatics/btu739</p> </li> <li> <p>Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., \u2026 Rives, A. (2023). Evolutionary-scale prediction of    atomic-level protein structure with a language model. Science (New York, N.Y.), 379(6637), 1123\u20131130.    doi:10.1126/science.ade2574</p> </li> </ol>"},{"location":"models/","title":"BioNeMo Framework: Available Models","text":"<p>State-of-the-art models are continually integrated into the BioNeMo Framework. The BioNeMo Framework currently offers the following pre-trained models:</p> Model Modality Uses ESM-2 Protein Representation Learning Geneformer Single Cell Representation Learning <p>For more information about the models included in BioNeMo Framework, refer to the Model Cards linked in the table above or the original publications referenced in the respective model descriptions.</p>"},{"location":"models/esm2/","title":"ESM-2","text":""},{"location":"models/esm2/#model-overview","title":"Model Overview","text":""},{"location":"models/esm2/#description","title":"Description","text":"<p>ESM-2 is a pre-trained, bi-directional encoder (BERT-style model) over amino acid sequences. ESM-2 models provide embeddings for amino acids that have led to state-of-the-art performance on downstream tasks such as structure and function prediction. ESM-2 has been trained at a number of different model sizes. BioNeMo2 includes converted checkpoints for the 650M and 3B parameter variants. The 650M model has 33 layers, 20 attention heads, and a hidden space dimension of 1280. The 3B model has 36 layers, 40 attention heads, and a hidden space dimension of 2,560.</p> <p>These models are ready for commercial use.</p>"},{"location":"models/esm2/#third-party-community-consideration","title":"Third-Party Community Consideration","text":"<p>This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party\u2019s requirements for this application and use case [1]; see link to Non-NVIDIA Model Card for ESM-2 3B model and non-NVIDIA Model Card for ESM-2 650M model</p>"},{"location":"models/esm2/#references","title":"References","text":"<p>[1] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y. and dos Santos Costa, A., 2023. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637), pp.1123-1130.</p> <p>[2] \"UniProt: the universal protein knowledgebase in 2021.\" Nucleic acids research 49, no. D1 (2021): D480-D489.</p> <p>[3] Devlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>"},{"location":"models/esm2/#model-architecture","title":"Model Architecture","text":"<p>Architecture Type: BERT</p> <p>Network Architecture: ESM-2</p>"},{"location":"models/esm2/#input","title":"Input","text":"<p>Input Type(s): Text (Protein Sequences)</p> <p>Input Parameters: 1D</p> <p>Other Properties Related to Input: Protein sequence represented as a string of canonical amino acids, of maximum length 1022. Longer sequences are automatically truncated to this length.</p>"},{"location":"models/esm2/#output","title":"Output","text":"<p>Output Type(s): Embeddings (Amino-acid and sequence-level)</p> <p>Output Parameters: 1D</p> <p>Other Properties Related to Output: Numeric vector with floating-point values corresponding to an embedding for each amino acid in the input protein sequence. Maximum output length is 1022 embeddings - one embedding vector per amino acid.</p>"},{"location":"models/esm2/#software-integration","title":"Software Integration","text":"<p>Runtime Engine(s)</p> <ul> <li>BioNeMo, NeMo, Megatron, TransformerEngine</li> </ul> <p>Supported Hardware Microarchitecture Compatibility</p> <ul> <li>[Ampere]</li> <li>[Hopper]</li> <li>[Volta]</li> </ul> <p>[Preferred/Supported] Operating System(s)</p> <ul> <li>[Linux]</li> </ul>"},{"location":"models/esm2/#model-versions","title":"Model Version(s)","text":"<ul> <li>esm2/650m:2.0</li> <li>esm2/3b:2.0</li> </ul>"},{"location":"models/esm2/#training-evaluation","title":"Training &amp; Evaluation","text":""},{"location":"models/esm2/#training-dataset","title":"Training Dataset","text":"<p>Original ESM-2 checkpoints from HuggingFace were trained with the UniProt 2021_04 sequence database. For more details on the training dataset, see Lin et al. 2023. The train / test splits used by the original authors were not distributed. A pre-training database compiled by NVIDIA following a similar approach is described in UniProt Dataset.</p>"},{"location":"models/esm2/#inference","title":"Inference","text":"<p>Engine: BioNeMo, NeMo</p> <p>Test Hardware</p> <ul> <li>[Ampere]</li> <li>[Hopper]</li> <li>[Volta]</li> </ul>"},{"location":"models/esm2/#license","title":"License","text":"<p>ESM-2 is as provided under the Apache 2.0 license.</p>"},{"location":"models/esm2/#competitive-benchmarking","title":"Competitive Benchmarking","text":""},{"location":"models/esm2/#accuracy","title":"Accuracy","text":"<p>A validation set of 328,360 UniRef50 representative sequences were randomly selected from UniRef 2024_03 (see UniProt Dataset). This validation set was used to ensure that the output of BioNeMo-converted checkpoints is consistent with their outputs when evaluated with the HuggingFace Transformers library.</p> Checkpoint HuggingFace BioNeMo2 Lin et al. 2023 650M 7.001 7.002 6.95  3B 6.003 6.004 6.49  <p>Different Validation Sets</p> <p>The HuggingFace and converted BioNeMo2 checkpoints were evaluated on a newly curated validation set. Perplexities from Lin et al. 2023 are reported for comparison, but the original train/test splits are not available.</p>"},{"location":"models/esm2/#training-performance","title":"Training Performance","text":""},{"location":"models/esm2/#single-node-training-performance","title":"Single-node Training Performance","text":"<p>The pure-pytorch baseline (compiled with <code>torch.compile()</code>) raised an out-of-memory error for batch sizes larger than 16 at the ESM2-650M model size. The <code>bionemo2</code> model could handle batch sizes of 46, reaching a model flops utilization of 59.2% on an NVIDIA A100.</p>"},{"location":"models/esm2/#model-scaling","title":"Model Scaling","text":"<p>Training ESM-2 at the 650M, 3B, and 15B model variants show improved performance with the BioNeMo2 framework over the pure-pytorch baseline. These experiments were conducted on 16x NVIDIA A100 or 16x NVIDIA H100 GPUs split across two nodes.</p>"},{"location":"models/esm2/#device-scaling","title":"Device Scaling","text":"<p>Training ESM-3B on 256 NVIDIA A100s on 32 nodes achieved 96.85% of the theoretical linear throughput expected from extrapolating single-node (8 GPU) performance, representing a model flops utilization of 60.6% at 256 devices.</p>"},{"location":"models/geneformer/","title":"Geneformer","text":"<p>Current checkpoints trained in BioNeMo1</p> <p>This document references performance numbers and runtime engines that are from the bionemo v1 variant of the model. These numbers will be updated in a coming release to reflect the new bionemo v2 codebase. The model architecture and training information will be the same, as checkpoints are converted from bionemo v1 format to v2 format. Benchmarks below are annotated with which version of bionemo generated them. Accuracy should be the same within a small epsilon since we have tests in place showing model equivalency between the two versions.</p>"},{"location":"models/geneformer/#model-overview","title":"Model Overview","text":""},{"location":"models/geneformer/#description","title":"Description:","text":"<p>Geneformer generates a dense representation of a sc-RNA cell by learning co-expression patterns within single cells. Geneformer is a tabular count model trained on sc-RNA from the Chan Zuckerberg Cell x Gene census. Geneformer computes a complete embedding for each cell over the top 1024 expressed genes. The embeddings are used as features for a variety of predictive tasks. This model is ready for both commercial and academic use.</p>"},{"location":"models/geneformer/#references","title":"References:","text":"<ul> <li>Geneformer, reference foundation model for single-cell RNA: Transfer learning enables predictions in network biology | Nature</li> <li>scGPT, alternative foundation model for single-cell RNA: scGPT: toward building a foundation model for single-cell multi-omics using generative AI | Nature Methods</li> <li>scBERT, alternative foundation model for single-cell RNA: scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data | Nature Machine Intelligence</li> <li>scFoundation, alternative foundation model for single-cell RNA: Large Scale Foundation Model on Single-cell Transcriptomics | bioRxiv   Cell x Gene census, public repository for sc-RNA experiments: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)</li> </ul>"},{"location":"models/geneformer/#model-architecture","title":"Model Architecture:","text":"<p>Architecture Type: Bidirectional Encoder Representations from Transformers (BERT) Network Architecture: Geneformer </p>"},{"location":"models/geneformer/#input","title":"Input:","text":"<p>Input Type(s): Number (Row represents cell, containing gene names and single cell expression counts)  Input Format(s): Array AnnData Input Parameters: 1D </p>"},{"location":"models/geneformer/#output","title":"Output:","text":"<p>Output Type(s): Vector (Dense Embedding Predictions)embeddings.  Output Format: NumPy  Output Parameters: 1D  Other Properties Related to Output: Numeric floating point vector (fp16, bf16, or fp32); geneformer-10M-240530 outputs 256 dimensional embeddings; geneformer-106M-240530 outputs 768 dimensional embeddings </p>"},{"location":"models/geneformer/#software-integration","title":"Software Integration:","text":"<p>Runtime Engine(s):</p> <ul> <li>BioNeMo, NeMo 1.2 </li> </ul> <p>Supported Hardware Microarchitecture Compatibility: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>[Preferred/Supported] Operating System(s): </p> <ul> <li>Linux </li> </ul>"},{"location":"models/geneformer/#model-versions","title":"Model Version(s):","text":"<ul> <li>geneformer-10M-240530 </li> <li>10.3M parameter geneformer variant.</li> <li>25429 ensemble ID based gene tokens</li> <li>256 hidden dimensions with 4 heads, 6 layers and an 512 dimensional FFN</li> <li>relu activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> <li>geneformer-106M-240530</li> <li>106M parameter geneformer variant.</li> <li>25429 ensemble ID based gene tokens</li> <li>768 hidden dimensions with 12 heads, 12 layers and an 3072 dimensional FFN</li> <li>relu activation</li> <li>1e-12 EPS layernorm</li> <li>bf16 mixed precision training with 32 bit residual connections</li> <li>2% hidden dropout, 10% attention dropout</li> </ul>"},{"location":"models/geneformer/#training-evaluation","title":"Training &amp; Evaluation:","text":""},{"location":"models/geneformer/#training-dataset","title":"Training Dataset:","text":"<p>Single cell expression counts from CELLxGENE Census used for the direct download of data matching similar criteria to those described in the geneformer publication. limiting cell data to organism=\u201dHomo sapiens\u201d, with a non \u201cna\u201d suspension_type, is_primary_data=True, and disease=\u201dnormal\u201d to limit to non-diseased tissues that are also the primary data source per cell to make sure that cells are only included once in the download. We tracked metadata including \u201cassay\u201d, \u201csex\u201d, \u201cdevelopment_stage\u201d, \u201ctissue_general\u201d, \u201cdataset_id\u201d and \u201cself_reported_ethnicity\u201d. The metadata \u201cassay\u201d, \u201ctissue_general\u201d, and \u201cdataset_id\u201d were used to construct dataset splits into train, validation, and test sets.</p> <p>The training set represented 99% of the downloaded cells. We partitioned the data by dataset_id into a train set (99%) and a hold-out set (1%), to make sure that the hold-out datasets were independently collected single cell experiments, which helps evaluate generalizability to new future datasets.</p> <p>In this training split, we made sure that all \u201cassay\u201d and \u201ctissue_general\u201d labels were present in the training set so that our model would have maximal visibility into different tissues and assay biases.</p> <p>The 1% hold-out evaluation set was split further into a validation and test set. This final split was mostly done randomly by cell; however, we set aside a full dataset into the test split so that we could evaluate performance after training on a completely unseen dataset, including when monitoring the validation loss during training.</p> <p>Link: Datasets downloaded from CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)  ** Data Collection Method by dataset </p> <ul> <li>[Human] </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): 23.64 million non-diseased and human-derived single cells were chosen from the CZI CELLxGENE census, which is characterized as follows: </p> <ul> <li>Assay Bias:</li> <li>The vast majority of the dataset is one of the 10x genomics assays, Approximately 20M of 26M cells are genomic assays, 4M are sci-RNA-seq, while Remaining assays (microwell-seq, drop-seq, bd rhapsody, smart-seq, seq-well, and MARS-seq) represent small fractions of the full datasets.</li> <li>Sex:</li> <li>12.5M are male-derived cells; 10M are female derived cells. The remaining cells are not annotated.</li> <li>Self-Reported Ethnicity:</li> <li>Approximately 12M cells are not annotated; 9M are annotated as \u201cEuropean.\u201d .5M are annotated as \u201cHan Chinese.\u201d followed by \u201cAfrican American\u201d.</li> <li>Age Bias:</li> <li>The dataset is heavily biased toward donors less than one year. The next highest group would be the segment that includes ages 21-30.</li> <li>Tissue Type Bias:</li> <li>9M cells are \u201cbrain\u201d derived. 4M are blood derived, followed by \u201clung\u201d, \u201cbreast\u201d, \u201cheart\u201d and \u201ceye\u201d at approximately 1M cells each.</li> </ul> <p>Dataset was derived from a limited number of public sources where methods and protocols may not represent sufficiently diverse sources to capture the full scope of gene expression.</p>"},{"location":"models/geneformer/#evaluation-dataset","title":"Evaluation Dataset:","text":"<p>Adamson et al 2016 PERTURB-seq dataset, accessed by Harvard dataverse. Link: adamson.zip - Harvard Dataverse  ** Data Collection Method by dataset </p> <ul> <li>Human </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Automated - Molecular Barcoding </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)): There are ~20k single cells, half of which represent unperturbed control samples, and the other half which contain an additional datatable containing the CRISPR knock-out targets for each cell.</p> <p>Link: CZ CELLxGENE Discover - Cellular Visualization Tool (cziscience.com)  ** Data Collection Method by dataset </p> <ul> <li>Human </li> </ul> <p>** Labeling Method by dataset </p> <ul> <li>Hybrid: Automated, Human </li> </ul> <p>Properties (Quantity, Dataset Descriptions, Sensor(s)):</p> <ul> <li>240,000 single cells were chosen from the CZI cell x gene census such that they did not share a <code>dataset_id</code> with any cell in the training data described previously.</li> </ul>"},{"location":"models/geneformer/#inference","title":"Inference:","text":"<p>Engine: BioNeMo, NeMo  Test Hardware: </p> <ul> <li>Ampere </li> <li>Hopper </li> <li>Volta </li> </ul> <p>*Additional description content may be included here</p>"},{"location":"models/geneformer/#ethical-considerations","title":"Ethical Considerations:","text":"<p>NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety &amp; Security, and Privacy Subcards [Insert Link to Model Card++ here]. Please report security vulnerabilities or NVIDIA AI Concerns here.</p>"},{"location":"models/geneformer/#training-diagnostics","title":"Training diagnostics","text":""},{"location":"models/geneformer/#geneformer-10m-240530","title":"geneformer-10M-240530","text":"<p>This checkpoint was trained for approximately 11 epochs through the CELLxGENE split. Training was performed on 8 servers with 8 A100 GPUs each for a total of 115430 steps of per-gpu micro batch size 32 and global batch size of 2048. Training took a total of 1 day, 20 hours and 19 minutes of wallclock time. As can be seen in the following image, training and validation curves both decreased fairly smoothly throughout the course of training. In fact validation (blue) and training (orange) loss were both still decreasing at the end of 11 epochs through the dataset. The model could likely be trained for more epochs without overfitting. </p> <p>Training curves from BioNeMo1</p> <p>Note that these curves were generated on BioNeMo1. We see the same general training curves in our initial testing of BioNeMo2, however. In the following figure the blue line is the previous training run of the 10M model and the red curve is an equivalent training run on BioNeMo2. As we release new checkpoints they will be trained on BioNeMo2.</p> <p></p>"},{"location":"models/geneformer/#geneformer-106m-240530","title":"geneformer-106M-240530","text":"<p>This checkpoint was trained for approximately 11 epochs through the CELLxGENE split. Training was performed on 16 servers with 8 A100 GPUs each for a total of 115430 steps of per-gpu micro batch size 16 and global batch size of 2048. Training took a total of 3 days, 18 hours and 55 minutes of wallclock time. As can be seen in the following image, training and validation curves both decreased fairly smoothly throughout the course of training. In fact validation (blue) and training (orange) loss were both still decreasing at the end of 11 epochs through the dataset. The model could likely be trained for more epochs without overfitting. </p> <p>Additionally, validation loss decreased both faster and continued to decrease at the same improved rate throughout training in the 106M parameter model (red) as compared to the 10M parameter model (blue). It would be interesting to test even larger models to see if we continue to observe improved performance in larger models. </p> <p>!! note \"Training curves from BioNeMo1\"</p> <pre><code>As stated in the previous section, the figures are from our BioNeMo1 code base where these checkpoints were originally\ntrained. As we release new checkpoints they will be trained on BioNeMo2.\n</code></pre>"},{"location":"models/geneformer/#benchmarking","title":"Benchmarking","text":""},{"location":"models/geneformer/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"models/geneformer/#masked-language-model-mlm-loss","title":"Masked language model (MLM) loss","text":"<p>The following describes the bert MLM token loss. Like in the original BERT paper, and the geneformer paper, 15% of all tokens are included in the loss. Of the included tokens, 80% are <code>\"[MASK]\"</code> token, 2% are a random gene token, and 18% are the correct output token. Note that this was an unintentional deviation from the original publication, but so far it seems to be working well. In the future we will test the intended 80%/10%/10% mixture proposed in the paper. The token loss in the following table is the mean cross entropy loss of the 15% of tokens included in the loss mask averaged across cells. As a baseline geneformer was downloaded from the ctheodoris/Geneformer page on hugging face on 2024/11/04 and applied to the same masking/unmasking problem on this dataset, but with model-specific cell representations due to the updated tokenizer and medians dictionary used to train, and the update from training with 2048 tokens to 4096 tokens per cell. The held-out <code>test</code> dataset from our training splits described previously was used, and it should be noted that some of these cells may have been involved in training the baseline geneformer.</p> Model Description Token Loss (lower is better) Baseline geneformer 2.26* geneformer-10M-240530 2.64 geneformer-106M-240530 2.34 <p>Baseline Geneformer was recently updated on huggingface making loss comparisons challenging.</p> <p>Geneformer was recently updated on hugging face to a new version. In a future release we will make checkpoint conversion scripts available so that the public model can be ran directly. Some key differences follow:</p> <ul> <li>Trained on a much larger 95M cell dataset. Our current checkpoints were trained with 23M cells.</li> <li>The new 12 layer baseline geneformer variant sits between our 10M and 106M parameter models in parameter count with   approximately 38M parameters.</li> <li>The model is trained with a 4096 context rather than a 2048 context. When forcing the model to make predictions   with a 2048 context, the MLM loss drops to 2.76, which is probably unfair because this may be \"out of domain\" for   training. It is really hard to compare these loss numbers directly is the only take-home here.</li> <li>The model was trained on a set of 20,275 genes, rather than the older set of 25,426 genes. This would also be   expected to give a boost in loss since there are fewer tokens to choose from.</li> </ul>"},{"location":"models/geneformer/#downstream-task-accuracy","title":"Downstream task accuracy","text":"<p>Here we benchmark four models, with two baselines. These models are tasked with cell type classification, using the Chron's disease small intestine dataset from Elmentaite et al. (2020), Developmental Cell. This dataset contains approximately 22,500 single cells from both healthy children aged 4-13 and chidlren with Chron's disease. This dataset contains 31 unique cell types which we assume to be annotated accurately. This dataset was held out of our pre-training dataset as all diseased samples were removed.</p> <ul> <li>Baseline 1) scRNA workflow: this model uses PCA with 10 components and random forest on normalized and log transformed expression counts to produce a result.</li> <li>Baseline 2) geneformer-qa, a model trained for approximately 100 steps with approximately random weights. We expect this model to perform no differently than working on counts directly.</li> <li>geneformer-10M-240530 and geneformer-106M-240530 as described above.</li> </ul> <p>For more details see the example notebook titled Geneformer-celltype-classification-example.ipynb</p> <p> </p>"},{"location":"models/geneformer/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>The 106M parameter variant of Geneformer achieves over 50 TFLOPS per GPU during training. This is consistent whether trained with 1 or 8 A100s.</p> <p></p> <p>TFLOPS from BioNeMo1</p> <p>We have observed an approximately 10% degradation in training performance comparing the 10M geneformer model on the new BioNeMo v2 repository vs the old BioNeMo v1 codebase. We are working to address this change and make them comparable or better in terms of cluster performance. The numbers above are from the original BioNeMo1 model card.</p> <p></p>"},{"location":"user-guide/","title":"What is BioNeMo?","text":"<p>BioNeMo is a software ecosystem produced by NVIDIA for the development and deployment of life sciences-oriented artificial intelligence models. BioNeMo provides a set of tools to help researchers build, train, and deploy AI models for various biological applications. The main components of BioNeMo are:</p> <ul> <li> <p>BioNeMo Framework: a free-to-use collection of programming tools and packages offering access to optimized, pre-trained biomolecular models and workflows. The framework enables building and customizing models, including training and fine-tuning. Capabilities span various workloads and therapeutic modalities, such as molecular generation, protein structure prediction, protein-ligand, and representation learning.</p> </li> <li> <p>BioNeMo NIMs: easy-to-use, enterprise-ready inference microservices with built-in API endpoints. NIMs are engineered for scalable, self- or cloud-hosted deployment of optimized, production-grade biomolecular foundation models. Check out the growing list of BioNeMo NIMs here.</p> </li> </ul> <p>When choosing between the BioNeMo Framework and BioNeMo NIMs, consider your project's specific requirements. The Framework is ideal for scenarios that require model training, fine-tuning, or customization, offering a comprehensive suite of tools and packages. In contrast, NIMs are optimized for inference-only workflows, providing easy-to-use, enterprise-ready microservices with built-in API endpoints. As a rule, use the Framework for custom model development or high-control modeling, and NIMs for inference against existing models.</p>"},{"location":"user-guide/#bionemo-user-success-stories","title":"BioNeMo User Success Stories","text":"<p>Enhancing Biologics Discovery and Development With Generative AI - Amgen leverages BioNeMo and DGX Cloud to train large language models (LLMs) on proprietary protein sequence data, predicting protein properties and designing biologics with enhanced capabilities. By using BioNeMo, Amgen achieved faster training and up to 100X faster post-training analysis, accelerating the drug discovery process.</p> <p>Cognizant to apply generative AI to enhance drug discovery for pharmaceutical clients with NVIDIA BioNeMo - Cognizant leverages BioNeMo to enhance drug discovery for pharmaceutical clients using generative AI technology. This collaboration enables researchers to rapidly analyze vast datasets, predict interactions between drug compounds, and create new development pathways, aiming to improve productivity, reduce costs, and accelerate the development of life-saving treatments.</p> <p>Cadence and NVIDIA Unveil Groundbreaking Generative AI and Accelerated Compute-Driven Innovations - Cadence's Orion molecular design platform will integrate with BioNeMo generative AI tool to accelerate therapeutic design and shorten time to trusted results in drug discovery. The combined platform will enable pharmaceutical companies to quickly generate and assess design hypotheses across various therapeutic modalities using on-demand GPU access.</p> <p>Find more user stories on NVIDIA's Customer Stories and Technical Blog sites.</p>"},{"location":"user-guide/SUMMARY/","title":"SUMMARY","text":"<ul> <li>What is BioNeMo?</li> <li>Getting Started</li> <li>Background</li> <li>Developer Guide</li> <li>Tutorials</li> <li>Developer Guide</li> <li>Contributing</li> <li>Appendix</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/","title":"Release Notes","text":""},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v20","title":"BioNeMo Framework v2.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features","title":"New Features:","text":"<ul> <li>ESM-2 implementation</li> <li>State of the art training performance and equivalent accuracy to the reference implementation</li> <li>650M, and 3B scale checkpoints available which mirror the reference model</li> <li>Flexible fine-tuning examples that can be copied and modified to accomplish a wide variety of downstream tasks</li> <li>First version of our NeMo v2 based reference implementation which re-imagines bionemo as a repository of megatron models, dataloaders, and training recipes which make use of NeMo v2 for training loops.</li> <li>Modular design and permissible Apache 2 OSS licenses enables the import and use of our framework in proprietary applications.</li> <li>NeMo2 training abstractions allows the user to focus on the model implementation while the training strategy handles distribution and model parallelism.</li> <li>Documentation and documentation build system for BioNeMo 2.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues","title":"Known Issues:","text":"<ul> <li>PEFT support is not yet fully functional.</li> <li>Partial implementation of Geneformer is present, use at your own risk. It will be optimized and officially released in the future.</li> <li>Command line interface is currently based on one-off training recipes and scripts. We are working on a configuration based approach that will be released in the future.</li> <li>Fine-tuning workflow is implemented for BERT based architectures and could be adapted for others, but it requires you to inherit from the biobert base model config. You can follow similar patterns in the short term to load weights from an old checkpoint partially into a new model, however in the future we will have a more direct API which is easier to follow.</li> <li>Slow memory leak occurs during ESM-2 pretraining, which can cause OOM during long pretraining runs. Training with a   microbatch size of 48 on 40 A100s raised an out-of-memory error after 5,800 training steps.</li> <li>Possible workarounds include calling <code>gc.collect(); torch.cuda.empty_cache()</code> at every ~1,000 steps, which appears     to reclaim the consumed memory; or training with a lower microbatch size and re-starting training from a saved     checkpoint periodically.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v19","title":"BioNeMo Framework v1.9","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_1","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable ESM-2nv notebooks demonstrating: Data preprocessing and model training with custom datasets, Fine-tuning on FLIP data, Inference on OAS sequences, Pre-training from scratch and continuing training</li> <li>[Documentation] New notebook demonstrating Zero-Shot Protein Design Using ESM-2nv. Thank you to @awlange from A-Alpha Bio for contributing the original version of this recipe!</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements","title":"Bug fixes and Improvements","text":"<ul> <li>[Geneformer] Fixed bug in preprocessing due to a relocation of dependent artifacts.</li> <li>[Geneformer] Fixes bug in finetuning to use the newer preprocessing constructor.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v18","title":"BioNeMo Framework v1.8","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_2","title":"New Features","text":"<ul> <li>[Documentation] Updated, executable MolMIM notebooks demonstrating: Training on custom data, Inference and downstream prediction, ZINC15 dataset preprocesing, and CMA-ES optimization</li> <li>[Dependencies] Upgraded the framework to NeMo v1.23, which updates PyTorch to version 2.2.0a0+81ea7a4 and CUDA to version 12.3.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_1","title":"Bug fixes and Improvements","text":"<ul> <li>[ESM2] Fixed a bug in gradient accumulation in encoder fine-tuning</li> <li>[MegaMolBART] Make MegaMolBART encoder finetuning respect random seed set by user</li> <li>[MegaMolBART] Finetuning with val_check_interval=1 bug fix</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_1","title":"Known Issues","text":"<ul> <li>Minor training speed regression observed for models DNABERT, Geneformer, MolMIM</li> <li>Two known critical CVEs GHSA-cgwc-qvrx-rf7f, GHSA-mr7h-w2qc-ffc2. The vulnerabilities arise within a package that's installed by lightning by default. We do not use that package in bionemo framework container. we are also unable to remove the package in question as it's installed as a side-effect of installing lightning.</li> <li>Two known High CVEs from pytorch : GHSA-pg7h-5qx3-wjr3, GHSA-5pcm-hx3q-hm94.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v17","title":"BioNeMo Framework v1.7","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models","title":"New Models","text":"<ul> <li>DSMBind, developed under the BioNeMo framework, is a model which can produce comparative values for ranking protein-ligand binding affinities. This release features the capability to perform inference using a newly trained checkpoint.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_3","title":"New Features","text":"<ul> <li>[EquiDock] Remove steric clashes as a post-processing step after equidock inference.</li> <li>[Documentation] Updated Getting Started section which sequentially describes prerequisites, BioNeMo Framework access, startup instructions, and next steps.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_2","title":"Known Issues","text":"<ul> <li>There is a known security vulnerability with NLTK that can allow for arbitrary code execution via pickle files that are external assets downloaded via nltk.download() (https://github.com/nltk/nltk/issues/3266). BioNeMo itself does not use this dependency in any way, however parts of NeMo text-to-speech (nemo.collections.tts) does use this vulnerable codepath. Since NeMo is installed in the BioNeMo release containers, users are urged to exercise caution when using  nemo.collections.tts or nltk.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v16","title":"BioNeMo Framework v1.6","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-features_4","title":"New Features","text":"<ul> <li>[Model Fine-tuning] <code>model.freeze_layers</code> fine-tuning config parameter added to freeze a specified number of layers. Thank you to github user @nehap25!</li> <li>[ESM2]  Loading pre-trained ESM-2 weights and continue pre-training on the MLM objective on a custom FASTA dataset is now supported.</li> <li>[OpenFold] MLPerf feature 3.2 bug (mha_fused_gemm) fix has merged.</li> <li>[OpenFold] MLPerf feature 3.10 integrated into bionemo framework.</li> <li>[DiffDock] Updated data loading module for DiffDock model training, changing from sqlite3 backend to webdataset.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v15","title":"BioNeMo Framework v1.5","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_1","title":"New Models","text":"<ul> <li>Geneformer is out of Beta status. This release includes newly trained checkpoints and benchmarks, including a variant based on the publication with 10M parameters, and the largest variant of geneformer publically available to date with 106M parameters.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v14","title":"BioNeMo Framework v1.4","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_2","title":"New Models","text":"<ul> <li>Beta Geneformer a foundation model for single-cell data that encodes each cell as represented by an ordered list of differentially expressed genes for that cell.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_5","title":"New Features","text":"<ul> <li>Beta Geneformer pretraining with custom datasets</li> <li>Low-Rank Adaptation (LoRA) finetuning for ESM2</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_2","title":"Bug fixes and Improvements","text":"<ul> <li>OpenFold training improved benchmarks and validation of optimizations</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_3","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.04 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact. A fix for this is scheduled in the 24.05 (May) release.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v13","title":"BioNeMo Framework v1.3","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_3","title":"New Models","text":"<ul> <li>MolMIM implementation under BioNeMo framework, a small molecule model developed at NVIDIA which can be used to produce embeddings and novel molecules.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_6","title":"New Features","text":"<ul> <li>MolMIM re-trained on more data is now available in the framework, and achieves state of the art performance.</li> <li>MolMIM property guided tutorial notebook covering property guided optimization using our new framework model.</li> <li>MolMIM training tutorial available walking users through either training from scratch or from an existing checkpoint on your own data.</li> <li>MolMIM tutorial notebook covering molecular sampling and property prediction is also now available.</li> <li>Numerous optimizations from NVIDIA's entry to the MLPerf competition have been added to OpenFold. Documentation and detailed benchmarks are works in progress and will be published in upcoming releases. This release contains the following performance optimizations:<ul> <li>Fused GEMMs in multi-head attention (MHA)</li> <li>Non-blocking data pipeline</li> <li>BF16 precision training</li> <li>Fused MHA gating</li> <li>Inductor Compiled LayerNorm</li> <li>OpenAI Triton LayerNorm kernels</li> <li>OpenAI Triton MHA</li> </ul> </li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_3","title":"Bug fixes and Improvements","text":"<ul> <li>NeMo upgraded to v1.22 (see NeMo release notes),</li> <li>PyTorch Lightning upgraded to 2.0.7</li> <li>NGC CLI has been removed from the release container. If users     download models from inside the container (e.g. using <code>bionemo_data_download</code> or via running specific unit tests),     the NGC CLI will be auto-installed to pull the models from NGC.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_4","title":"Known Issues","text":"<ul> <li>BioNeMo Framework v24.03 container is vulnerable to GHSA-whh8-fjgc-qp73 in onnx 1.14.0. Users are advised not to open untrusted onnx files with this image. Restrict your mount point to minimize directory traversal impact.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v12","title":"BioNeMo Framework v1.2","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_4","title":"New Models","text":"<ul> <li>OpenFold implementation under BioNeMo framework, derived from public OpenFold and DeepMind AlphaFold-2.</li> <li>DNABERT implementation for computing embeddings for each nucleotide in the input DNA sequence.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_7","title":"New Features","text":"<ul> <li>Training recipes for DNABERT and OpenFold, including automated data processing and full configuration for training.</li> <li>Example tutorials for running inference using OpenFold.</li> <li>Splice Prediction downstream task example for DNABERT.</li> <li>Wrapper scripts for DNABERT and OpenFold to launch jobs on BCP.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_4","title":"Bug fixes and Improvements","text":"<ul> <li>Interface improvements for ESM-2 data ingestion and pre-processing. The interface allows for explicit specification of training, validation, and test sets. The user may set <code>config.model.data.default_dataset_path</code> to maintain prior behavior, or set <code>config.model.data.train.dataset_path</code>, <code>config.model.data.val.dataset_path</code>, <code>config.model.data.test.dataset_path</code> which may all be unique.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_5","title":"Known Issues","text":"<ul> <li>OpenFold training speed does not yet include MLPerf optimizations, and these will be released in the subsequent release.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v11","title":"BioNeMo Framework v1.1","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_5","title":"New Models","text":"<ul> <li>EquiDock for protein-protein docking pose prediction</li> <li>DiffDock for protein-ligand blind docking pose generation</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_8","title":"New Features","text":"<ul> <li>Training recipes for EquiDock and DiffDock, including automated data processing and full configuration for training.</li> <li>Accelerated inference and training for DiffDock via fast tensor-product kernels.</li> <li>Example tutorials for running inference using EquiDock and DiffDock.</li> <li>Recipes for running EquiDock and DiffDock on BCP and Slurm.</li> <li>Pipeline parallel supported for ESM-2nv.</li> <li>Migration of inference notebooks to using pytriton.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bug-fixes-and-improvements_5","title":"Bug fixes and Improvements","text":"<ul> <li>Faster pre-processing of data on BCP.</li> <li>Refactor of download_models.sh to download_models.py for easier CLI use.</li> <li>Refactor of install structure to move from /opt/nvidia to /workspace/bionemo. The environment variable $BIONEMO_HOME now points to the repo base and is required to be set for tests to pass.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#security-notice","title":"Security Notice","text":"<p>SchedMD Slurm in the release container is shipped with a security vulnerability, CVE-2022-29501, and therefore this version of Slurm should not be used to run a Slurm cluster (specifically, the processes <code>slurmdbd</code>, <code>slurmctld</code>, and <code>slurmd</code>.</p> <p>In general, the BioNeMo Framework release is designed to ship code and an environment that would be executed on local workstations, or deployed on clusters for large scale training jobs. This container is not designed to run as a service with public facing APIs. A full summary of security vulnerabilities can be found here.</p>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v10","title":"BioNeMo Framework v1.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_6","title":"New Models","text":"<ul> <li>ESM-2nv for protein sequence representations, pretrained weights of ESM-2 650M and ESM-2 3B converted from HF checkpoint available.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_9","title":"New Features","text":"<ul> <li>Pre-training recipes for ESM-2nv, including automated data processing and full configuration for training</li> <li>Fine-tuning of ESM-2nv with encoder frozen or trainable</li> <li>Downstream task finetuning support for single-value classification (e.g. subcellular localization), single-value regression (e.g. meltome) and per-token classification (e.g. secondary structure)</li> <li>Validation in loop to evaluate performance on downstream tasks during training</li> <li>Example tutorials for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#bionemo-framework-v040","title":"BioNeMo Framework v0.4.0","text":""},{"location":"user-guide/appendix/releasenotes-fw/#new-models_7","title":"New Models","text":"<ul> <li>ESM-1nv for protein sequence representations, pretrained weights available</li> <li>ProtT5nv for protein sequence representation and sequence-to-sequence tasks, pretrained weights available</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-features_10","title":"New Features","text":"<ul> <li>Pre-training for all models, including automated data processing and full configuration for training</li> <li>Fine-tuning of MegaMolBART, ESM-1nv, and ProtT5nv with encoder frozen or trainable</li> <li>Downstream task example applications \u2013 secondary structure prediction for ESM-1nv and ProtT5nv, physchem prediction (lipophilicity, FreeSolv, ESOL) and retrosynthesis prediction for MegaMolBART</li> <li>Validation in loop to evaluate performance on downstream tasks during training: physchem prediction (MegaMolBART) and secondary structure prediction (ESM-1nv and ProtT5nv).</li> <li>Pipeline parallelism supported as a beta feature. Not fully tested.</li> <li>Example notebooks for pre-training, fine tuning, and downstream tasks</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#known-issues_6","title":"Known Issues","text":"<ul> <li>Data preprocessing on DGX Cloud is slow. Faster to do it on a local machine.</li> </ul>"},{"location":"user-guide/appendix/releasenotes-fw/#new-apis","title":"New APIs","text":"<ul> <li>BioNeMoDataModule - Encapsulates dataset instantiation in bionemo models so that many different datasets can be used with the same model</li> <li>EncoderFineTuning - Base class to facilitate implementation of downstream tasks built on embeddings from other models</li> </ul>"},{"location":"user-guide/background/SUMMARY/","title":"SUMMARY","text":"<ul> <li>NeMo2 Parallelism</li> <li>Megatron Dataset Considerations</li> </ul>"},{"location":"user-guide/background/megatron_datasets/","title":"Writing Megatron-LM Compatible Datamodules","text":"<p>Megatron-LM relies on determinism in the training dataset classes to ensure that input tensors are initialized correctly across model-parallel ranks (see NeMo2 Parallelism). As a consequence, new dataset classes must be careful to preserve the required determinism. Common operations such as data augmentation, masking, etc. can cause <code>dataset[i]</code> to return random results for a given index, breaking this megatron contract.</p>"},{"location":"user-guide/background/megatron_datasets/#multi-epoch-training","title":"Multi-Epoch Training","text":"<p>One training regime where this limitation is most apparent is is multi-epoch training, where standard training recipes would apply different random masks or different data augmentation strategies each time the data is encountered. BioNeMo provides a number of utilities that make multi-epoch training easier while still obeying the determinism requirements of megatron.</p> <p>The MultiEpochDatasetResampler class simplifies the process of multi-epoch training, where the data should both be re-shuffled each epoch with different random effects applied each time the data is seen. To be compatible with this resampler, the provided dataset class's <code>__getitem__</code> method should accept a EpochIndex tuple that contains both an epoch and index value. Random effects can then be performed by setting the torch random seed based on the epoch value:</p> <pre><code>class MyDataset:\n    def __getitem__(self, idx: EpochIndex):\n        rng = torch.Generator()\n        rng.manual_seed(idx.epoch)\n        ...\n</code></pre> <p>Avoid <code>torch.manual_seed</code></p> <p>Megatron-LM handles torch seeding internally. Calling <code>torch.cuda.manual_seed</code> inside the user-provided dataset can cause issues with model parallelism. See megatron/core/tensor_parallel/random.py#L198-L199 for more details.</p> <p>For deterministic datasets that still want to train for multiple epochs with epoch-level shuffling, the IdentityMultiEpochDatasetWrapper class can simplify this process by wrapping a dataset that accepts integer indices and passing along the EpochIndex index values from the resampled dataset.</p> <pre><code>class MyDeterministicDataset:\n    def __getitem__(self, index: int):\n        ...\n\ndataset = IdentityMultiEpochDatasetWrapper(MyDeterministicDataset())\nfor sample in MultiEpochDatasetResampler(dataset, num_epochs=3, shuffle=True):\n    ...\n</code></pre> <p>Very large datasets</p> <p>For datasets where <code>len(dataset)</code> is too large for a shuffled list of indices to comfortably fit in memory, PRNGResampleDataset offers a simple solution for shuffling a dataset with replacement in O(1) memory.</p>"},{"location":"user-guide/background/megatron_datasets/#training-resumption","title":"Training Resumption","text":"<p>To ensure identical behavior with and without job interruption, BioNeMo provides MegatronDataModule to save and load state dict for training resumption, and provides WrappedDataLoader to add a <code>mode</code> attribute to DataLoader.</p> <pre><code>class MyDataModule(MegatronDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ...\n\n    def train_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"train\",\n        )\n\n    def val_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"validation\",\n        )\n\n    def test_dataloader(self):\n        self.update_init_global_step()  # required to set the correct `global_step` for resumption\n        return WrappedDataLoader(\n            ...,  # any other arguments for DataLoader\n            mode=\"test\",\n        )\n</code></pre> <p>MegatronDataModule</p> <p>Users will see non-overlapping training curve if their datamodule is not inheritting from <code>MegatronDataModule</code>, unless similar logics are handled by the users. In <code>MegatronDataModule</code>, <code>self.update_init_global_step()</code> must be called right before the dataloaders are returned to ensure that training resumes with the correct sample index instead of restarting from 0 everytime. We recommend users to inherit from <code>MegatronDataModule</code> similar to the pattern above.</p> <p>WrappedDataLoader</p> <p>The <code>WrappedDataLoader</code> class is a wrapper around the PyTorch DataLoader class that adds the <code>mode</code> attribute to the dataloader. The dataloader will resume from the last sample index only when mode is 'train'. <code>val_dataloader</code> and <code>test_dataloader</code> are unaffected.</p> <p>WARNING: 'train' is the default value of <code>mode</code> in <code>WrappedDataLoader</code>. If not set, users might find their validation/test dataloader changes behavior by resuming from a non-zero sample index.</p>"},{"location":"user-guide/background/megatron_datasets/#testing-datasets-for-megatron-compatibility","title":"Testing Datasets For Megatron Compatibility","text":"<p>BioNeMo also provides utility functions for test suites to validate that datasets conform to the megatron data model. The assert_dataset_compatible_with_megatron function calls the dataset with identical indices and ensures the outputs are identical, while also checking to see if <code>torch.manual_seed</code> was used.</p> <p>Example datasets in BioNeMo</p> <p>The ESMMaskedResidueDataset demonstrates one approach for leveraging EpochIndex indices to perform epoch-level randomization within the confines of megatron's data model.</p>"},{"location":"user-guide/background/nemo2/","title":"NeMo2 Parallelism","text":"<p>NeMo2 represents tools and utilities to extend the capabilities of <code>pytorch-lightning</code> to support training and inference with megatron models. While pytorch-lightning supports parallel abstractions sufficient for LLMs that fit on single GPUs (distributed data parallel, aka DDP) and even somewhat larger architectures that need to be sharded across small clusters of GPUs (Fully Sharded Data Parallel, aka FSDP), when you get to very large architectures and want the most efficient pretraining and inference possible, megatron-supported parallelism is a great option.</p> <p>So in other words, NeMo2 adds the Megatron strategy in addition to the standard DDP and FSDP strategies.</p> <p>Many downstream constraints and conventions are driven by the underlying constraints of megatron.</p>"},{"location":"user-guide/background/nemo2/#deeper-background-on-megatron","title":"Deeper background on megatron","text":""},{"location":"user-guide/background/nemo2/#other-options-for-parallelizing-smaller-models","title":"Other options for parallelizing smaller models","text":"<p>Megatron is a system for supporting advanced varieties of model parallelism. While vanilla models can be executed in parallel with systems such as distributed data parallel (DDP) or moderately large models can be trained with Meta's Fully Sharded Data Parallel (FSDP/FSDP2), when you get to very large models and you want to train them with maximal efficiency, you want some variant of megatron.</p>"},{"location":"user-guide/background/nemo2/#ddp-background","title":"DDP background","text":"<p>DDP is the best option when you can fit the entire model on every GPU in your cluster. With DDP, you can parallelize your <code>global batch</code> across multiple GPUs by splitting it into smaller <code>mini-batches</code>, one for each GPU. Each GPU computes the forward and backward pass independently for its subset of data, allowing for maximal utilization. Synchronization of gradients occurs after the backward pass is complete for each batch, followed by a weight update that ensures all GPUs have synchronized parameters for the next iteration. Here is an example of how this might appear on your cluster with a small model:</p> <p></p>"},{"location":"user-guide/background/nemo2/#fsdp-background","title":"FSDP background","text":"<p>FSDP extends DDP by sharding (splitting) model weights across GPUs in your cluster to optimize memory usage. While data is still split across GPUs in the same way as DDP, FSDP strategically synchronizes and broadcasts the necessary shards of model weights to all GPUs just-in-time for computation during the forward pass.</p> <p>For example, when a layer is needed for computation, the owning GPU sends that shard of weights to the other GPUs, which then perform the forward computation on that layer. After the computation is complete, FSDP frees the memory for that layer on all GPUs except the one that owns the shard. This process continues iteratively for each layer until the entire model has been executed on the data.</p> <p>Note that this process parallelizes the storage in a way that enables too large models to be executed (assuming a single layer is not too large to fit on a GPU). Megatron (next) co-locates both storage and compute.</p> <p>The following two figures show two steps through the forward pass of a model that has been sharded with FSDP.  </p>"},{"location":"user-guide/background/nemo2/#model-parallelism","title":"Model Parallelism","text":"<p>Model parallelism is the catch-all term for the variety of different parallelism strategies that could be applied to parallelizing your model across a cluster. Below we explain several varieties of model parallelism that are implemented in megatron. As mentioned in the previous section, one key advantage to the megatron-specific parallelism types described next are that they co-locate storage and compute of the layers. Inefficiencies caused by naive scheduler implementations are also addressed (discussed in the section on schedulers).</p>"},{"location":"user-guide/background/nemo2/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism is similar to FSDP, but the model blocks that are sharded are also computed in parallel on the nodes that own the model weight in question. You can think of this as a larger simulated GPU that happens to be spread across several child GPUs. Examples of this include <code>parallel_state.is_pipeline_last_stage()</code> which is commonly used to tell if a particular node is on last pipeline stage, where you compute the final head outputs, loss, etc. . Similarly there are convenience environmental lookups for the first pipeline stage (where you compute the embedding for example) <code>parallel_state.is_pipeline_first_stage()</code>.</p>"},{"location":"user-guide/background/nemo2/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism represents splitting single layers across GPUs. This can also solve the problem where some individual layers could in theory be too large to fit on a single GPU, which would make FSDP not possible. This would still work since individual layer weights (and computations) are distributed. Examples of this in megatron include <code>RowParallelLinear</code> and <code>ColumnParallelLinear</code> layers. </p>"},{"location":"user-guide/background/nemo2/#sequence-parallelism","title":"Sequence Parallelism","text":"<p>In megatron, \"sequence parallelism\" refers to the parallelization of the dropout, and layernorm blocks of a transformer. The idea is roughly as follows. First, remember that in a typical transformer architecture, the <code>embedding_dimension</code> is the only dimension that <code>LayerNorm</code> is applied over. Similarly, Dropout (outside of the attention block) is an operation that is applied on the last embedding dimension. These two layers are independent over the sequence dimension, so they can be processed in blocks on separate GPUs. As can be seen in the following figure, the initial <code>LayerNorm</code> in a multi-headed transformer block is executed in parallel. Next the results are gathered for the self attention and linear layers (which are typically set up for tensor parallelism). Next the result from those layers is scattered back to sequence parallel nodes which execute dropout, do a residual connection from the previous sequence parallel output, and a layernorm. Next those results are again gathered for the final FFN and activation layers prior to a final scattering across sequence parallel GPUs for the output of that transformer block. </p> <p>As a user, if you know that your transformer is executed in parallel and you have custom losses or downstream layers, you need to make sure that the appropriate gather operations are occurring for your loss computation etc.</p>"},{"location":"user-guide/background/nemo2/#context-parallelism","title":"Context Parallelism","text":"<p>Context parallelism extends sequence parallelism by also parallelizing the attention mechanism itself, similar to Ring Attention. In general, if you are using a transformer, context parallelism is going to perform better than sequence parallelism for very long input sequences. That said, due to the necessity of all-gather and reduce scatter operations throughout the architecture, the general advice that you should avoid these kinds of parallelism if a micro-batch fits on a single device still holds. Splitting across elements in a global batch represent the fewest necessary communications between GPUs on your cluster, so standard DDP should run the fastest if you can get your training loop for a micro batch to fit on one GPU.</p>"},{"location":"user-guide/background/nemo2/#mixing-parallelism-strategies","title":"Mixing parallelism strategies","text":"<p>You can mix multiple kinds of parallelism together to achieve a more performant result. In general experimentation should be done to identify the optimal mix of parallelism. See this YouTube tutorial from Jared Casper for more background on megatron parallelism strategies.</p> <p>Below is a figure demonstrating how mixing strategies results in larger \"virtual GPUs\", which similarly means you have fewer distinct micro-batches in flight across your cluster. Also note that the number of virtual GPUs is multiplicative so if you have <code>TP=2</code> and <code>PP=2</code> then you are creating a larger virtual GPU out of <code>2*2=4</code> GPUs, so your cluster size needs to be a multiple of 4 in this case. </p>"},{"location":"user-guide/background/nemo2/#scheduling-model-parallelism","title":"Scheduling model parallelism","text":"<p>You can improve on naive schedules by splitting up micro-batches into smaller pieces, executing multiple stages of the model on single GPUs, and starting computing the backwards pass of one micro-batch while another is going through forward. These optimizations allow for better cluster GPU utilization to be achieved. For example the following figure shows how more advanced splitting techniques in megatron (eg the interleaved scheduler) provide better utilization when model parallelism is used. Again when you can get away without using model parallelism (DDP), that is generally the best approach. </p>"},{"location":"user-guide/contributing/code-review/","title":"Code Review","text":"<p>This document describes the process and etiquette for code review in the BioNeMo repo. You should read this document if you are a developer working in the BioNeMo repo.</p> <p>The purpose of these guidelines is to help reduce the friction between engineers writing code and those reviewing code. As with many rules, there are exceptions. These exceptions are not comprehensive, so if you find exceptions that should be listed, please raise them so they can be evaluated for their inclusion.</p>"},{"location":"user-guide/contributing/code-review/#code-review-process","title":"Code Review Process","text":"<p>The code review process is progressive:</p> <ol> <li>Review by your team</li> <li>Review by domain experts (CODEOWNERS)</li> <li>Approval by Approval-list users.</li> <li>(Optional) Coverage Check approval by Approval-list users.</li> </ol>"},{"location":"user-guide/contributing/code-review/#1-team-review","title":"1. Team Review","text":"<p>You should first ask contributors to review your change. The contributing team can provide the most contextualized feedback, and this review step is where most issues with the change should be caught and addressed.</p> <p>Proceed to the next step after you've addressed your team's comments and have received an approval. There is no actual requirement in gitlab to receive your team-based approval - it is simply best practice.</p>"},{"location":"user-guide/contributing/code-review/#2-owner-review","title":"2. Owner Review","text":"<p>Code owners are domain experts for a particular part of the repository. They are typically the original authors of a set of source files. Code ownership structures tend to mirror team structures; a single team is often responsible for a functional component.</p> <p>You must receive approval from at least one owner of every file you change. Unless the file(s) do not have any owners specified in the <code>CODEOWNERS</code> file.</p> <p>If your change only modifies files owned by your team, owner review happens implicitly in the team review step (i.e. in many cases you may already have this requirement satisfied after step 1 above).</p>"},{"location":"user-guide/contributing/code-review/#3-approval","title":"3. Approval","text":"<p>You must also receive approval for your change. Approval indicates that the change is ready to be merged, and is the final step in the review process. Self approval is strictly forbidden.</p> <p>Approval is granted by an approver, in the form of an approval stamp in gitlab. Approvers review an entire change, in contrast to owners, who focus on reviewing the files that they own.</p> <p>To find an approver, first ask your team if there is a preferred approver. You can also check in #clara-discovery-bionemo-dev.</p> <p>*If your team has an approver, approval will usually be granted shortly after your team has had a chance to review your change.</p> <p>*Often, a single review from one person will be sufficient to complete the entire review process. If the reviewer is on your team, is an approver, and is an owner of the files you modified, then the process may collapse to a single review.</p>"},{"location":"user-guide/contributing/code-review/#4-coverage-check","title":"4. Coverage Check","text":"<p>Our repository has automated checks to ensure test coverage has not regresed. The coverage check approvers will be the same as the Approval-list users. If codeline test coverage regresses, the Approvers must make a judgement call whether it is acceptible or not the merge the code. Occaisonally the coverage check algorithm has a false positive (i.e. code coverage doesn't regress, yet coverage check approval is flagged by gitlab), and in this case Approvers are i free to simply approve the false coverage regression.</p>"},{"location":"user-guide/contributing/code-review/#responsibilities","title":"Responsibilities","text":""},{"location":"user-guide/contributing/code-review/#all-commenters-reviewers-owners-approvers-etc","title":"All Commenters (Reviewers, Owners, Approvers, etc.)","text":"<p>If a comment thread is start by anyone, it is expected that the thread starter resolves the comment. Resolving a thread by the original thread starter indicates that the person who started the discussion is happy with the outcome.</p>"},{"location":"user-guide/contributing/code-review/#reviewers","title":"Reviewers","text":"<p>All developers can and should review changes. These reviews are fundamental to maintaining the quality of the codebase. Reviews are also an important way to stay aware of what people are working on and increase the bus factor for areas of the code.</p> <p>Reviewers should always do the following when reviewing code:</p> <ul> <li> <p>Be respectful.</p> </li> <li> <p>Assume best intentions.</p> </li> <li> <p>Review the code, not the person.</p> </li> <li> <p>Leave clear, actionable feedback. Use comments in gitlab to signal that you expect   changes to be made, and explicitly enumerate them. If you don't, it can leave the author of the patch wondering if   your comments are optional. Requesting code changes should not be interpreted as a   judgment of the change, but rather as an indicator that the   reviewer wants to engage with the author to turn it into an   approval.</p> </li> </ul> <p>A more detailed etiquette guide can be found later in this document.</p>"},{"location":"user-guide/contributing/code-review/#owners","title":"Owners","text":"<p>As an owner, you have the ability to delay code from being merged by withholding your signoff. This ability comes with important responsibilities:</p> <ul> <li> <p>A duty to respond to reviews in a timely manner. If you can't stay   on top of review requests, you should relinquish ownership.</p> </li> <li> <p>A bias to \"yes\". \"No\", by itself, is never an acceptable answer.   When you reject a change you must work with the change author to   arrive at a mutually agreeable solution.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#approvers","title":"Approvers","text":"<p>As an approver, you have additional responsibilities beyond that of an owner:</p> <ul> <li> <p>You are responsible for the change and the effect it has on the   codebase.</p> </li> <li> <p>You must ensure that reviews have been performed by the appropriate   reviewers, that these reviews were not rushed.</p> </li> <li> <p>If a change breaks something, you are expected to be actively   involved in the cleanup.</p> </li> </ul> <p>You should always decline to approve a change if you're unfamiliar with the areas of code it touches.</p>"},{"location":"user-guide/contributing/code-review/#becoming-an-owner","title":"Becoming an Owner","text":"<p>Code ownership information is stored in CODEOWNERS file. Since these CODEOWNERS file are stored in the repository, changing them follows the same process as changing code.</p> <p>To become an owner, add your github username to the CODEOWNERS file that you want to be a part of, and submit the change to Gitlab. The change to the CODEOWNERS file will follow the same review process as any other code change. The existing owners will decide whether to delegate ownership to you or not.</p>"},{"location":"user-guide/contributing/code-review/#becoming-an-approver","title":"Becoming an Approver","text":"<p>As an approver, you are responsible for ensuring the consistency and integrity of our code base. Before becoming an approver, study this document so that you are completely familiar with the responsibilities of reviewers and approvers. Additionally, make sure that you are intimately familiar with our coding style guides and best practices:</p> <ul> <li>Contributing</li> <li>In addition, make sure that you understand and can apply all elements of the     Google Python style guide, which we adhere     to for all Python code</li> </ul>"},{"location":"user-guide/contributing/code-review/#details-on-etiquette-for-good-citizens","title":"Details on Etiquette for Good Citizens","text":"<p>In the following sections, we provide deeper thoughts and recommendations for everyone contributing to the repo, in order to have a fruitful interaction across the team members.</p>"},{"location":"user-guide/contributing/code-review/#patch-changes-sizes-and-policies","title":"Patch changes, sizes, and policies","text":"<ul> <li> <p>It takes 30-45 minutes to review every 100-200 lines of code. Be   mindful of the size of changes you are introducing and try to keep   your patch under 500 lines of code whenever possible. As a change   grows in size (lines of codes) the reviewer's ability to find   issues diminishes.</p> </li> <li> <p>As a corollary, for a refactor-type change, consider separating     \"no-logic-change\" and \"logic-change\" into distinct reviews     (perhaps in a dependent review change) to ease the pattern     matching burden on your reviewers.</p> </li> <li> <p>All reviewers may request an PR is too large if it is larger than   500 lines of net code addition. The only exception are MRs into   the <code>bionemo2/contrib</code> directory, where larger MRs are permissible.   This includes lines of code, but not something such as dummy data or   a fake dataset that may contain thousands of lines of stuff that is not   actually functional code.</p> </li> <li> <p>Each patch should be kept to one logical change, which should be   described in the title of the patch. Unrelated changes should be   split out into separate patches. Fixing whitespace on a line   you're editing is reasonable. Fixing whitespace around the code   you're working on should be a separate 'cleanup' patch.</p> </li> <li> <p>Where possible, larger patches (&gt;500 LOC) should be split into   multiple smaller patches that are consistent individually. Test   your patches before submitting them to Gitlab via gitlab pipelines or locally.   The more you test before submitting your patch for review, the better. It's also   appreciated if you add a line to the commit message describing how   the patch was tested. This prevents people from having to ask   whether and how the patch was tested. Stating that the patch was   not tested is also fine, although you might be asked to do some   testing in cases where that would be reasonable.</p> </li> <li> <p>Abandon patches that are no longer useful or that you don't intend   to keep working on.</p> </li> <li> <p>Follow code styling and rules stated in the project's documents   (for example, contributing.md, of which the Google Python   Style Guide is a subet) as these define the   look and feel of the code which defines the most fundamentals of how the code should be   developed and allows reviewers to focus on the most important aspects of a new piece of code.   For bash scripting please follow the Google Shell Style Guide here</p> </li> <li> <p>We follow a revert + fix policy in the codebase for any showstopper   bug that might appear as a result of an PR introducing errors not   caught by sanity. In exceptional circumstances when an MR cannot be   reverted and there is a hotfix ready, leadership can consider   merging without revert. However, this should be an exception.   Failures policies are described in more depth here.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#review-timelines","title":"Review timelines","text":"<ul> <li> <p>In general, patches should remain open for review for at least 24   hours since the last significant modification to the change. The   purpose is to let developers around the world have a chance to   review. Complex reworks, even if they don't change the purpose of   the patch but the way it's implemented, should restart the waiting   period.</p> </li> <li> <p>Speedy changes: A change can go in without the waiting period if its   purpose is to fix a recently-introduced issue that has not been   possible to revert. In that case, the commit message has to   explain what change introduced the problem and the nature of the   problem so that the emergency need becomes apparent. The change   itself should be as limited in scope and impact as possible to   make it simple to assess the impact.</p> </li> <li> <p>Trivial changes that deal with minor issues like inconsistencies in   whitespace or spelling fixes that don't impact the final binary   output also don't need to wait for the round of the world reviews.   Such changes should point out in their commit messages how the   author verified that the binary output is identical. Note that   trivial fixes shouldn't necessarily be expedited: Just like   they're not critical enough for things to go wrong because of   them, they're not critical enough to require quick handling.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#reviewers_1","title":"Reviewers","text":"<ul> <li> <p>Do not approve if you have not reviewed the code you were asked to   review. Spend time reviewing or re-assign to someone else. Someone   that approves the change must review the entire change holistically   If you are a code owner of a particular file, it is appropriate to only reviews the files you own.</p> </li> <li> <p>If request an PR change their code, you are responsible for giving concrete   recommendations for what could be changed to resolve the issue the   patch addresses. If you feel strongly that a patch should NEVER be   merged, you are responsible for defending your position and   listening to other points of view. Asking for changes and walking away is   not acceptable, and may cause your approval status to be removed by the   leadership team.</p> </li> <li> <p>Include justification for critique: When you review code, you should   always try to include justification for your critique, unless that   critique is a nit, a style guide violation, or an obvious bug.   Nits and style guide violations tend to overlap, and you shouldn't   have to justify the use of a shared style guide or things like   proper spelling. Likewise, you shouldn't have to justify bug-free   code. However, when it comes to design choices you should always   include justification for when you might want to change certain   things</p> </li> <li> <p>If there have been comments or discussion on a patch, verify that   the comments have been addressed before giving an approval. If you feel   that a comment is invalid, please respond to that comment instead   of just ignoring it.</p> </li> <li> <p>Be conscientious when approving patches. As the arbiter, you need to make sure the MR   is complete, that proper reviews are done, that all the required   tests have passed, that API changes have been reviewed by the API   owners. Please make sure that the necessary convergence tests and unit tests   have passed and have not regressed. If KPI regression (convergence tests) is found, you may need to   consult with other stakeholders before approving the MR. In some   cases a Cl will require convergence testing. Owners/ Approvers   should make sure convergence testing is performed when necessary and   that no new issues are identified. And that overall, the code   changes are sound and integrate well with the rest of the modules   and systems. In the event that the patch breaks things, you are   expected to be actively involved in the cleanup effort and support   the authors by reverting and speeding the fixes. This means you   shouldn't approve a patch just because you trust the author of a   patch - Make sure you understand what the implications of a patch   might be, or leave the review to others. Partial reviews,   reviewing code style, for example, can be given a positive review or a LGTM.   This also applies if you think the patch looks good, but may   not have the experience to know if there may be unintended   consequences.</p> </li> <li> <p>Please make sure that the code changes are covered by the existing   unit tests and if necessary ask the contributor to add or update   tests.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#contributors","title":"Contributors","text":"<ul> <li> <p>Before providing a patch for review ask yourself these questions:</p> </li> <li> <p>Is this PR the right size? If it's too long break it up.</p> </li> <li> <p>Is this MR and all the changes included necessary? (all code has     to be maintained)</p> </li> <li> <p>Does this MR duplicate existing functionality? If yes, can I     extend what is there?</p> </li> <li> <p>Is the code readable? Am I using esoteric language constructs     that affect readability? Does the code follow the conventions     of the codebase?</p> </li> <li> <p>Is the MR production ready? In other words - does it have tests,     documentation, error handling, etc, etc.</p> </li> <li> <p>Bring attention to patches that you would like reviewed. Add   reviewers, ask for reviewers on slack or even just rebase it   against the current codebase to bring it to the top of the Gitlab   list. If you're not sure who would be a good reviewer, gitlab will suggest OWNERS based   on the CODEOWNERS file in the UI or look at the   git history of the files that you've changed, and add those   people. For NIM-API based changes there is a small team managing these   therefore seek for those people to review APIs.</p> </li> <li> <p>Try to coordinate with other significant contributors to the code   when making changes to areas you do not own. Before you type a   single line of code!. These people made the most significant   changes to that part of the code and therefore are knowledgeable   of any tradeoffs to be made. Coming with new code already written   will cause painful back and forth and will be less efficient for   all. Learn the design, propose changes and get an agreement before   making changes.</p> </li> <li> <p>Don't modify other people's branches unless you have coordinated this   with the owner of that branch. Not only is this considered rude,   but your changes could be unintentionally lost. An exception to   this would be for branches that have not been updated for more than   90 days and therefore can be considered orphaned.</p> </li> <li> <p>Respond to anyone who has taken the time to review your branches,   even if it's just to say that you disagree. While it may seem   annoying to address a request to fix spelling or 'trivial' issues,   it's generally easy to handle in Gitlab's built-in editor. If you   do use the built-in editor, remember to get that change to your   local copy before re-pushing. It's also acceptable to add fixes   for these sorts of comments to another branch, but it's recommended   that that branch be pushed to Gitlab before the initial branch gets   submitted.</p> </li> <li> <p>Check if there's documentation that needs to be updated to remain   current after your change. If there's no documentation for the   part you're working on, consider adding some.</p> </li> <li> <p>When contributing a significant change to core parts of the code   base, or when introducing a new way of doing something that you   think is worthwhile to apply across the tree, please bring up your   design doc to the commit, so reviewers can read it.</p> </li> <li> <p>Don't expect that people will review your patch unless you ask them   to. Adding other people as reviewers is the easiest way.   But also you can use Slack to actively ping the reviewers, which   is especially useful for urgent MRs.</p> </li> <li> <p>Don't expect people to drop all of what they are doing to review   your patch. Everyone has a day-time job, and while code reviews   are part of that job, they usually do not extend the full working   day.</p> </li> <li> <p>Do not resolve (ack/Done) any comments open by others so they can   find their comments easily and resolve them. Unless being told to   self-resolve.</p> </li> <li> <p>As a contributor you are responsible for the code you bring in and   any failures being caught during unit or convergence testing. Etc.   You should monitor that your code gets merged successfully and   that no errors have appeared after the code has been merged   (nightly testing / convergence testing) and respond promptly to address   any failures.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#general-etiquette","title":"General Etiquette","text":"<ul> <li> <p>We try to assume the best of each other in this community. It's okay   to discuss mistakes (e.g., isolated instances of non-trivial and   non-critical changes submitted early) but try to keep such   inquiries blameless. If a change leads to problems with the code,   the focus should be on fixing the issue, not on assigning blame.</p> </li> <li> <p>Be respectful to others when commenting on branches. Comments should   be kept to the code and should be kept in a polite tone. Assume   your colleagues are intelligent and do not intend any malice or   disrespect. Resist the urge to retaliate against perceived verbal   misconduct, such behavior is not conducive to getting branches   merged. Also, avoid absolute and aggressive language as this can   tend to escalate emotions. Comments are meant to be collaborative.   Very often, the commenter might also be incorrect since they may not have   the full story of the patch in mind since they were not the author. A comment   is not a demand, it's a suggestion towards a mutually acceptable solution   between the author and the reviewer.</p> </li> <li> <p>Don't submit code that you know will break other   platforms/dependencies. If your patch affects code that is used by   others, it should be compatible with those. While it would be nice   to update any other dependents, you must at least provide a path   that will allow other platforms to continue working.</p> </li> <li> <p>Don't write commit messages that are vague or wouldn't make sense to   partners that read the logs. For example, do not write \"[topic]   Bugfix\" as your header in the commit message. Keep links to videos   out of the commit message. Again, partners are going to see these   logs and it does not make sense to link to something they will not   have access to view. Keep your commit messages under 72 chars in   length per line. Good commits have an appropriate topic with a   nice one-liner that explains the change briefly. This is then   followed by a few sentences or more on a description of the   change. Be professional in your language and do not put things in   the message that a partner would not understand. Avoid the use of   acronyms, abbreviations, or codenames, especially those meaningful   only at nvidia. Also, use correct English (check your spelling   please). This pertains to the final commit message after the branch   is squashed. Intermediary commit messages do not matter at all. Commits   must be squashed on merge.</p> </li> <li> <p>Consider breaking up large individual patches into smaller patches   grouped by areas. This makes the patches easier to review but   increases the number of patches. The way this is handled is a   personal decision, as long as each patch is still one logical   change.</p> </li> <li> <p>Contributions made to the <code>bionemo/contrib</code> folder have more flexible requirements. All requirements not mentioned below still   apply to the contrib folder. These exceptions are</p> </li> <li> <p>Code line numbers limit is increased to 2200 lines.</p> </li> <li>Unit test coverage requirements are decreased to 65% coverage.</li> <li>Code line length of &lt;=120 character spaces is acceptable.</li> <li> <p>Commit messages do not need have as verbose of an explanation.     All other requirements pertaining to approver, contributor, and reviewer responsibility still apply.</p> </li> <li> <p>Reviews are about the code. It's easy to take it personally when   someone is criticizing your code, but the whole idea is to get   better code into our codebase. Again, this also applies in the   other direction: review code, critically think about and respond   to the code, but don't make it personal.</p> </li> <li> <p>Don't develop on a personal branch for a long time and then dump a   large number of files and lines of code into a review as a new   \"feature.\" Features should be developed off of the main trunk just   like any other change. It is even more important to follow   processes as the code becomes more critical. Features can   be broken into small working changes and don't need to be   developed all at once. Creating large changes like this leads to   less efficiency in the review process and can lead to more   mistakes.</p> </li> <li> <p>In BioNeMo, for features under development that are not ready for   production, we put them inside the <code>bionemo2/contrib</code> folder. This allows for teams to develop   faster (less strict reviews) while testing code. When a feature is   complete and well tested, we move it to <code>bionemo2/src</code> or   <code>bionemo2/core</code> and we complete all the requirements for productroduction.</p> </li> </ul>"},{"location":"user-guide/contributing/code-review/#references","title":"References","text":"<ul> <li>Coreboot gerrit guidelines (heavily - inspired the etiquette portion of this document)</li> <li>Code Review Culture</li> <li>Proven practices for peer review</li> <li>https://confluence.nvidia.com/display/DS/SDK+Code+reviews</li> <li>https://confluence.nvidia.com/display/DS/SDK+Best+Practices (Review and source control sections)</li> </ul>"},{"location":"user-guide/contributing/contributing/","title":"Contributing Guidelines","text":"<p>Note</p> <p>For code review standards please see the Code Review page.</p> <p>For all PRs, an approved NVIDIA staff member must sign off and trigger the continuous integration (CI) tests. These are initiated by the member commenting <code>/build-ci</code> directly on the PR. All PRs must have successful CI runs and sufficient code review before being merged.</p>"},{"location":"user-guide/contributing/contributing/#python-coding-standards","title":"Python Coding Standards","text":"<p>This page contains the Python coding standards for the BioNeMo repository. They apply to all Python code in the repository (unless external constraints prevent it).</p>"},{"location":"user-guide/contributing/contributing/#coding-style","title":"Coding Style","text":"<ul> <li>We follow the Google Python Style Guide with a few tweaks.</li> <li>The most important parts of this style guide that our code must adhere to are:</li> <li>Docstring</li> <li>Mutable global state</li> <li>Do not use mutable values as default arguments</li> <li>Default iterators</li> <li>Bad naming / abbreviation</li> <li>The exceptions to this style guide are:<ul> <li>Module imports. If a module is uniquely named, import the module. Otherwise, import the value, type, or function directly.</li> </ul> </li> <li>Linting and formatting of all code is required by using <code>ruff</code> with BioNeMo's configured options.</li> <li>Unit testing with <code>pytest</code>.</li> <li>Add type annotations everywhere. In particular, new code should all be type-annotated as thoroughly as possible. This   also obviates the need for including type hints in the function docstring. It is ok to omit annotations for private   helper functions, but use your best judgement.</li> <li>Include docstrings for every class, function, and method exposed to the user.<ul> <li>Docstrings should answer (a) what is the code doing and (b) why would someone use it.</li> </ul> </li> <li>Never use wildcard imports.</li> <li>Define <code>__all__ = (,)</code> in modules: make explicit the API of each module, auto-documenting the most important definitions.</li> <li>Minimize the use of <code>**kwargs</code>.</li> <li><code>raise</code> an <code>Exception</code> instead of using an <code>assert</code> statement.</li> <li>F-strings are preferred to format strings.</li> <li>Loggers are preferred to print. In BioNeMo, you can use logger from <code>import logging</code>.</li> <li>Private functions (functions starting with <code>_</code>) shouldn't be called outside its host file.</li> </ul>"},{"location":"user-guide/contributing/contributing/#general-guidelines","title":"General Guidelines","text":"<ul> <li>User-oriented: make it easy for end users, even at the cost of writing more code in the background</li> <li>Robust: make it hard for users to make mistakes.</li> <li>Well-tested: please add simple, fast unit tests.</li> <li>Reusable: for every piece of code, think about how it can be reused in the future and make it easy to reuse.</li> <li>Readable: code should be easy to read and well documented (with comments and docstrings).</li> <li>Legal: if you copy even one line of code from the Internet, make sure that the code allows the license that   BioNeMo supports. Give credit and link back to the code.</li> <li>Sensible: code should make sense. If you think a piece of code might be confusing, write comments.</li> <li>Consistent: we work in a team. It is important to integrate changes with existing code.</li> <li>Readable: your code should be easy to read and understand by any other engineer, including outside NVIDIA. Some   tips:<ul> <li>Document your code. Make all comments complete sentences, starting with a capitalized letter and ending with a period.</li> <li>Avoid abbreviations: 'bn' is harder to understand than 'batch_norm'.</li> <li>Avoid baked-in constants throughout the code. Instead, specify them as parameters to your function. If you must have a constant, follow the naming guideline (e.g., <code>GLOBAL_CONSTANT</code>).</li> <li>Avoid functions that span hundreds of lines. Large functions are more difficult to read and more difficult to test. If &gt;120 lines, consider re-factoring it into smaller logical functions, each unit-tested and well-documented.</li> <li>Re-use code by importing. Do not copy and paste code.</li> <li>Usage of third-party code should be legally compatible and attributed.</li> </ul> </li> </ul>"},{"location":"user-guide/contributing/contributing/#pull-request-pr-guidelines","title":"Pull Request (PR) Guidelines","text":""},{"location":"user-guide/contributing/contributing/#labeling-your-pr","title":"Labeling Your PR","text":"<p>If you are an external contributor (not an NVIDIA employee), please add the <code>contribution</code> label to your PR before submitting. Labels can be accessed in the right sidebar of the GitHub user interface when creating or editing a PR.</p>"},{"location":"user-guide/contributing/contributing/#signing-your-work","title":"Signing Your Work","text":"<ul> <li> <p>We require that all contributors \"sign-off\" on their commits (not GPG signing, just adding the <code>-s | --signoff</code>   argument, or follow the instructions below for auto-signing). This sign-off certifies that the contribution is your original   work, or you have rights to submit it under the same license or a compatible license.</p> </li> <li> <p>Any contribution which contains commits that are not signed-off will not be accepted.</p> </li> <li> <p>To sign off on a commit you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes:   <pre><code>$ git commit -s -m \"Add cool feature.\"\n</code></pre>   This will append the following to your commit message:   <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre></p> </li> </ul> <p>If you would like this to happen automatically to all of your commits, you can modify   your local <code>~/.git-config-template.txt</code> file. You can do this with a command like the   following:</p> <pre><code>echo \"Signed-off-by: Your Name &lt;your@email.com&gt;\" &gt; ~/.git-commit-template.txt\ngit config --local commit.template ~/.git-commit-template.txt\n</code></pre> <p>If you have a commit that you want to retroactively sign, you can do that with:</p> <pre><code>git commit --amend --no-edit --signoff\n</code></pre> <ul> <li> <p>Full text of the DCO:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n</code></pre> <pre><code>Developer's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source\nlicense indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate\nopen source license and I have the right under that license to submit that work with modifications, whether created\nin whole or in part by me, under the same open source license (unless I am permitted to submit under a different\nlicense), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not\nmodified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution\n(including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be\nredistributed consistent with this project or the open source license(s) involved.\n</code></pre> </li> </ul>"},{"location":"user-guide/contributing/contributing/#developer-workflows","title":"Developer workflows:","text":"<p>You should always carefully test your changes. Run <code>pytest ...</code> in your container locally. All tests are done via <code>pytest</code>.</p> <p>Changes that affect model training accuracy or compute performance should be tested on SLURM.</p> <p>Developer workflow for external code contributions is as follows:</p> <ol> <li> <p>External developers must first fork the upstream BioNeMo OSS repository and for BioNeMo2 (this branch) use the <code>main</code> branch as base.</p> </li> <li> <p>Clone the forked repository and push changes to the personal fork.</p> </li> </ol> <pre><code>git clone https://github.com/YOUR_USERNAME/YOUR_FORK.git bionemo-framework\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin &lt;local-branch&gt;:&lt;remote-branch&gt;\n</code></pre> <p>Developer workflow for internal or those developers that have been granted push access to our repository is as follows:</p> <ol> <li>Clone this repository locally</li> <li>Create a branch which ideally should be of the form <code>username/branch_description</code></li> <li>Push branch up to our repository <code>git push -u origin HEAD</code></li> </ol> <p>For both internal and external developers, the next step is opening a PR:</p> <ol> <li>Once the code changes are staged on the fork and ready for review, a   Pull Request (PR) can be     requested to merge the changes from a branch of the     fork or branch into <code>main</code>.<ul> <li>Exercise caution when selecting the source and target branches for the PR. Note that versioned releases of TensorRT OSS are posted to <code>release/</code> branches of the upstream repo.</li> <li>Creation of a PR creation kicks off the code review process.</li> <li>At least one TensorRT engineer will be assigned for the review.</li> <li>While under review, mark your PRs as work-in-progress by prefixing the PR title with [WIP].</li> </ul> </li> <li>Once ready, CI can be started by a developer with permissions when they add a <code>/build-ci</code> comment. This must pass   prior to merging.</li> </ol>"},{"location":"user-guide/contributing/contributing/#general-guidelines_1","title":"General guidelines","text":"<p>Send your PRs to the <code>main</code> branch. Branch off from <code>main</code> when making your changes. Prefix your branches with your name or initials (for example, <code>your_name/branch_description</code>) if you have push access to our repository otherwise please create a fork with your branch and submit a PR with <code>main</code> as the target.</p> <ul> <li>Make sure your PR does one thing. Have a clear answer to \"What does this PR do?\"</li> <li>Make sure you have the linters enabled via pre-commit hooks (<code>pre-commit install</code>)</li> <li>Follow the default PR template</li> <li>Make sure all unit tests finish successfully before running PR pipeline by invoking <code>pytest scripts sub-packages</code>.</li> <li>Make sure you added necessary tests and documentation changes (could be just comments in the config files) for the   feature in your PR</li> <li>Rebase your feature branch with the latest <code>main</code> to include any new changes that have been added. Resolve merge   conflicts, if any</li> <li>Send your PR and request a review</li> <li>If your PR is still a work in progress, mark it as \"Draft\"</li> <li>Your merge request must pass all pipelines and be peer-reviewed before it can be merged.</li> <li>Make sure to merge your PR when it is ready and pipeline is successful</li> </ul>"},{"location":"user-guide/contributing/contributing/#unit-tests","title":"Unit tests","text":"<p>Contributors to BioNeMo FW are expected to unit test their introduced changes.</p> <p>After testing your code locally, trigger tests in the PR's CI. Let a code-owner know that you are ready for the build to  run and they will leave a <code>/build-ci</code> comment on your PR which will run the CI test suite.</p>"},{"location":"user-guide/contributing/contributing/#adding-unit-tests","title":"Adding unit tests","text":"<p>Add unit tests under <code>tests</code> to examine use cases of new classes or methods that are being added to the codebase. Each test file must be for a particular file or module. For example if you have a file that is under <code>src/path/to/module/my_file_name.py</code> then your test should match the path at <code>tests/path/to/module/test_my_file_name.py</code>. Check the tests folders in the sub-modules of this repository for examples. If you are testing a module, such as integrating multiple examples of different files, then you can use the following pattern to test the module, say in the above example, if you wanted to test functions from several files together that all exist in the same <code>src/path/to/module</code> then you could create a <code>tests/path/to/test_module.py</code> file. The same is true for parents of that module and so on. Generally unit tests should exist at the level of the individual file however.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/","title":"Writing Good and Thorough Documentation","text":"<p>As a contributor to our codebase, writing high-quality documentation is an essential part of ensuring that others can understand and work with your code effectively. Good documentation helps to reduce confusion, facilitate collaboration, and streamline the development process. In this guide, we will outline the principles and best practices for writing thorough and readable documentation that adheres to the Chicago Manual of Style.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#chicago-manual-of-style","title":"Chicago Manual of Style","text":"<p>Our documentation follows the Chicago Manual of Style, a widely accepted standard for writing and formatting. This style guide provides a consistent approach to writing, grammar, and punctuation, making it easier for readers to understand and navigate our documentation.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/#key-principles","title":"Key Principles","text":"<p>When writing documentation, keep the following principles in mind:</p> <ol> <li>Clarity: Use clear and concise language to convey your message. Avoid ambiguity and jargon that may confuse readers.</li> <li>Accuracy: Ensure that your documentation is accurate and up-to-date. Verify facts, details, and code snippets     before publishing.</li> <li>Completeness: Provide all necessary information to understand the code, including context, syntax, and examples.</li> <li>Consistency: Use a consistent tone, voice, and style throughout the documentation.</li> <li>Accessibility: Make your documentation easy to read and understand by using headings, bullet points, and short paragraphs.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#documentation-structure","title":"Documentation Structure","text":"<p>A well-structured documentation page should include the following elements:</p> <ol> <li>Header: A brief title that summarizes the content of the page.</li> <li>Introduction: A short overview of the topic, including its purpose and relevance.</li> <li>Syntax and Parameters: A detailed explanation of the code syntax, including parameters, data types, and return values.</li> <li>Examples: Concrete examples that illustrate how to use the code, including input and output.</li> <li>Tips and Variations: Additional information, such as best practices, common pitfalls, and alternative approaches.</li> <li>Related Resources: Links to relevant documentation, tutorials, and external resources.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#best-practices","title":"Best Practices","text":"<p>To ensure high-quality documentation, follow these best practices:</p> <ol> <li>Use headings and subheadings: Organize your content with clear headings and subheadings to facilitate scanning and navigation.</li> <li>Use bullet points and lists: Break up complex information into easy-to-read lists and bullet points.</li> <li>Provide context: Give readers a clear understanding of the code's purpose, history, and relationships to other components.</li> <li>Review and edit: Carefully review and edit your documentation to ensure accuracy, completeness, and consistency.</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/#resources","title":"Resources","text":"<p>For more information on the Chicago Manual of Style, refer to their online published version.</p> <p>By following these guidelines and principles, you wi ll be able to create high-quality documentation that helps others understand and work with your code effectively. Remember to always prioritize clarity, accuracy, and completeness, and to use the Chicago Style Guide as your reference for writing and formatting.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/","title":"Jupyter Notebook Support","text":"In\u00a0[1]: Copied! <pre>a = 1\nb = 2\na + b\n</pre> a = 1 b = 2 a + b Out[1]: <pre>3</pre> In\u00a0[2]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxs = np.linspace(0, 2*np.pi, 100)\nplt.plot(xs, np.sin(xs))\n</pre> %matplotlib inline import matplotlib.pyplot as plt import numpy as np  xs = np.linspace(0, 2*np.pi, 100) plt.plot(xs, np.sin(xs)) Out[2]: <pre>[&lt;matplotlib.lines.Line2D at 0x79f258429570&gt;]</pre> In\u00a0[3]: Copied! <pre>#NBVAL_CHECK_OUTPUT\n# pragma: allowlist secret\n\nimport torch\nprint(torch.__version__)\n</pre> #NBVAL_CHECK_OUTPUT # pragma: allowlist secret  import torch print(torch.__version__) <pre>2.3.0a0+ebedce2\n</pre>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#jupyter-notebook-support","title":"Jupyter Notebook Support\u00b6","text":"<p>Jupyter notebooks can be rendered as part of the documentation build system as an alternative to markdown files. The docs site uses mkdocs-jupyter to build and render jupyter notebooks as markdown files.</p> <p>Note: There are some limitations to jupyter rendering.</p> <ol> <li>Notebooks are not executed as part of the docs publishing pipeline. CI tests to ensure notebook consistency are run separately (see Testing Jupyter Notebooks).</li> <li>Notebook markdown cells don't support the full range of mkdocs-material configuration, including things like admonitions, referencing automatically-generated API documentation via mkdocstrings etc. (more here).</li> </ol>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#example-code-block","title":"Example code block\u00b6","text":"<p>Markdown headings can be used to create a TOC similarly to traditional mkdocs pages.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#embedded-visualizations","title":"Embedded visualizations\u00b6","text":"<p>We can also embed images using standard approaches to embedding graphics in notebooks.</p>"},{"location":"user-guide/contributing/Writing%20Documentation/jupyter-notebooks/#testing-jupyter-notebooks","title":"Testing Jupyter Notebooks\u00b6","text":"<p>Jupyter notebooks are run as part of the CI build suite using <code>nbval</code>. To run these tests locally, run</p> <pre>pytest --nbval-lax docs/\n</pre> <p>from the repository root. By default, <code>nbval</code> will only check that the notebook executes successfully. To add additional checks to ensure the consistency of the output, add a <code>#NBVAL_CHECK_OUTPUT</code> marker comment, which will ensure that the output of the saved jupyter notebook matches the output when the notebook is executed in CI.</p> <p>For example:</p>"},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/","title":"MkDocs","text":""},{"location":"user-guide/contributing/Writing%20Documentation/mkdocs/#build-system","title":"Build system","text":"<p>BioNeMo 2 uses Material for MkDocs to build it's documentation. Docstrings are converted to automatically-generated API reference pages using <code>mkdocstrings</code>, and can be linked from markdown pages using paths.</p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/","title":"bionemo-core","text":"<p>Common code that all BioNeMo framework packages depend on. Contains highly reusable, battle-tested abstractions and implementations that are valuable across a wide variety of domains and applications.</p> <p>Crucially, the <code>bionemo-core</code> Python package (namespace <code>bionemo.core</code>) depends on PyTorch and PyTorch Lightning. Other key BioNeMo component libraries, such as <code>bionemo-llm</code> and <code>bionemo-geometric</code>, obtain their PyTorch dependencies via <code>bionemo-core</code>.</p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/#developer-setup","title":"Developer Setup","text":"<p>After following the setup specified in the README, you may install this project's code in your environment via executing: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests with code coverage, execute: <pre><code>pytest -v --cov=bionemo --cov-report=term .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-core/bionemo-core-Overview/#package-highlights","title":"Package Highlights","text":"<p>In <code>bionemo.core.model.config</code>: - <code>ModelOutput</code>: A Model's forward pass may produce a tensor, multiple tensors, or named tensors. - <code>LossType</code>: A generic type parameter for a loss function. - <code>Model</code>: An interface for any ML model that accepts and produces <code>torch.Tensor</code>s. - <code>ModelType</code>: A generic type parameter that is constrained to the <code>Model</code> interface. - <code>BionemoModelConfig</code>: An abstract class that enables parameterizable model instantiation that is compatible with Megatron. - <code>BionemoTrainableModelConfig</code>: An extension that includes the loss function to use with the model during training.</p> <p>In <code>bionemo.core.utils</code>: - the <code>batching_utils</code> module's <code>pad_token_ids</code>, which pads token ids with padding value &amp; returns a mask. - the <code>dtype</code> module's <code>get_autocast_dtype</code>, which converts from nemo/nvidia datatypes to their PyTorch equivalents. - the <code>random_utils</code> module, which includes functions for managing random seeds and performing sampling.</p> <p>In the <code>bionemo.data</code> package, there is: - <code>multi_epoch_dataset</code>: contains many dataset implements that are useful for mutli-epoch training. - <code>resamplers</code>: contains a P-RNG based Dataset implementation.</p> <p>There's a constant global value, <code>bionemo.core.BIONEMO_CACHE_DIR</code>, which is used as a local on-disk cache for resources.</p>"},{"location":"user-guide/developer-guide/bionemo-esm2/bionemo-esm2-Overview/","title":"bionemo-esm2","text":"<p>ESM-2 is a protein language model with BERT architecture trained on millions of protein sequences from UniProt. ESM-2 learns the patterns and dependencies between amino acids that ultimately give rise to a protein\u2019s structure. ESM-2 is pretrained on a masked language model (MLM) objective. During pretraining, 15% of the input sequence is perturbed, and within which 80% of the residues are replaced with a mask token, 10% are replaced with a random token, and 10% are left unchanged. The model is then trained to predict the original amino acids at the perturbed positions with the context of the surrounding amino acids.</p> <p>Despite pretraining on an MLM objective, the sequence representation learned by ESM-2 is highly transferable to downstream tasks. ESM-2 can be fine-tuned on a variety of tasks, including secondary structure prediction as, and whole-sequence prediction on cellular localization, thermostability, solubility, and other protein properties.</p>"},{"location":"user-guide/developer-guide/bionemo-esm2/bionemo-esm2-Overview/#setup","title":"Setup","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/","title":"bionemo example model Overview","text":""},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#bionemo-example_model","title":"bionemo-example_model","text":""},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#introduction","title":"Introduction","text":"<p>This is a minimalist package containing an example model that makes use of bionemo2 and nemo conventions. It contains the necessary models, dataloaders, datasets, and custom loss fucntions. The referenced classes and function are in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>This tutorial demonstrates the creation of a simple MNIST model. This should be run in a BioNeMo container. For this tutorial, we will reuse elements from the BioNeMo example_model package.</p> <p><code>Megatron</code>/<code>NeMo</code> modules and datasets are special derivatives of PyTorch modules and datasets that extend and accelerate the distributed training and inference capabilities of PyTorch.</p> <p>Some distinctions of Megatron/NeMo are:</p> <ul> <li><code>torch.nn.Module</code>/<code>LightningModule</code> changes into <code>MegatronModule</code>.</li> <li>Loss functions should extend the <code>MegatronLossReduction</code> module and implement a <code>reduce</code> method for aggregating loss across multiple micro-batches.</li> <li>Megatron configuration classes (for example <code>megatron.core.transformer.TransformerConfig</code>) are extended with a <code>configure_model</code> method that defines how model weights are initialized and loaded in a way that is compliant with training via NeMo2.</li> <li>Various modifications and extensions to common PyTorch classes, such as adding a <code>MegatronDataSampler</code> (and re-sampler such as <code>PRNGResampleDataset</code> or <code>MultiEpochDatasetResampler</code>) to your <code>LightningDataModule</code>.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#loss-functions","title":"Loss Functions","text":"<p>First, we define a simple loss function in <code>bionemo.example_model.lightning.lightning_basic</code>. These should extend the <code>MegatronLossReduction</code> class. The output of forward and backward passes happen in parallel. There should be a forward function that calculates the loss defined. The reduce function is required.</p> <p>Loss functions used here are <code>MSELossReduction</code> and <code>ClassifierLossReduction</code>. These functions return a Tensor, which contain the losses for the microbatches, and a <code>SameSizeLossDict</code> containing the average loss. This is a Typed Dictionary that is the return type for a loss that is computed for the entire batch, where all microbatches are the same size.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#datasets-and-datamodules","title":"Datasets and Datamodules","text":"<p>Datasets used for model training must be compatible with Megatron datasets. To enable this, the output of a given index and epoch must be deterministic. However, we may wish to have a different ordering in every epoch. To enable this, the items in the dataset should be accessible by both the epoch and the index. This can be done by accessing elements of the dataset with <code>EpochIndex</code> from <code>bionemo.core.data.multi_epoch_dataset</code>. A simple way of doing this is to wrap a dataset with <code>IdentityMultiEpochDatasetWrapper</code> imported from <code>bionemo.core.data.multi_epoch_dataset</code>. In this example, in in <code>bionemo.example_model.lightning.lightning_basic</code>, we use a custom dataset <code>MNISTCustomDataset</code> that wraps the <code>__getitem__</code> method of the MNIST dataset such that it return a dict instead of a Tuple or tensor. The <code>MNISTCustomDataset</code> returns elements of type <code>MnistItem</code>, which is a <code>TypedDict</code>.</p> <p>In the data module/data loader class, it is necessary to have a data_sampler method to shuffle the data and that allows the sampler to be used with Megatron. This is a nemo2 peculiarity. A <code>nemo.lightning.pytorch.plugins.MegatronDataSampler</code> is the best choice. It sets up the capability to utilize micro-batching and gradient accumulation. It is also the place where the global batch size is constructed.</p> <p>Also the sampler will not shuffle your data. So you need to wrap your dataset in a dataset shuffler that maps sequential IDs to random IDs in your dataset. This can be done with <code>MultiEpochDatasetResampler</code> from <code>bionemo.core.data.multi_epoch_dataset</code>.</p> <p>This is implemented in the <code>MNISTDataModule</code>. In the setup method of the dataloader, the train, test and validation sets are <code>MNISTCustomDataset</code> are wrapped in the <code>IdentityMultiEpochDatasetWrapper</code>. These are then wrapped in the <code>MultiEpochDatasetResampler</code>. More information about <code>MegatronCompatability</code> and how to set up more complicated datasets can be found in <code>docs.user-guide.background.megatron_datasets.md</code>.</p> <p>We also define a <code>train_dataloader</code>, <code>val_dataloader</code>, and <code>predict_dataloader</code> methods that return the corresponding dataloaders.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#models","title":"Models","text":"<p>Models need to be Megatron modules. At the most basic level this just means:</p> <ol> <li>They extend <code>MegatronModule</code> from megatron.core.transformer.module.</li> <li>They need a config argument of type <code>megatron.core.ModelParallelConfig</code>. An easy way of implementing this is to inherit from <code>bionemo.llm.model.config.MegatronBioNeMoTrainableModelConfig</code>. This is a class for BioNeMo that supports usage with Megatron models, as NeMo2 requires. This class also inherits <code>ModelParallelConfig</code>.</li> <li>They need a self.<code>model_type:megatron.core.transformer.enums.ModelType</code> enum defined (<code>ModelType.encoder_or_decoder</code> is a good option.)</li> <li><code>def set_input_tensor(self, input_tensor)</code> needs to be present. This is used in model parallelism. This function can be a stub/placeholder function.</li> </ol> <p>The following models are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p><code>ExampleModelTrunk</code> is a base model. This returns a tensor. <code>ExampleModel</code> is a model that extends the base model with a few linear layers and it is used for pretraining. This returns the output of the base model and of the full model.</p> <p><code>ExampleFineTuneModel</code> extends the <code>ExampleModelTrunk</code> by adding a classification layer. This returns a tensor of logits over the 10 potential digits.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#model-configs","title":"Model Configs","text":"<p>The model config class is used to instantiate the model. These configs must have: 1. A <code>configure_model</code> method which allows the Megatron strategy to lazily initialize the model after the parallel computing environment has been setup. These also handle loading starting weights for fine-tuning cases. Additionally these configs tell the trainer which loss you want to use with a matched model. 2. A <code>get_loss_reduction_class</code> method that defines the loss function.</p> <p>The following configs are implemented in <code>bionemo.example_model.lightning.lightning_basic</code>.</p> <p>Here, a base generic config <code>ExampleGenericConfig</code> is defined.  <code>PretrainConfig</code> extends this class. This defines the model class and the loss class in: <pre><code>class PretrainConfig(ExampleGenericConfig[\"PretrainModel\", \"MSELossReduction\"], iom.IOMixinWithGettersSetters):\n\n    model_cls: Type[PretrainModel] = PretrainModel\n    loss_cls: Type[MSELossReduction] = MSELossReduction\n</code></pre></p> <p>Similarly, <code>ExampleFineTuneConfig</code> extends <code>ExampleGenericConfig</code> for finetuning.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-module","title":"Training Module","text":"<p>It is helfpul to have a training module that inherits from <code>pytorch_lightning.LightningModule</code> which organizes the model architecture, training, validation, and testing logic while abstracting away boilerplate code, enabling easier and more scalable training. This wrapper can be used for all model and loss combinations specified in the config. In <code>bionemo.example_model.lightning.lightning_basic</code>, we define <code>BionemoLightningModule</code>.</p> <p>In this example, <code>training_step</code>, <code>validation_step</code>, and <code>predict_step</code> define the training, validation, and prediction loops are independent of the forward method. In nemo:</p> <ol> <li>NeMo's Strategy overrides the <code>train_step</code>, <code>validation_step</code> and <code>prediction_step</code> methods.</li> <li>The strategies' training step will call the forward method of the model.</li> <li>That forward method then calls the wrapped forward step of <code>MegatronParallel</code> which wraps the forward method of the model.</li> <li>That wrapped forward step is then executed inside the <code>MegatronCore</code> scheduler, which calls the <code>_forward_step</code> method from the <code>MegatronParallel</code> class.</li> <li>Which then calls the <code>training_step</code>, <code>validation_step</code> and <code>prediction_step</code> function here.</li> </ol> <p>Additionally, during these steps, we log the validation, testing, and training loss. This is done similarly to https://lightning.ai/docs/torchmetrics/stable/pages/lightning.html. These logs can then be exported to wandb, or other metric viewers. For more complicated tracking, it may be necessary to use pytorch callbacks: https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html.</p> <p>Further <code>loss_reduction_class()</code>, <code>training_loss_reduction()</code>, <code>validation_loss_reduction(),</code> and<code>test_loss_reduction()</code> are defined based on what's in the config. Additionally,  <code>configure_model()</code> is definated based on the config.</p>"},{"location":"user-guide/developer-guide/bionemo-example_model/bionemo-example_model-Overview/#training-the-models","title":"Training the models","text":"<p>In <code>bionemo.example_model.lightning.lightning_basic</code> a checkpoint_callback variable is defined. This enables .nemo file-like checkpointing.</p> <p>The remaining functions are defined in the training scripts: <code>pretrain_mnist.py</code>, <code>finetune_mnist.py</code>, and <code>predict_mnist.py</code>.</p> <p>We specify a training strategy of type <code>nemo.lightning.MegatronStrategy</code>. This strategy implements model parallelism using NVIDIA's Megatron-LM framework. It supports various forms of parallelism including tensor model parallelism, pipeline model parallelism, sequence parallelism, and expert parallelism for efficient training of large language models.</p> <p>We specify a trainer of type <code>nemo.lightning.Trainer</code>, which is an extension of the pytorch lightning trainer. This is where the devices, validation intervals, maximal steps, maximal number of epochs, and how frequently to log are specified.</p> <p>we specify a nemo-logger. We can set TensorBoard and WandB logging, along with extra loggers. Here, we specify a <code>CSVLogger</code> from pytorch_lightning.loggers.</p> <p>We can now proceed to training. The first pre-training scripts is <code>bionemo/example_model/training_scripts/pretrain_mnist.py</code></p> <p>Then, we train the model with the <code>BionemoLightningModule</code>, <code>MNISTDataModule</code>, trainer and nemo_logger.</p> <p>This script will print out the location of the final model:  <p>Then we can run a finetuning-script: <pre><code>python src/bionemo/example_model/training_scripts/training_scripts/finetune_mnist.py ---pretrain_ckpt_dirpath &lt;pretrain_directory&gt;\n</code></pre></p> <p>A nuance here is that in the config file, we specify the initial checkpoint path, along with which keys to skip. In the previous model checkpoint, we did not have a head labelled \"digit_classifier\", so we specify it as a head to be skipped. This script will print the location of the finetuned directory: . <p>Finally, we can run a classification task with <pre><code>python src/bionemo/example_model/training_scripts/predict_mnist.py  --finetune_dir &lt;finetune_dir&gt;.\n</code></pre></p> <p>The results can be viewed with TensorBoardLogger if that is configured, or as a CSV file created by the CSVLogger.</p>"},{"location":"user-guide/developer-guide/bionemo-fw/bionemo-fw-Overview/","title":"bionemo-fw","text":"<p>The BioNeMo Framework (FW): a production grade framework for AI-enabled Drug Discovery.</p> <p>The <code>bionemo-fw</code> Python package contains framework-spanning code under the <code>bionemo.fw</code> namespace. All other namespaces of the BioNeMo Framework (<code>bionemo.*</code>) are dependencies of this package.</p>"},{"location":"user-guide/developer-guide/bionemo-fw/bionemo-fw-Overview/#developer-setup","title":"Developer Setup","text":"<p>After following the setup specified in the README, you may install this project's code in your environment via executing: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests with code coverage, execute: <pre><code>pytest -v --cov=bionemo --cov-report=term .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/","title":"bionemo-geneformer","text":"<p>Geneformer is a foundational single-cell RNA (scRNA) language model using a BERT architecture trained on millions of single-cell RNA sequences. It captures gene co-expression patterns to learn cellular representations, enabling predictive tasks across biology and medicine. Geneformer is trained on a masked language model (MLM) objective, where expression rank-ordered \"gene tokens\" in single-cell RNA sequences are masked, replaced, or left unchanged, and the model learns to predict these masked genes based on context. This module provides Dataset classes, collators for expression rank ordering, and Config objects for constructing Geneformer-style models.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#setup","title":"Setup","text":"<p>To install, execute the following from this directory (or point the install to this directory):</p> <pre><code>pip install -e .\n</code></pre> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#acquiring-data","title":"Acquiring Data","text":"<p>Datasets are expected to be in the form of AnnData (.h5ad) objects such as those downloaded from Cell x Gene | CZI. They are then pre-processed with either <code>bionemo-geneformer/src/bionemo/geneformer/data/singlecell/sc_memmap.py</code> or with sc-DL.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#geneformer-nv-10m-and-106m","title":"Geneformer-nv 10M and 106M","text":"<p>Refer to the Dataset cards and Model cards to learn more about the pre-trained checkpoints provided for both 10M and 106M of Geneformer-nv.</p>"},{"location":"user-guide/developer-guide/bionemo-geneformer/bionemo-geneformer-Overview/#see-also","title":"See Also","text":"<ul> <li>sc-DL pypi</li> <li>sc-DL github</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-geometric/bionemo-geometric-Overview/","title":"bionemo-geometric","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-llm/bionemo-llm-Overview/","title":"bionemo-llm","text":"<p>The Bionemo Large Language Model (LLM) submodule contains common code used in submodules that train LLMs on biological datasets (currently <code>bionemo-esm2</code> and <code>bionemo-geneformer</code>). This includes data masking and collate functions, the bio-BERT common architecture code, loss functions, and other NeMo / Megatron-LM compatibility functions. Sub-packages should only depend on <code>bionemo-llm</code> if they need access to NeMo and Megatron-LM.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/","title":"BioNemo-SCDL: Single Cell Data Loading for Scalable Training of Single Cell Foundation Models.","text":""},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#package-overview","title":"Package Overview","text":"<p>BioNeMo-SCDL provides an independent pytorch-compatible dataset class for single cell data with a consistent API. BioNeMo-SCDL is developed and maintained by NVIDIA. This package can be run independently from BioNeMo. It improves upon simple AnnData-based dataset classes in the following ways:</p> <ul> <li>A consistent API across input formats that is promised to be consistent across package versions.</li> <li>Improved performance when loading large datasets. It allows for loading and fast iteration of large datasets.</li> <li>Ability to use datasets that are much, much larger than memory. This is because the datasets are stored in a numpy memory-mapped format.</li> <li>Additionally, conversion of large (significantly larger than memory) AnnData files into the SCDL format.</li> <li>[Future] Full support for ragged arrays (i.e., datasets with different feature counts; currently only a subset of the API functionality is supported for ragged arrays).</li> <li>[Future] Support for improved compression.</li> </ul> <p>BioNeMo-SCDL's API resembles that of AnnData, so code changes are minimal. In most places a simple swap from an attribute to a function is sufficient (i.e., swapping <code>data.n_obs</code> for <code>data.number_of_rows()</code>).</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#installation","title":"Installation","text":"<p>This package can be installed with <pre><code>pip install bionemo-scdl\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#usage","title":"Usage","text":""},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#getting-example-data","title":"Getting example data","text":"<p>Here is how to process an example dataset from CellxGene with ~25,000 cells:</p> <p>Download \"https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\" to hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-a-single-cell-dataset-from-an-h5ad-file","title":"Loading a single cell dataset from an H5AD file","text":"<pre><code>from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\n\ndata = SingleCellMemMapDataset(\"97e_scmm\", \"hdf5s/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad\")\n</code></pre> <p>This creates a <code>SingleCellMemMapDataset</code> that is stored at 97e_scmm in large, memory-mapped arrays that enables fast access of datasets larger than the available amount of RAM on a system.</p> <p>If the dataset is large, the AnnData file can be lazy-loaded and then read in based on chunks of rows in a paginated manner. This can be done by setting the parameters when instantiating the <code>SingleCellMemMapDataset</code>: - <code>paginated_load_cutoff</code>, which sets the minimal file size in megabytes at which an AnnData file will be read in in a paginated manner. - <code>load_block_row_size</code>, which is the number of rows that are read into memory at a given time.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#interrogating-single-cell-datasets-and-exploring-the-api","title":"Interrogating single cell datasets and exploring the API","text":"<pre><code>data.number_of_rows()\n## 25382\n\ndata.number_of_variables()\n## [34455]\n\ndata.number_of_values()\n## 874536810\n\ndata.number_nonzero_values()\n## 26947275\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#saving-scdl-single-cell-dataloader-datasets-to-disk","title":"Saving SCDL (Single Cell Dataloader) datasets to disk","text":"<p>When you open a SCDL dataset, you must choose a path where the backing data structures are stored. However, these structures are not guaranteed to be in a valid serialized state during runtime.</p> <p>Calling the <code>save</code> method guarantees the on-disk object is in a valid serialized state, at which point the current python process can exit, and the object can be loaded by another process later.</p> <pre><code>data.save()\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#loading-scdl-datasets-from-a-scdl-archive","title":"Loading SCDL datasets from a SCDL archive","text":"<p>When you're ready to reload a SCDL dataset, just pass the path to the serialized data:</p> <pre><code>reloaded_data = SingleCellMemMapDataset(\"97e_scmm\")\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#using-scdl-datasets-in-model-training","title":"Using SCDL datasets in model training","text":"<p>SCDL implements the required functions of the PyTorch Dataset abstract class. You can use PyTorch-compatible DataLoaders to load batches of data from a SCDL class. With a batch size of 1 this can be run without a collating function. With a batch size greater than 1, there is a collation function (<code>collate_sparse_matrix_batch</code>), that will collate several sparse arrays into the CSR (Compressed Sparse Row) torch tensor format.</p> <pre><code>from torch.utils.data import DataLoader\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n\n## Mock model: you can remove this and pass the batch to your own model in actual code.\nmodel = lambda x : x\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 2\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#examples","title":"Examples","text":"<p>The examples directory contains various examples for utilizing SCDL.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#converting-existing-cell-x-gene-data-to-scdl","title":"Converting existing Cell x Gene data to SCDL","text":"<p>If there are multiple AnnData files, they can be converted into a single <code>SingleCellMemMapDataset</code>. If the hdf5 directory has one or more AnnData files, the <code>SingleCellCollection</code> class crawls the filesystem to recursively find AnnData files (with the h5ad extension).</p> <p>To convert existing AnnData files, you can either write your own script using the SCDL API or utilize the convenience script <code>convert_h5ad_to_scdl</code>.</p> <p>Here's an example:</p> <pre><code>convert_h5ad_to_scdl --data-path hdf5s --save-path example_dataset\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#future-work-and-roadmap","title":"Future Work and Roadmap","text":"<p>SCDL is currently in public beta. In the future, expect improvements in data compression and data loading performance.</p>"},{"location":"user-guide/developer-guide/bionemo-scdl/bionemo-scdl-Overview/#license","title":"LICENSE","text":"<p>BioNemo-SCDL has an Apache 2.0 license, as found in the LICENSE file.</p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/","title":"bionemo-size-aware-batching","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#summary-of-usage","title":"Summary of Usage","text":"<p>This package provides a simple way to create mini-batches in a memory consumption-aware (or size-aware) manner, making it useful for tasks like training models on datasets with varying memory requirements. The usage typically consists of the following steps:</p> <ol> <li>Use the <code>collect_cuda_peak_alloc</code> function to collect CUDA peak memory    allocation statistics for a user-defined workflow. It's expected that the    user-defined workflow will return a list of features extracted from the data    so that the memory model in the following step can use these features to    predict the memory allocation.</li> <li>User defines and trains a memory model using the features and memory allocation    data from previous step. This memory model can then be used to predict memory    consumption.</li> <li>Use <code>SizeAwareBatchSampler</code> or <code>size_aware_batching</code> with the memory model    prediction (from the previous step) to build batch of data so that the    resulting mini-batches do not exceed a specified maximum total memory size.</li> </ol> <p>In addition, this package provides one solution to create homogeneous mini-batches, which can be useful to reduce the padding when aligning the shape of inputs when training or evaluating the models. This <code>BucketBatchSampler</code> can be used in conjunction with <code>torch.utils.data.BatchSampler</code>, <code>SizeAwareBatchSampler</code> or other user-defined batch samplers. This usage can leverage the <code>create_buckets</code> function and follow the steps below:</p> <ol> <li>Gather the tensor sizes of elements in the dataset, which are the shapes of    tensors in a specific dimension where you want to reduce the padding.</li> <li>Provide your own bucket boundaries based on the tensor sizes, or create bucket    boundaries with <code>create_buckets</code> function with the tensor sizes and bucket    maximal width and the minimal bucket count. The <code>create_buckets</code> function    will try to create buckets with smallest widths and element counts &gt;= minimal    bucket count, unless the maximal width or the boundary is reached.</li> <li>Use <code>BucketBatchSampler</code> with base batch sampler like <code>torch.utils.data.BatchSampler</code> or    <code>SizeAwareBatchSampler</code> for each bucket. The <code>BucketBatchSampler</code> will select a bucket each time    and generate a mini-batch from this bucket using the base batch sampler for this bucket.    As such, the padding necessary for the generated mini-batches will be always smaller    than the width of buckets.</li> </ol> <p>Refer to the later sections for the API documentation and examples on how to achieve each of the steps above.</p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils-module","title":"utils Module","text":"<ul> <li> <p>collect_cuda_peak_alloc: A function that     collects CUDA peak memory allocation statistics and features to be used for     memory usage prediction for a given workflow.</p> </li> <li> <p>create_buckets: A function to create buckets for a     list of integers with pre-defined maximal width of ranges and minimal     bucket sizes.</p> </li> </ul>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler-module","title":"sampler Module","text":"<ul> <li>size_aware_batching: A generator that batches elements from an iterable while     ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>SizeAwareBatchSampler: A class that batches elements of varying sizes while     ensuring that the total size of each batch does not exceed a specified maximum.</li> <li>BucketBatchSampler: A class that groups elements of varying sizes based on predefined     bucket ranges, and create batches with elements from each bucket to ensure that each batch has elements with     homogeneous sizes.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#api-reference-and-examples","title":"API reference and examples","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#utils","title":"utils","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#collect_cuda_peak_alloc","title":"collect_cuda_peak_alloc","text":"<pre><code>def collect_cuda_peak_alloc(\n    dataset: Iterable[Data],\n    work: Callable[[Data], Feature],\n    device: torch.device,\n    cleanup: Optional[Callable[[], None]] = None\n) -&gt; Tuple[List[Feature], List[int]]\n</code></pre> <p>Collects CUDA peak memory allocation statistics for a given workflow.</p> <p>This function iterates through the provided dataset, applies the given feature function to each data point, and records the peak CUDA memory allocation during this process. The features extracted from the data points are collected along with their corresponding memory usage statistics.</p> <p>Note that the first few iterations of the workflow might result in smaller memory allocations due to uninitialized data (e.g., internal PyTorch buffers). Therefore, users may want to skip these initial data points when analyzing the results.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - An iterable containing the input data.</li> <li><code>work</code> - A function that takes a data point and returns its corresponding feature. This is where     the main computation happens and memory allocations are tracked.</li> <li><code>device</code> - The target Torch CUDA device.</li> <li><code>cleanup</code> - A function that is called after each iteration to perform any necessary cleanup.</li> </ul> <p>Returns:</p> <p>A tuple containing the collected features and their corresponding memory usage statistics.</p> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided device is not a CUDA device.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import collect_cuda_peak_alloc\n\n\n&gt;&gt;&gt; # prepare dataset, model and other components of a workflow\n&gt;&gt;&gt; # for which the user want to collect CUDA peak memory allocation statistics\n&gt;&gt;&gt; dataset, model, optimizer = ...\n&gt;&gt;&gt; # Set the target Torch CUDA device.\n&gt;&gt;&gt; device = torch.device(\"cuda:0\")\n&gt;&gt;&gt; model = model.to(device)\n\n&gt;&gt;&gt; # Define a function that takes an element of the dataset as input and\n&gt;&gt;&gt; # do a training step\n&gt;&gt;&gt; def work(data):\n...     # example body of a training loop\n...     optimizer.zero_grad()\n...     output = model(data.to(device))\n...     loss = compute_loss(output)\n...     loss.backward()\n...     optimizer.step()\n...     # extract the feature for later to be modeled or analyzed\n...     return featurize(data)\n\n&gt;&gt;&gt; # can optionally use a cleanup function to release the references\n&gt;&gt;&gt; # hold during the work(). This cleanup function will be called\n&gt;&gt;&gt; # at the end of each step before garbage collection and memory allocations measurement\n&gt;&gt;&gt; def cleanup():\n...     model.zero_grad(set_to_none=True)\n\n&gt;&gt;&gt; # Collect features (i.e., model outputs) and memory usage statistics for the workflow.\n&gt;&gt;&gt; features, alloc_peaks = collect_cuda_peak_alloc(\n...     dataset=batches,\n...     work=work,\n...     device=device,\n...     cleanup=cleanup,\n... )\n\n\n&gt;&gt;&gt; # use features and alloc_peaks as needed, e.g., fit a model\n&gt;&gt;&gt; # that can use these statistics to predict memory usage\n&gt;&gt;&gt; memory_model = ...\n&gt;&gt;&gt; memory_model.fit(features, alloc_peaks)\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#create_buckets","title":"create_buckets","text":"<pre><code>def create_buckets(sizes: torch.Tensor, max_width: int,\n                   min_bucket_count: int) -&gt; Buckets\n</code></pre> <p>Create buckets for a list of integers with pre-defined maximal width of interval and minimal bucket count.</p> <p>It will return a named tuple containing the bucket boundaries and the actual bucket sizes. e.g. torch.tensor([0, 5, 7]), torch.tensor([3,2]): specifies 2 buckets: one with range 0&lt;= sizes &lt; 5, width=5 and 3 elements and the other one with range 5 &lt;= sizes &lt; 7, width=2 and 2 elements.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - An 1D tensor of integers.</li> <li><code>max_width</code> - The maximum width of a bucket, should be a positive integer.</li> <li><code>min_bucket_count</code> - The minimum count of a bucket, should be a positive integer.   Bucket size may be smaller than min_bucket_count if its width reaches max_width.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If the provided sizes is empty, or not integers.</li> <li><code>ValueError</code> - If max_width is not a positive integer or min_bucket_count is not a positive integer.</li> </ul> <p>Returns:</p> <p>A named tuple containing bucket boundaries in ascending order and the number of elements in each bucket.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.utils import create_buckets\n\n&gt;&gt;&gt; sizes = torch.tensor([1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 22, 22, 22, 22])\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=5, min_bucket_count=10)\n&gt;&gt;&gt; # 5 buckets: 1 &lt;= sizes &lt; 6, 6 &lt;= sizes &lt; 11, 11 &lt;= sizes &lt; 16, 16 &lt;= sizes &lt; 21, 21 &lt;= sizes &lt; 23\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 1,  6, 11, 16, 21, 23])\n\n&gt;&gt;&gt; # each with 12, 0, 0, 0, 4 elements respectively.\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([12,  0,  0,  0,  4])\n\n&gt;&gt;&gt; sizes = torch.arange(20)\n&gt;&gt;&gt; # min_bucket_count is used to control bucket size\n&gt;&gt;&gt; buckets = create_buckets(sizes, max_width=10, min_bucket_count=5)\n&gt;&gt;&gt; print(buckets.bucket_boundaries)\ntensor([ 0,  5, 10, 15, 20])\n\n&gt;&gt;&gt; print(buckets.bucket_sizes)\ntensor([5, 5, 5, 5])\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sampler","title":"sampler","text":""},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#size_aware_batching","title":"size_aware_batching","text":"<pre><code>def size_aware_batching(\n    dataset: Iterable[Data],\n    sizeof: Callable[[Data], Real],\n    max_total_size: Real,\n    collate_fn: Optional[Callable[[Iterable[Data]], BatchCollated]] = None,\n    info_logger: Optional[Callable[[str], None]] = None,\n    warn_logger: Optional[Callable[[str], None]] = None\n) -&gt; Iterator[Union[List[Data], BatchCollated]]\n</code></pre> <p>A generator that batches elements from an iterable while ensuring that the total size of each batch does not exceed a specified maximum. Here the size can be a measurement of memory consumption of the elements in the batch. This can be useful for both indexible data or non-indexible but iterable data.</p> <p>Arguments:</p> <ul> <li><code>dataset</code> - The input iterable.</li> <li><code>sizeof</code> - A function or mapping that returns the \"size\" of each element in <code>dataset</code>.   E.g., this can be used to determine how much memory an element consumes. Its return   type must be comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total \"size\" of each batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>collate_fn</code> - An optional function to collate batches. Defaults to None, in which case   each batch is a list of elements from the input dataset</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults to None.</li> </ul> <p>Yields:</p> <p>A generator that yields batches from <code>dataset</code>.</p> <p>Assumptions   1. Linear complexity. This function consumes the given Iterable of data (<code>dataset</code>) once,   by going over the data item one by one to build a batch and yield it as soon as the   addition of the next data item to the batch would exceed <code>max_total_size</code> or if the   batch is the last one (end of iteration)   2. Additive size measurement. For the general usage case of building mini-batches with   a threshold of the batch's memory consumption, it assumes that the size of the batch is   the sum of all elements in the batch (additive property).   3. Comparable type of <code>max_total_size</code> and <code>sizeof</code>'s return. <code>sizeof</code>'s return values   must be compared with <code>max_total_size</code> to threshold the size of batches</p> <p>Caveat - <code>1</code> - The generated batch sizes may have large variance   - how to workaround: filter the output of this generator using a batch size threshold - <code>2</code> - The number of batches may vary a lot across different epochs.   - how to workaround: increase the number of steps that compose an epoch,   e.g., in the Lightning training/validation loop, which effectively increases the input   dataset size per epoch</p> <p>Example</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch.utils.data import default_collate\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import size_aware_batching\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n&gt;&gt;&gt; # Define a sizeof function that returns the size of each tensor\n&gt;&gt;&gt; def sizeof(x):\n...     return x.numel()\n\n&gt;&gt;&gt; # Create a generator with max_total_size=4 and default_collate_fn\n&gt;&gt;&gt; gen = size_aware_batching(dataset, sizeof, 4, collate_fn=default_collate)\n&gt;&gt;&gt; batches = list(gen)\n&gt;&gt;&gt; print(batches)\n    [tensor([[1, 2], [3, 4]]), tensor([[5, 6], [7, 8]]), tensor([[9, 10]])]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#sizeawarebatchsampler-objects","title":"SizeAwareBatchSampler Objects","text":"<pre><code>class SizeAwareBatchSampler(Sampler[List[int]])\n</code></pre> <p>A sampler that batches elements of varying sizes while ensuring that the total size of each batch does not exceed a specified maximum.</p> <p>This is useful when dealing with datasets where each element has a different size, such as graphs or sequences of varying lengths. The sampler uses a provided <code>sizeof</code> function to determine the size of each element in the dataset and ensures that the total size of each batch does not exceed the specified <code>max_total_size</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n\n\n&gt;&gt;&gt; # Define a sample dataset with torch.tensor\n&gt;&gt;&gt; dataset = [torch.tensor([1, 2]), torch.tensor([3, 4]), torch.tensor([5, 6]),\n...            torch.tensor([7, 8]), torch.tensor([9, 10])]\n\n\n&gt;&gt;&gt; # Define a function that returns the size of each element in the dataset.\n&gt;&gt;&gt; def sizeof(index):\n...     return dataset[index].numel()\n\n\n&gt;&gt;&gt; # Create a SizeAwareBatchSampler with a maximum total batch size of 10.\n&gt;&gt;&gt; batch_sampler = SizeAwareBatchSampler(\n...     sampler=torch.utils.data.SequentialSampler(dataset),\n...     sizeof=sizeof,\n...     max_total_size=4\n... )\n\n\n&gt;&gt;&gt; # Iterate over batches of indices that do not exceed the maximum total size.\n&gt;&gt;&gt; print(list(batch_sampler))\n    [[0, 1], [2, 3], [4]]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(sampler: Union[Sampler[List[int]], Iterable[int]],\n             sizeof: Callable[[int], Real],\n             max_total_size: Real,\n             info_logger: Optional[Callable[[str], None]] = None,\n             warn_logger: Optional[Callable[[str], None]] = None) -&gt; None\n</code></pre> <p>Initializes the SizeAwareBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sampler</code> - The underlying sampler.</li> <li><code>sizeof</code> - A function that returns the size at each index. E.g., this can used to   determine how much memory an element consumes. Its return type must be   comparable with <code>max_total_size</code> and it must be addable (operator <code>+</code>).</li> <li><code>max_total_size</code> - The maximum total size of a mini-batch. The semantics of \"size\"   is defined by the <code>sizeof</code> argument. The type of this value must be comparable   with the return type of sizeof, i.e., the operator <code>&lt;</code> and <code>==</code> must be meaningful.</li> <li><code>info_logger</code> - A function to log info. Defaults to None.</li> <li><code>warn_logger</code> - A function to log warnings. Defaults None.</li> </ul> <p>Raises:</p> <ul> <li><code>TypeError</code> - If sampler is not an instance of Sampler or Iterable, or if sizeof is not a callable, dictionary, or sequence container.</li> <li><code>ValueError</code> - If max_total_size is not a positive number.</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter__","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices that do not exceed the maximum total size.</p> <p>Yields:</p> <p>A batch of indices that do not exceed the maximum total size.</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#bucketbatchsampler-objects","title":"BucketBatchSampler Objects","text":"<pre><code>class BucketBatchSampler(Sampler[List[int]])\n</code></pre> <p>A batch sampler to create batches with sizes of elements from each pre-defined bucket ranges.</p> <p>Elements of the dataset are first grouped into each bucket based on the bucket ranges and the sizes of elements. Then, a base batch sampler is used for each bucket to create mini-batches.</p> <p>The bucket ranges are specified by <code>bucket_boundaries</code>, which will be first sorted internally and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals. e.g. if bucket_boundaries tensor is [10, 5, 0, 16], it will be sorted as [0, 5, 10, 16] and 3 buckets will be created with ranges: [0, 5), [5, 10), [10, 16).</p> <p>The base batch sampler will be created by passing the element indices in each bucket as the data source, and <code>base_batch_sampler_shared_kwargs</code> and <code>base_batch_sampler_individual_kwargs</code> to the constructor of the base batch sampler class specified as <code>base_batch_sampler_class</code>. e.g. <code>base_batch_sampler_shared_kwargs = {'drop_last': True}</code> and <code>base_batch_sampler_individual_kwargs = {'batch_size': [8,10,12]}</code> will be used to create 3 batch samplers with drop_last=True and batch_size=8, 10 and 12, and initialized like <code>base_batch_sampler_class(bucket_element_indices[0], batch_size=8, drop_last=True)</code>.</p> <p>In the <code>__iter__</code> method, if <code>shuffle</code> is <code>True</code>, the element indices in each bucket will be shuffled, and a bucket is randomly selected each time to create a mini-batch. If <code>shuffle</code> is <code>False</code>, there is no shuffle on element indices, and the bucket is selected in ascending order of its interval boundaries.</p> <p>This class is used to create homogeneous batches of data for training or evaluation, and reduce the padding necessary to align the shape of elements.</p> <p>Modified from https://github.com/rssrwn/semla-flow/blob/main/semlaflow/data/util.py</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import BucketBatchSampler\n\n&gt;&gt;&gt; # Define the sizes for a dataset\n&gt;&gt;&gt; sizes = torch.arange(25)\n&gt;&gt;&gt; # Define bucket ranges\n&gt;&gt;&gt; bucket_boundaries = torch.tensor([0, 6, 15, 25])\n\n&gt;&gt;&gt; # Create a bucket batch sampler with torch.utils.data.BatchSampler as base batch sampler\n&gt;&gt;&gt; # As there are 3 buckets, there will be 3 base batch samplers with batch sizes 2, 3, and 5.\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=False,\n    )\n\n&gt;&gt;&gt; # Iterate over batches of indices that lies in the same bucket and with different batch sizes.\n&gt;&gt;&gt; print(list(batch_sampler))\n[[0, 1], [2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n\n&gt;&gt;&gt; # randomize the dataset and buckets\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=torch.utils.data.BatchSampler,\n        base_batch_sampler_shared_kwargs={'drop_last': False},\n        base_batch_sampler_individual_kwargs={'batch_size': [2,3,5]},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(batch_sampler))\n[[24, 17, 16, 22, 19], [2, 5], [12, 10, 11], [3, 0], [15, 18, 20, 21, 23], [7, 13, 6], [14, 9, 8], [1, 4]]\n&gt;&gt;&gt; print(list(batch_sampler))\n[[14, 9, 13], [23, 16, 20, 21, 15], [5, 0], [8, 10, 11], [17, 24, 22, 18, 19], [12, 6, 7], [4, 2], [3, 1]]\n\n&gt;&gt;&gt; # Combine with SizeAwareBatchSampler to control the cost of each batch\n&gt;&gt;&gt; from bionemo.size_aware_batching.sampler import SizeAwareBatchSampler\n&gt;&gt;&gt; item_costs = sizes.tolist()\n&gt;&gt;&gt; def cost_of_element(index):\n        return item_costs[index]\n&gt;&gt;&gt; batch_sampler = BucketBatchSampler(\n        sizes=sizes,\n        bucket_boundaries=bucket_boundaries,\n        base_batch_sampler_class=SizeAwareBatchSampler,\n        base_batch_sampler_shared_kwargs={\"sizeof\": cost_of_element, \"max_total_size\": 40},\n        base_batch_sampler_individual_kwargs={},\n        shuffle=True,\n        generator=torch.Generator().manual_seed(0),\n    )\n&gt;&gt;&gt; print(list(iter(batch_sampler)))\n[[24], [2, 5, 3, 0, 1, 4], [12, 10, 11, 7], [13, 6, 14], [17, 16], [22], [19, 15], [9, 8], [18, 20], [21], [23]]\n</code></pre> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(sizes: torch.Tensor,\n             bucket_boundaries: torch.Tensor,\n             base_batch_sampler_class: Type[Sampler],\n             base_batch_sampler_shared_kwargs: Optional[Dict[str, Any]] = None,\n             base_batch_sampler_individual_kwargs: Optional[Dict[\n                 str, Iterable]] = None,\n             shuffle: Optional[bool] = True,\n             generator: Optional[torch.Generator] = None) -&gt; None\n</code></pre> <p>Initializes the BucketBatchSampler.</p> <p>Arguments:</p> <ul> <li><code>sizes</code> - A 1D tensor of real numbers representing the size of each element in the dataset.</li> <li><code>bucket_boundaries</code> - A 1D tensor of real numbers representing the boundaries of the bucket ranges.   It will be first sorted and used to create <code>len(bucket_boundaries) - 1</code> left-closed right-open intervals as bucket ranges.   It should not contain any duplicate values.</li> <li><code>base_batch_sampler_class</code> - Base batch sampler class type, which will be used for each bucket, and initialized with the bucket element indices,   <code>base_batch_sampler_shared_kwargs</code> and the corresponding <code>base_batch_sampler_individual_kwargs</code>.</li> <li><code>base_batch_sampler_shared_kwargs</code> - Shared keyword argument dictionary used to initialize all base batch samplers for all buckets.   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_individual_kwargs</code>. Default to  {}.</li> <li><code>base_batch_sampler_individual_kwargs</code> - Keyword argument dictionary used to initialize   each bucket batch sampler with the corresponding key value pairs.   Length of each value in this dict must be equal to len(bucket_boundaries) - 1 (the number of buckets).   Sufficient and valid arguments should be provided for <code>base_batch_sampler_class</code> with <code>base_batch_sampler_shared_kwargs</code>.   Default to  {}.</li> <li><code>shuffle</code> - A boolean indicating whether to shuffle the dataset and buckets. Defaults to True.</li> <li><code>generator</code> - Generator used in sampling. Defaults to None.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code> - If <code>sizes</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>bucket_boundaries</code> is not a 1D tensor of real numbers.</li> <li><code>ValueError</code> - If <code>base_batch_sampler_individual_kwargs</code> or <code>base_batch_sampler_individual_kwargs</code> is not a keyword argument dictionary.</li> <li><code>ValueError</code> - If the length of values in the dict of <code>base_batch_sampler_individual_kwargs</code> must be equal to len(bucket_boundaries) - 1.</li> <li><code>RuntimeError</code> - If there is no elements with sizes inside the ranges specified by <code>bucket_boundaries</code>.</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__len__","title":"__len__","text":"<pre><code>def __len__() -&gt; int\n</code></pre> <p>Get the number of batches.</p> <p>Can only be called if the <code>base_batch_sampler_class</code> has len() implemented</p> <p>Returns:</p> <ul> <li><code>int</code> - Number of batches</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-size-aware-batching/bionemo-size-aware-batching-Overview/#__iter___1","title":"__iter__","text":"<pre><code>def __iter__() -&gt; Iterator[List[int]]\n</code></pre> <p>Iterate over batches of indices.</p> <p>This function yields batches of indices of elements with sizes from each bucket range.</p> <p>Yields:</p> <ul> <li><code>List[int]</code> - A batch of indices of elements with sizes from each bucket range.</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-testing/bionemo-testing-Overview/","title":"bionemo-testing","text":"<p>A package of test-time requirements and utilities for bionemo sub-packages. In particular, the <code>bionemo-testing</code> package handles downloading and caching data and other assets for running unit tests and example notebooks. For more information on test data handling, see BioNeMo test data management</p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/","title":"bionemo-webdatamodule","text":"<p>To install, execute the following: <pre><code>pip install -e .\n</code></pre></p> <p>To run unit tests, execute: <pre><code>pytest -v .\n</code></pre></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#webdatamodule","title":"WebDataModule","text":"<pre><code>class WebDataModule(L.LightningDataModule)\n</code></pre> <p>A LightningDataModule for using webdataset tar files to setup dataset and dataloader. This data module takes as input a dictionary: Split -&gt; tar file directory and vaiours webdataset config settings. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow</p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#examples","title":"Examples","text":"<ol> <li> <p>create the data module with input directory to webdataset tar files. Depending on which of the downstream Lightning.Trainer methods are called, e.g., <code>Trainer.fit()</code>, <code>Trainer.validate()</code>, <code>Trainer.test()</code> or <code>Trainer.predict()</code>, only a subset of the train, val and test splits need to be specified in the various input options to the data module:</p> </li> <li> <p><code>Trainer.fit()</code> requires the <code>train</code> and <code>val</code> splits</p> </li> <li><code>Trainer.validate()</code> requires the <code>val</code> split</li> <li><code>Trainer.test()</code> requires the <code>test</code> splits</li> <li><code>Trainer.predict()</code> requires the <code>test</code> splits</li> </ol> <p>Here is an example of constructing the data module for <code>Trainer.fit()</code>: <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, WebDataModule\n&gt;&gt;&gt;\n&gt;&gt;&gt; tar_file_prefix = \"shards\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; dirs_of_tar_files = {\n&gt;&gt;&gt;     Split.train: \"/path/to/train/split/tars\",\n&gt;&gt;&gt;     Split.val: \"/path/to/val/split/tars\",\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; n_samples {\n&gt;&gt;&gt;     Split.train: 1000,\n&gt;&gt;&gt;     Split.val: 100,\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # this is the string to retrieve the corresponding data object from the\n&gt;&gt;&gt; # webdataset file (see\n&gt;&gt;&gt; # https://github.com/webdataset/webdataset?tab=readme-ov-file#the-webdataset-format\n&gt;&gt;&gt; # for details)\n&gt;&gt;&gt; suffix_keys_wds = \"tensor.pyd\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # see the API doc for the definition of global_batch_size\n&gt;&gt;&gt; global_batch_size = 16\n&gt;&gt;&gt;\n&gt;&gt;&gt; seed = 27193781\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Specify the routines to process the samples in the WebDataset object.\n&gt;&gt;&gt; # The routine is a generator of an Iterable of generators that are chained\n&gt;&gt;&gt; # together by nested function calling. The following is equivalent of\n&gt;&gt;&gt; # defining a overall generator of `shuffle(untuple(...))` which\n&gt;&gt;&gt; # untuples the samples and shuffles them. See webdataset's Documentation\n&gt;&gt;&gt; # for details.\n&gt;&gt;&gt; # NOTE: the `untuple` is almost always necessary due to the webdataset's\n&gt;&gt;&gt; # file parsing rule.\n&gt;&gt;&gt;\n&gt;&gt;&gt; untuple = lambda source : (sample for (sample,) in source)\n&gt;&gt;&gt;\n&gt;&gt;&gt; from webdatast import shuffle\n&gt;&gt;&gt; pipeline_wds = {\n&gt;&gt;&gt;     Split.train : [untuple, shuffle(n_samples[Split.train],\n&gt;&gt;&gt;                                     rng=random.Random(seed_rng_shfl))],\n&gt;&gt;&gt;     Split.val: untuple\n&gt;&gt;&gt; }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Similarly the user can optionally define the processing routine on the\n&gt;&gt;&gt; # WebLoader (the dataloader of webdataset).\n&gt;&gt;&gt; # NOTE: these routines by default take unbatched sample as input so the\n&gt;&gt;&gt; # user can customize their batching routines here\n&gt;&gt;&gt;\n&gt;&gt;&gt; batch = batched(local_batch_size, collation_fn=lambda\n                    list_samples : torch.vstack(list_samples))\n&gt;&gt;&gt; pipeline_prebatch_wld = {\n        Split.train: [shuffle(n_samples[Split.train],\n                              rng=random.Random(seed_rng_shfl)), batch],\n        Split.val : batch,\n        Split.test : batch\n    }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # the user can optionally specify the kwargs for WebDataset and\n&gt;&gt;&gt; # WebLoader\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wds = {\n&gt;&gt;&gt;     split : {'shardshuffle' : split == Split.train,\n&gt;&gt;&gt;              'nodesplitter' : wds.split_by_node,\n&gt;&gt;&gt;              'seed' : seed_rng_shfl}\n&gt;&gt;&gt;     for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; kwargs_wld = {\n&gt;&gt;&gt;     split : {\"num_workers\": 2} for split in Split\n&gt;&gt;&gt;     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; # construct the data module\n&gt;&gt;&gt; data_module = WebDataModule(dirs_of_tar_files, n_samples, suffix_keys_wds,\n                                global_batch_size,\n                                prefix_tars_wds=tar_file_prefix,\n                                pipeline_wds=pipeline_wds,\n                                pipeline_prebatch_wld=pipeline_prebatch_wld,\n                                kwargs_wds=kwargs_wds,\n                                kwargs_wld=kwargs_wld)\n</code></pre></p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init__","title":"__init__","text":"<pre><code>def __init__(\n        dirs_tars_wds: Dict[Split, str],\n        n_samples: Dict[Split, int],\n        suffix_keys_wds: Union[str, Iterable[str]],\n        global_batch_size: int,\n        prefix_tars_wds: str = \"wdshards\",\n        pipeline_wds: Optional[Dict[Split, Union[Iterable[Iterable[Any]],\n                                                 Iterable[Any]]]] = None,\n        pipeline_prebatch_wld: Optional[Dict[Split,\n                                             Union[Iterable[Iterable[Any]],\n                                                   Iterable[Any]]]] = None,\n        kwargs_wds: Optional[Dict[Split, Dict[str, Any]]] = None,\n        kwargs_wld: Optional[Dict[Split, Dict[str, Any]]] = None)\n</code></pre> <p>constructor</p> <p>Arguments:</p> <ul> <li><code>dirs_tars_wds</code> Dict[Split, str] - input dictionary: Split -&gt; tar file   directory that contains the webdataset tar files for each split</li> <li><code>n_samples</code> Dict[Split, int] - input dictionary: Split -&gt; number of   data samples for each split</li> <li><code>suffix_keys_wds</code> Union[str, Iterable[str]] - a set of keys each   corresponding to a data object in the webdataset tar file   dictionary. The data objects of these keys will be extracted and   tupled for each sample in the tar files</li> <li><code>global_batch_size</code> int - size of batch summing across nodes in Data   Distributed Parallel, i.e., local_batch_size * n_nodes. NOTE:   this data module doesn't rely on the input <code>global_batch_size</code>   for batching the samples. The batching is supposed to be done as   a part of the input <code>pipeline_prebatch_wld</code>. <code>global_batch_size</code>   is only used to compute a (pseudo-) epoch length for the data   loader so that the loader yield approximately n_samples //   global_batch_size batches   Kwargs:</li> <li><code>prefix_tars_wds</code> str - name prefix of the input webdataset tar   files. The input tar files are globbed by   \"{dirs_tars_wds[split]}/{prefix_tars_wds}-*.tar\"   pipeline_wds (Optional[Dict[Split, Union[Iterable[Iterable[Any]],</li> <li><code>Iterable[Any]]]])</code> - a dictionary of webdatast composable, i.e.,   functor that maps a iterator to another iterator that   transforms the data sample yield from the dataset object, for   different splits, or an iterable to such a sequence of such   iterators. For example, this can be used to transform the   sample in the worker before sending it to the main process of   the dataloader   pipeline_prebatch_wld (Optional[Dict[Split,   Union[Iterable[Iterable[Any]], Iterable[Any]]]]): a dictionary   of webloader composable, i.e., functor that maps a iterator to   another iterator that transforms the data sample yield from the   WebLoader object, for different splits, or an iterable to a   seuqnence of such iterators. For example, this can be used for   batching the samples. NOTE: this is applied before batching is   yield from the WebLoader</li> <li><code>kwargs_wds</code> Optional[Dict[Split, Dict[str,  Any]]] - kwargs for the   WebDataset.init()</li> <li><code>kwargs_wld</code> Optional[Dict[Split, Dict[str,  Any]]] - kwargs for the   WebLoader.init(), e.g., num_workers, of each split</li> </ul> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow. Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses.</p> <p>Returns: None</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#setup","title":"setup","text":"<pre><code>def setup(stage: str) -&gt; None\n</code></pre> <p>This is called on all Lightning-managed nodes in a multi-node training session</p> <p>Arguments:</p> <ul> <li><code>stage</code> str - \"fit\", \"test\" or \"predict\"</li> <li><code>Returns</code> - None</li> </ul>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#pickleddatawds","title":"PickledDataWDS","text":"<pre><code>class PickledDataWDS(WebDataModule)\n</code></pre> <p>A LightningDataModule to process pickled data into webdataset tar files and setup dataset and dataloader. This inherits the webdataset setup from its parent module <code>WebDataModule</code>. This data module takes a directory of pickled data files, data filename prefixes for train/val/test splits, data filename suffixes and prepare webdataset tar files by globbing the specific pickle data files <code>{dir_pickles}/{name_subset[split]}.{suffix_pickles}</code> and outputing to webdataset tar file with the dict structure: <pre><code>    {\"__key__\" : name.replace(\".\", \"-\"),\n     suffix_pickles : pickled.dumps(data) }\n</code></pre> NOTE: this assumes only one pickled file is processed for each sample. In its setup() function, it creates the webdataset object chaining up the input <code>pipeline_wds</code> workflow. In its train/val/test_dataloader(), it creates the WebLoader object chaining up the <code>pipeline_prebatch_wld</code> workflow.</p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#examples_1","title":"Examples","text":"<ol> <li>create the data module with a directory of pickle files and the file name prefix thereof for different splits to used by <code>Lightning.Trainer.fit()</code></li> </ol> <pre><code>&gt;&gt;&gt; from bionemo.webdatamodule.datamodule import Split, PickledDataWDS\n\n&gt;&gt;&gt; dir_pickles = \"/path/to/my/pickles/dir\"\n\n&gt;&gt;&gt; # the following will use `sample1.mydata.pt` and `sample2.mydata.pt` as the\n&gt;&gt;&gt; # training dataset and `sample4.mydata.pt` and `sample5.mydata.pt` as the\n&gt;&gt;&gt; # validation dataset\n\n&gt;&gt;&gt; suffix_pickles = \"mydata.pt\"\n\n&gt;&gt;&gt; names_subset = {\n&gt;&gt;&gt;     Split.train: [sample1, sample2],\n&gt;&gt;&gt;     Split.val: [sample4, sample5],\n&gt;&gt;&gt; }\n\n&gt;&gt;&gt; # the following setting will attempt to create at least 5 tar files in\n&gt;&gt;&gt; # `/path/to/output/tars/dir/myshards-00000{0-5}.tar`\n\n&gt;&gt;&gt; n_tars_wds = 5\n&gt;&gt;&gt; prefix_tars_wds = \"myshards\"\n&gt;&gt;&gt; output_dir_tar_files = \"/path/to/output/tars/dir\"\n\n&gt;&gt;&gt; # see the `WebDataModule` API doc for the definition of global_batch_size\n&gt;&gt;&gt; global_batch_size = 16\n\n&gt;&gt;&gt; # user can optionally customize the data processing routines and kwargs used\n&gt;&gt;&gt; # in the WebDataset and WebLoader (see the examples in `WebDataModule`)\n\n&gt;&gt;&gt; pipeline_wds = { Split.train: ... }\n\n&gt;&gt;&gt; pipeline_prebatch_wld = { Split.train: ... }\n\n&gt;&gt;&gt; kwargs_wds = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; kwargs_wld = { Split.train: ..., Split.val: ... }\n\n&gt;&gt;&gt; # create the data module\n&gt;&gt;&gt; data_module = PickledDataWDS(\n&gt;&gt;&gt;     dir_pickles,\n&gt;&gt;&gt;     suffix_pickles,\n&gt;&gt;&gt;     names_subset,\n&gt;&gt;&gt;     output_dir_tar_files,\n&gt;&gt;&gt;     global_batch_size, # `WebDataModule` args\n&gt;&gt;&gt;     n_tars_wds=n_tars_wds,\n&gt;&gt;&gt;     prefix_tars_wds=prefix_tars_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_wds=pipeline_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     pipeline_prebatch_wld=pipelines_wdl_batch, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wds=kwargs_wds, # `WebDataModule` kwargs\n&gt;&gt;&gt;     kwargs_wld=kwargs_wld, # `WebDataModule` kwargs\n&gt;&gt;&gt; )\n</code></pre>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#__init___1","title":"__init__","text":"<pre><code>def __init__(dir_pickles: str,\n             suffix_pickles: str,\n             names_subset: Dict[Split, List[str]],\n             prefix_dir_tars_wds: str,\n             *args,\n             n_tars_wds: Optional[int] = None,\n             **kwargs)\n</code></pre> <p>constructor</p> <p>Arguments:</p> <ul> <li><code>dir_pickles</code> str - input directory of pickled data files</li> <li><code>suffix_pickles</code> str - filename suffix of the input data in   dir_pickles. This is also used as the key mapped to the   tarballed pickled object in the webdataset</li> <li><code>names_subset</code> Dict[Split, List[str]] - list of filename prefix of   the data samples to be loaded in the dataset and dataloader for   each of the split</li> <li><code>prefix_dir_tars_wds</code> str - directory name prefix to store the output   webdataset tar files. The actual directories storing the train, val   and test sets will be suffixed with \"train\", \"val\" and \"test\"   respectively.</li> <li><code>*args</code> - arguments passed to the parent WebDataModule</li> </ul> <p>Kwargs: - <code>n_tars_wds</code> int - attempt to create at least this number of   webdataset shards - <code>**kwargs</code> - arguments passed to the parent WebDataModule</p> <p></p>"},{"location":"user-guide/developer-guide/bionemo-webdatamodule/bionemo-webdatamodule-Overview/#prepare_data_1","title":"prepare_data","text":"<pre><code>def prepare_data() -&gt; None\n</code></pre> <p>This is called only by the main process by the Lightning workflow. Do not rely on this data module object's state update here as there is no way to communicate the state update to other subprocesses. The nesting <code>pickles_to_tars</code> function goes through the data name prefixes in the different splits, read the corresponding pickled file and output a webdataset tar archive with the dict structure: {\"key\" : name.replace(\".\", \"-\"), suffix_pickles : pickled.dumps(data) }.</p> <p>Returns: None</p>"},{"location":"user-guide/examples/bionemo-esm2/finetune/","title":"ESM-2 Fine-Tuning","text":"<p>This readme serves as a demo for implementing ESM-2 Fine-tuning module, running a regression example and using the model for inference.</p> <p>The ESM-2 model is a transformer-based protein language model that has achieved state-of-the-art results in various protein-related tasks. When fine-tuning ESM2, the task head plays a crucial role. A task head refers to the additional layer or set of layers added on top of a pre-trained model, like the ESM-2 transformer-based protein language model, to adapt it for a specific downstream task. As a part of transfer learning, a pre-trained model is often utilized to learn generic features from a large-scale dataset. However, these features might not be directly applicable to the specific task at hand. By incorporating a task head, which consists of learnable parameters, the model can adapt and specialize to the target task. The task head serves as a flexible and adaptable component that learns task-specific representations by leveraging the pre-trained features as a foundation. Through fine-tuning, the task head enables the model to learn and extract task-specific patterns, improving performance and addressing the nuances of the downstream task. It acts as a critical bridge between the pre-trained model and the specific task, enabling efficient and effective transfer of knowledge.</p>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#setup-and-assumptions","title":"Setup and Assumptions","text":"<p>In this tutorial, we will demonstrate how to create a fine-tune module, train a regression task head, and use the fine-tuned model for inference.</p> <p>All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at <code>/workspace/bionemo2</code>. (Note: This <code>WORKDIR</code> may be <code>/workspaces/bionemo-framework</code> if you are using the VSCode Dev Container.) For more information on how to build or pull the BioNeMo2 container, refer to the Access and Startup.</p> <p>To successfully accomplish this we need to define some key classes:</p> <ol> <li>Loss Reduction Method - To compute the supervised fine-tuning loss.</li> <li>Fine-Tuned Model Head - Downstream task head model.</li> <li>Fine-Tuned Model - Model that combines ESM-2 with the task head model.</li> <li>Fine-Tuning Config - Configures the fine-tuning model and loss to use in the training and inference framework.</li> <li>Dataset - Training and inference datasets for ESM2.</li> </ol>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#1-loss-reduction-class","title":"1 - Loss Reduction Class","text":"<p>A class for calculating the supervised loss of the fine-tune model from targets. We inherit from Megatron Bert Masked Language Model Loss (<code>BERTMLMLossWithReduction</code>) and override the <code>forward()</code> pass to compute MSE loss of the regression head within a micro-batch. The <code>reduce()</code> method is used for computing the average over the micro-batches and is only used for logging.</p> <pre><code>class RegressorLossReduction(BERTMLMLossWithReduction):\n    def forward(\n        self, batch: Dict[str, torch.Tensor], forward_out: Dict[str, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Union[PerTokenLossDict, SameSizeLossDict]]:\n\n        targets = batch[\"labels\"]  # [b, 1]\n        regression_output = forward_out\n        loss = torch.nn.functional.mse_loss(regression_output, targets)\n        return loss, {\"avg\": loss}\n\n    def reduce(self, losses_reduced_per_micro_batch: Sequence[ReductionT]) -&gt; torch.Tensor:\n        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n        return losses.mean()\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#2-fine-tuned-model-head","title":"2 - Fine-Tuned Model Head","text":"<p>An MLP class for sequence-level regression. This class inherits <code>MegatronModule</code> and uses the fine-tune config (<code>TransformerConfig</code>) to configure the regression head for the fine-tuned ESM-2 model.</p> <pre><code>class MegatronMLPHead(MegatronModule):\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        layer_sizes = [config.hidden_size, 256, 1]\n        self.linear_layers = torch.nn.ModuleList(\n            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]\n        )\n        self.act = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n\n    def forward(self, hidden_states: torch.Tensor) -&gt; List[torch.Tensor]:\n        ...\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#3-fine-tuned-model","title":"3 - Fine-Tuned Model","text":"<p>A fine-tuned ESM-2 model class for token classification tasks. This class inherits from the <code>ESM2Model</code> class and adds the custom regression head <code>MegatronMLPHead</code> the we created in the previous step. Optionally one can freeze all or parts of the encoder by parsing through the model parameters in the model constructor.</p> <pre><code>class ESM2FineTuneSeqModel(ESM2Model):\n    def __init__(self, config, *args, post_process: bool = True, return_embeddings: bool = False, **kwargs):\n        super().__init__(config, *args, post_process=post_process, return_embeddings=True, **kwargs)\n\n        # freeze encoder parameters\n        if config.encoder_frozen:\n            for _, param in self.named_parameters():\n                param.requires_grad = False\n\n        if post_process:\n            self.regression_head = MegatronMLPHead(config)\n\n    def forward(self, *args, **kwargs,):\n        output = super().forward(*args, **kwargs)\n        ...\n        regression_output = self.regression_head(embeddings)\n        return regression_output\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#4-fine-tuning-config","title":"4 - Fine-Tuning Config","text":"<p>A <code>dataclass</code> that configures the fine-tuned ESM-2 model. In this example <code>ESM2FineTuneSeqConfig</code> inherits from <code>ESM2GenericConfig</code> and adds custom arguments to setup the fine-tuned model. The <code>configure_model()</code> method of this <code>dataclass</code> is called within the <code>Lightning</code> module to call the model constructor with the <code>dataclass</code> arguments.</p> <p>The common arguments among different fine-tuning tasks are</p> <ul> <li><code>model_cls</code>: The fine-tune model class (<code>ESM2FineTuneSeqModel</code>)</li> <li><code>initial_ckpt_path</code>: BioNeMo 2.0 ESM-2 pre-trained checkpoint</li> <li><code>initial_ckpt_skip_keys_with_these_prefixes</code>: skip keys when loading parameters from a checkpoint. Here we should not look for <code>regression_head</code> in the pre-trained checkpoint.</li> <li><code>get_loss_reduction_class()</code>: Implements selection of the appropriate <code>MegatronLossReduction</code> class, e.g. <code>bionemo.esm2.model.finetune.finetune_regressor.RegressorLossReduction</code>.</li> </ul> <pre><code>@dataclass\nclass ESM2FineTuneSeqConfig(ESM2GenericConfig[ESM2FineTuneSeqModel], iom.IOMixinWithGettersSetters):\n    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n    # self.override_parent_fields will be loaded from the checkpoint and override those values here.\n    initial_ckpt_path: str | None = None\n    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n    # that has this new head and want to keep using these weights, please drop this next line or set to []\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n\n    encoder_frozen: bool = True  # freeze encoder parameters\n    ft_dropout: float = 0.25  # MLP layer dropout\n\n    def get_loss_reduction_class(self) -&gt; Type[MegatronLossReduction]:\n        return RegressorLossReduction\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#5-dataset","title":"5 - Dataset","text":"<p>We will use a sample dataset for demonstration purposes. Create a dataset class by extending from <code>torch.utils.data.Dataset</code>. For the purposes of this demo, we'll assume dataset consists of small set of protein sequences with a target value of <code>len(sequence) / 100.0</code> as their labels.</p> <pre><code>data = [\n    (\"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERH\", 0.33),\n    ...\n]\n</code></pre> <p>Therefore, the custom BioNeMo dataset class will be appropriate (found in <code>bionemo.esm2.model.finetune.finetune_regressor.InMemorySingleValueDataset</code>) as it facilitates predicting on a single value. An excerpt from the class is shown below. This example dataset expected a sequence of <code>Tuple</code> that hold <code>(sequence, target)</code> values. However, one can simply extend <code>InMemorySingleValueDataset</code> class in a similar way to customize your class for your data.</p> <pre><code>class InMemorySingleValueDataset(Dataset):\n    def __init__(\n        self,\n        data: Sequence[Tuple[str, float]],\n        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n        seed: int = np.random.SeedSequence().entropy,\n    ):\n</code></pre> <p>For any arbitrary data file formats, user can process the data into a list of tuples containing (sequence, label) and use this dataset class. Or override the dataset class to load their custom data files.</p> <p>To coordinate the creation of training, validation and testing datasets from your data, we need to use a <code>datamodule</code> class. To do this we can directly use or extend the <code>ESM2FineTuneDataModule</code> class (located at <code>bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule</code>) which defines helpful abstract methods that use your dataset class.</p> <pre><code>dataset = InMemorySingleValueDataset(data)\ndata_module = ESM2FineTuneDataModule(\n    train_dataset=train_dataset,\n    valid_dataset=valid_dataset\n    micro_batch_size=4,   # size of a batch to be processed in a device\n    global_batch_size=8,  # size of batch across all devices. Should be multiple of micro_batch_size\n)\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#fine-tuning-the-regressor-task-head-for-esm2","title":"Fine-Tuning the Regressor Task Head for ESM2","text":"<p>Now we can put these five requirements together to fine-tune a regressor task head starting from a pre-trained ESM-2 model (<code>pretrain_ckpt_path</code>). We can take advantage of a simple training loop in <code>bionemo.esm2.model.fnetune.train</code> and use the <code>`train_model()</code> function to start the fine-tuning process in the following.</p> <pre><code># create a List[Tuple] with (sequence, target) values\nartificial_sequence_data = [\n    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n]\n\ndata = [(seq, len(seq)/100.0) for seq in artificial_sequence_data]\n\n# we are training and validating on the same dataset for simplicity\ndataset = InMemorySingleValueDataset(data)\ndata_module = ESM2FineTuneDataModule(train_dataset=dataset, valid_dataset=dataset)\n\nwith tempfile.TemporaryDirectory() as experiment_tempdir_name:\n    experiment_dir = Path(experiment_tempdir_name)\n    experiment_name = \"finetune_regressor\"\n    n_steps_train = 50\n    seed = 42\n\n    config = ESM2FineTuneSeqConfig(\n        # initial_ckpt_path=str(pretrain_ckpt_path)\n    )\n\n    checkpoint, metrics, trainer = train_model(\n        experiment_name=experiment_name,\n        experiment_dir=experiment_dir,  # new checkpoint will land in a subdir of this\n        config=config,  # same config as before since we are just continuing training\n        data_module=data_module,\n        n_steps_train=n_steps_train,\n    )\n</code></pre> <p>This example is fully implemented in <code>bionemo.esm2.model.finetune.train</code> and can be executed by: <pre><code>python /workspace/bionemo2/sub-packages/bionemo-esm2/src/bionemo/esm2/model/finetune/train.py\n</code></pre></p>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#notes","title":"Notes","text":"<ol> <li>The above example is fine-tuning a randomly initialized ESM-2 model for demonstration purposes. In order to fine-tune a pre-trained ESM-2 model, please download the ESM-2 650M checkpoint from NGC resources using the following bash command     <pre><code>download_bionemo_data esm2/650m:2.0 --source ngc\n</code></pre>     and pass the output path (e.g. <code>.../.cache/bionemo/975d29ee980fcb08c97401bbdfdcf8ce-esm2_650M_nemo2.tar.gz.untar</code>) as an argument into <code>initial_ckpt_path</code> while setting the config object:     <pre><code>config = ESM2FineTuneSeqConfig(\n    initial_ckpt_path=str(pretrain_ckpt_path)\n)\n</code></pre></li> <li>Due to Megatron limitations, the log produced by the training run iterates on steps/iterations and not epochs. Therefore, <code>Training epoch</code> counter stays at value zero while <code>iteration</code> and <code>global_ste</code>p increase during the course of training (example in the following).     <pre><code>Training epoch 0, iteration &lt;x/max_steps&gt; | ... | global_step: &lt;x&gt; | reduced_train_loss: ... | val_loss: ...\n</code></pre>     to achieve the same epoch-based effect while training, please choose the number of training steps (<code>n_steps_train</code>) so that:     <pre><code>n_steps_train * global_batch_size = len(dataset) * desired_num_epochs\n</code></pre></li> <li>We are using a small dataset of artificial sequences as our fine-tuning data in this example. You may experience over-fitting and observe no change in the validation metrics.</li> </ol>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#fine-tuned-esm-2-model-inference","title":"Fine-Tuned ESM-2 Model Inference","text":"<p>Once we have a checkpoint we can create a config object by pointing the path in <code>initial_ckpt_path</code> and use that for inference. Since we need to load all the parameters from this checkpoint (and don't skip the head) we reset the <code>nitial_ckpt_skip_keys_with_these_prefixes</code> in this config. Now we can use the <code>bionemo.esm2.model.fnetune.train.infer</code> to run inference on prediction dataset.</p> <pre><code>config = ESM2FineTuneSeqConfig(\n    initial_ckpt_path = finetuned_checkpoint,\n    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=list)\n)\n</code></pre> <p>This example is implemented in <code>bionemo.esm2.model.finetune.infer</code> and can be executed by:</p> <pre><code>python /workspace/bionemo2/sub-packages/bionemo-esm2/src/bionemo/esm2/model/finetune/infer.py\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/finetune/#notes_1","title":"Notes","text":"<ol> <li>For demonstration purposes, executing the above command will infer a randomly initialized <code>ESM2FineTuneSeqModel</code> unless <code>initial_ckpt_path</code> is specified and set to an already trained model.</li> <li>If a fine-tuned checkpoint is provided as (<code>initial_ckpt_path</code>) the <code>initial_ckpt_skip_keys_with_these_prefixes</code> should reset to <code>field(default_factory=list)</code> and avoid skipping any parameters.</li> </ol>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/","title":"ESM-2 Pretraining","text":"<p>This tutorial serves as a demo for pretraining ESM2 from scratch with UniProt sequences.</p> <p>The ESM-2 model is a transformer-based protein language model that was pretrained on masked language model (MLM) task. The objective is to recover the original amino acid types of the perturbed locations from the rest of the protein sequences. Through pretraining, ESM-2 learns the evolutionary information in protein sequences similar to conservation analysis and Pott's model, and predicts the optimal mutations on any given protein sequence.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#setup-and-assumptions","title":"Setup and Assumptions","text":"<p>In this tutorial, we will demonstrate how to create an ESM-2 pretraining data module, and create and train a ESM-2 model.</p> <p>All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at <code>/workspace/bionemo2</code>.  For more information on how to build or pull the BioNeMo2 container, refer to the Initialization Guide.</p> <p>Note</p> <p>This <code>WORKDIR</code> may be <code>/workspaces/bionemo-framework</code> if you are using the VSCode Dev Container.</p> <p>Similar to PyTorch Lightning, we have to define some key classes:</p> <ol> <li><code>MegatronStrategy</code> - To launch and setup parallelism for NeMo and Megatron-LM.</li> <li><code>Trainer</code> - To configure training configurations and logging.</li> <li><code>ESMDataModule</code> - To load pretraining training and validation data with mapped UniRef90 sequences to UniRef50 clusters.</li> <li><code>ESM2Config</code> - To configure the ESM-2 model as <code>BionemoLightningModule</code>.</li> </ol>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#1-megatronstrategy","title":"1 - MegatronStrategy","text":"<p>BioNeMo2 supports data parallel (DP), tensor parallel (TP) and pipeline parallel (PP) for training large models. Instead of <code>DDPStrategy</code> in PyTorch Lightning, we use <code>MegatronStrategy</code> to launch and setup parallelism for NeMo and Megatron-LM.</p> <pre><code>from nemo import lightning as nl\nfrom bionemo.llm.utils.datamodule_utils import infer_global_batch_size\n\nmicro_batch_size = 2\nnum_nodes = 1\ndevices = 2\naccumulate_grad_batches = 1\ntensor_model_parallel_size = 2\npipeline_model_parallel_size = 1\n\nglobal_batch_size = infer_global_batch_size(\n    micro_batch_size=micro_batch_size,\n    num_nodes=num_nodes,\n    devices=devices,\n    accumulate_grad_batches=accumulate_grad_batches,\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n)\n\nstrategy = nl.MegatronStrategy(\n    tensor_model_parallel_size=tensor_model_parallel_size,\n    pipeline_model_parallel_size=pipeline_model_parallel_size,\n    ddp=\"megatron\",\n    find_unused_parameters=True,\n    ckpt_include_optimizer=True,\n)\n</code></pre>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#2-trainer","title":"2 - Trainer","text":"<p>BioNeMo2 trainer is very similar to PyTorch Lightning trainer. We can configure the training configurations and logging.</p> <pre><code>from pytorch_lightning.callbacks import LearningRateMonitor, RichModelSummary\nfrom bionemo.llm.lightning import PerplexityLoggingCallback\n\nnum_steps = 20\nlimit_val_batches = 2  # limit the validation epoch to 2 batches\nval_check_interval = 10  # validation epoch every 10 steps\nprecision = \"bf16-mixed\"  # use bf16-mixed precision\n\ntrainer = nl.Trainer(\n    devices=devices,\n    max_steps=num_steps,\n    accelerator=\"gpu\",\n    strategy=strategy,\n    limit_val_batches=limit_val_batches,\n    val_check_interval=val_check_interval,\n    num_nodes=num_nodes,\n    callbacks=[\n        PerplexityLoggingCallback(),\n        RichModelSummary(max_depth=4),\n        LearningRateMonitor(),\n    ],\n    plugins=nl.MegatronMixedPrecision(precision=precision),  # precision is handled through plugins in BioNeMo2\n)\n</code></pre> <p>Here are examples of other possible configurations. <pre><code>from bionemo.core.utils.dtypes import PrecisionTypes\n\nlimit_val_batches_all_data = 1.  # validate on 100% of the validation dataset\nlimit_val_batches_half_data = 0.5  # validate on 50% of the validation dataset\nlimit_val_batches_one_batch = 1  # validate on 1 batch\n\nprint(PrecisionTypes)  # show all possible precision types\n</code></pre></p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#3-esmdatamodule","title":"3 - ESMDataModule","text":"<p>Before instantiating with data module, we can first download the testing ESM-2 pretraining data with <code>download_bionemo_data</code>. The command line will download the data if we haven't yet, and will return the path to the testing data, which is needed to instantiate <code>ESMDataModule</code>.</p> <pre><code>download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc  # test data\n# download_bionemo_data esm2/fulldata_esm2_pretrain:2.0 --source ngc  # full data (~80GB)\n</code></pre> <p>On top of the path to the data directory, BioNeMo2 data module requires global and micro batch sizes to ensure that the input tensors are initialized correctly across model-parallel ranks (see Megatron Dataset Considerations).</p> <pre><code>from bionemo.esm2.data.datamodule import ESMDataModule\nfrom bionemo.esm2.data.dataset import RandomMaskStrategy\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\n\ndata_path = __your_downloaded_test_data_path__  # fill your path from the command line output\n\ntrain_cluster_path = f\"{data_path}/2024_03_sanity/train_clusters_sanity.parquet\"\ntrain_database_path = f\"{data_path}/2024_03_sanity/train_sanity.db\"\nvalid_cluster_path = f\"{data_path}/2024_03_sanity/valid_clusters.parquet\"\nvalid_database_path = f\"{data_path}/2024_03_sanity/validation.db\"\n\nmin_seq_length = None  # optional; filter sequences by minimum length if given\nmax_seq_length = 128  # required; default to 1024\n\nnum_dataset_workers = 1\nrandom_mask_strategy = RandomMaskStrategy.ALL_TOKENS  # default in BioNemo2 and HuggingFace implementation\n\ndata = ESMDataModule(\n    train_cluster_path=train_cluster_path,  # UniRef50 training cluster centers\n    train_database_path=train_database_path,  # UniRef90 training sequences\n    valid_cluster_path=valid_cluster_path,  # UniRef50 validation cluster centers\n    valid_database_path=valid_database_path,  # UniRef90 validation sequences\n    global_batch_size=global_batch_size,\n    micro_batch_size=micro_batch_size,\n    min_seq_length=min_seq_length,\n    max_seq_length=max_seq_length,\n    num_workers=num_dataset_workers,\n    random_mask_strategy=random_mask_strategy,\n)\n</code></pre> <p><code>RandomMaskStrategy</code></p> <p>When trained on MLM objective, the loss function randomly includes 15% of the tokens, within which 80% are masked, 10% are replaced with a random token, and 10% are kept unchanged. Since the vocabulary includes amino acids as well as special tokens, part of the protein sequence may be replaced by a special token. This is the default in both BioNeMo2 and HuggingFace ESM-2 implementation.</p> <p>To enforce amino-acid-only replacement, users can pass <code>random_mask_strategy=RandomMaskStrategy.AMINO_ACID_ONLY</code> to <code>ESMDataModule</code>.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#4-esm2config","title":"4. ESM2Config","text":"<p>Instead of initializing the whole model on each rank, sharded models are lazily created on the target rank with the help of a configuration object. <code>ESM2Config</code> is a dataclass that envelopes architecture parameters (such as <code>num_layers</code>) and the specification of each torch module (<code>ModuleSpec</code>) in the transformer, which are accelerated with flash and fused attentions in TransformerEngine. While we can initialize a model from <code>ESM2Config</code>, its setup is only completed in under <code>trainer.setup</code>, which is called on individual devices.</p> <pre><code>from megatron.core.optimizer import OptimizerConfig\nfrom nemo.lightning.pytorch.optim import MegatronOptimizerModule\n\nfrom bionemo.core.utils.dtypes import get_autocast_dtype\nfrom bionemo.esm2.api import ESM2Config\nfrom bionemo.esm2.data.tokenizer import get_tokenizer\nfrom bionemo.esm2.model.lr_scheduler import WarmupAnnealDecayHoldScheduler\nfrom bionemo.llm.lightning import BionemoLightningModule\nfrom bionemo.llm.model.biobert.lightning import biobert_lightning_module\nfrom bionemo.llm.model.biobert.model import BiobertSpecOption\n\n# ESM-2 650M config\nnum_layers = 33\nhidden_size = 1280\nnum_attention_heads = 20\nffn_hidden_size = 4 * hidden_size\n\nnemo1_init_path = None  # initialize from nemo1 checkpoint\nrestore_from_checkpoint_path = None  # initialize from nemo2 checkpoint\nneed_megatron_variable_seq_lengths_reductions: bool = (\n    pipeline_model_parallel_size * tensor_model_parallel_size &gt; 1 and min_seq_length != max_seq_length\n)  # essential for pipeline/tensor parallel\nbiobert_spec_option = BiobertSpecOption.esm2_bert_layer_with_transformer_engine_spec  # accelerated esm2 with transformer engine\n\nwarmup_steps = 2000\nlr = 1e-4\n\n# Create model config\nesm2_config = ESM2Config(\n    seq_length=max_seq_length,\n    num_layers=num_layers,\n    hidden_size=hidden_size,\n    num_attention_heads=num_attention_heads,\n    ffn_hidden_size=ffn_hidden_size,\n    params_dtype=get_autocast_dtype(precision),\n    pipeline_dtype=get_autocast_dtype(precision),\n    autocast_dtype=get_autocast_dtype(precision),  # setting this speeds things up a lot\n    biobert_spec_option=biobert_spec_option,\n    nemo1_ckpt_path=str(nemo1_init_path) if nemo1_init_path is not None else None,\n    initial_ckpt_path=str(restore_from_checkpoint_path) if restore_from_checkpoint_path is not None else None,\n    variable_seq_lengths=need_megatron_variable_seq_lengths_reductions,\n)\n\n# Create model instance\ntokenizer = get_tokenizer()\n\nmodel: BionemoLightningModule = biobert_lightning_module(\n    esm2_config,\n    tokenizer=tokenizer,\n    optimizer=MegatronOptimizerModule(\n        config=OptimizerConfig(\n            lr=lr,\n            optimizer=\"adam\",\n            use_distributed_optimizer=True,\n            weight_decay=0.01,\n            adam_beta1=0.9,\n            adam_beta2=0.98,\n        ),\n        lr_scheduler=WarmupAnnealDecayHoldScheduler(\n            warmup_steps=warmup_steps, max_steps=num_steps, max_lr=lr, min_lr=lr / 10.0, anneal_percentage=0.10\n        ),\n    ),\n)\n</code></pre> <p><code>ModuleSpec</code></p> <p><code>ModelSpec</code> decides what torch modules are used in the transformer layers. By default, BioNeMo2 accelerates ESM-2 architecture with TransformerEngine layers. Users can define their own <code>ModelSpec</code> for customized transformer layers. See <code>get_biobert_spec</code>.</p> <p><code>BionemoLightningModule</code></p> <p>Since the model is lazily initialized in the target rank, breakpoints for debugging purposes should be added after <code>trainer.setup</code>.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#model-pretraining","title":"Model Pretraining","text":"<p>To close the loop, users can make use of <code>llm.train</code> from NeMo to begin training.</p> <pre><code>from typing import Optional\n\nfrom nemo.collections import llm\nfrom nemo.lightning import resume\nfrom nemo.lightning.pytorch import callbacks as nl_callbacks\n\nfrom bionemo.llm.utils.logger_utils import WandbLoggerOptions, setup_nemo_lightning_logger\n\n\n# WANDB logging\nwandb_options: Optional[WandbLoggerOptions] = (\n    None\n    if wandb_project is None\n    else WandbLoggerOptions(\n        offline=False,\n        project=__your_wandb_project__,\n        entity=__your_wandb_entity__,\n        tags=None,\n        group=None,\n        id=None,\n        anonymous=False,\n        log_model=False,\n    )\n)\n\ncheckpoint_callback = nl_callbacks.ModelCheckpoint(\n    save_last=True,\n    monitor=\"val_loss\",\n    save_top_k=1,\n    every_n_train_steps=100,\n    always_save_context=True,\n)\n\nnemo_logger = setup_nemo_lightning_logger(\n    root_dir=__your_result_dir__,\n    name=__your_experiment_name__,\n    initialize_tensorboard_logger=True,\n    wandb_kwargs=wandb_options,\n    ckpt_callback=checkpoint_callback,\n)\n\nllm.train(\n    model=model,\n    data=data,\n    trainer=trainer,\n    log=nemo_logger,\n    resume=resume.AutoResume(\n        resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n        resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n    ),\n)\n</code></pre> <p>Or simply call <code>esm2_pretrain.py</code> directly. <pre><code># Enable fused attention in transformer engine for speed-up\nexport NVTE_FUSED_ATTN=1\nexport NVTE_FLASH_ATTN=0\n\nDATA_DIR=$(download_bionemo_data esm2/testdata_esm2_pretrain:2.0 --source ngc)\n\npython scripts/protein/esm2/esm2_pretrain.py \\\n    --train-cluster-path ${DATA_DIR}/2024_03_sanity/train_clusters_sanity.parquet \\\n    --train-database-path ${DATA_DIR}/2024_03_sanity/train_sanity.db \\\n    --valid-cluster-path ${DATA_DIR}/2024_03_sanity/valid_clusters.parquet \\\n    --valid-database-path ${DATA_DIR}/2024_03_sanity/validation.db \\\n    --precision=\"bf16-mixed\" \\\n    --num-gpus 1 \\\n    --num-nodes 1 \\\n    --num-steps 100 \\\n    --val-check-interval 25 \\\n    --max-seq-length 1024 \\\n    --limit-val-batches 2 \\\n    --micro-batch-size 2 \\\n    --num-layers 33 \\\n    --hidden-size 1280 \\\n    --num-attention-head 20 \\\n    --ffn-hidden-size 5120 \\\n    --tensor-model-parallel-size 1 \\\n    --create-tensorboard-logger \\\n    --wandb_project=__your_wandb_project__ \\\n    --experiment-name=__your_wandb_experiment_name\n</code></pre></p> <p>This script will automatically create <code>./results</code> and store the checkpoints under <code>esm2</code>. Automatic pretraining resumption is handled automatically when <code>--resume-if-exists</code> set to True, and <code>--restore-from-checkpoint-path</code> is available if users want to restore from a specific path.</p> <p>Weight And Biases</p> <p>If intend to use <code>--wandb_project</code>, users should log in Weight and Biases or alternatively export the environment variable <code>WANDB_API_KEY</code>. If not provided, the logger will be disabled.</p> <p>Non-critical Warnings from Command Line Runs</p> <p>Users might experience <code>torch._dynamo.convert_frame</code> warning messages and depreciation warning on <code>async_grad_allreduce</code> from Megatron-LM. Users can safely ignore them and is non-critical to pretraining.</p>"},{"location":"user-guide/examples/bionemo-esm2/pretrain/#recommended-pretraining-configuration","title":"Recommended Pretraining Configuration","text":"<p>We benchmark our implementation on the following model sizes<sup>1</sup>. These parameters are handled by</p> Model Size # Layers Hidden Size # Attention Heads FFN Hidden Size 8M 8 320 20 1280 650M 33 1280 20 5120 3B 36 2560 40 10240 15B 48 5120 40 20480 <p>In our current benchmark, we recommend the following trainiing and device configurations on A100 80GB GPUs to match with the published 2M token global batch size.</p> Model Size # GPUs Micro Batch Size Tensor Model Parallel Size 8M 32 64 1 650M 64 32 1 3B 128 16 1 15B 3120 2 2 <p>Additional Notes on Micro Batch Size</p> <p>While the above micro batch sizes are selected in 2^n to arrive at 2,097,152 tokens global batch size, users should observe performance boost by fitting the largest possible micro batch size onto the device without OOM. The currently largest batch sizes are listed below.</p> Model Size Max. micro batch size Tensor Model Parallel Size 8M 70 1 650M 48 1 3B 16 1 15B 3 2 <p>The only exception is 15B model where the authors reported 3.2M tokens global batch size. We arrived at 3,194,880 tokens on 390 A100 nodes.</p> <p>Maximum micro batch sizes for these model sizes are tested on 2 nodes of A100 80GB GPUs.</p> <p>Memory Allocation from Distributed Optimizer</p> <p>Distributed optimizer is enabled by default for improved memory allocation. Users might observe that the same micro batch size used on multi-device pretraining results in OOM on a single device. If additional optimization is necessary, we recommend running short benchmark on the same number of devices as in the production run.</p> <ol> <li> <p>Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, et al. \u201cEvolutionary-Scale Prediction of Atomic-Level Protein Structure with a Language Model.\u201d Science 379, no. 6637 (March 17, 2023): 1123\u201330. https://doi.org/10.1126/science.ade2574\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/","title":"Geneformer Cell Type Classification Benchmark","text":"In\u00a0[1]: Copied! <pre>cleanup:bool=True\n</pre> cleanup:bool=True In\u00a0[2]: Copied! <pre>#NBVAL_CHECK_OUTPUT\nimport cellxgene_census\nCENSUS_VERSION = \"2023-12-15\"\nwith cellxgene_census.open_soma(census_version=CENSUS_VERSION) as census:\n    adata = cellxgene_census.get_anndata(census, \"Homo sapiens\",\n            obs_value_filter='dataset_id==\"8e47ed12-c658-4252-b126-381df8d52a3d\"',\n        )\nuq_cells = sorted(adata.obs['cell_type'].unique().tolist())\nuq_cells\n</pre> #NBVAL_CHECK_OUTPUT import cellxgene_census CENSUS_VERSION = \"2023-12-15\" with cellxgene_census.open_soma(census_version=CENSUS_VERSION) as census:     adata = cellxgene_census.get_anndata(census, \"Homo sapiens\",             obs_value_filter='dataset_id==\"8e47ed12-c658-4252-b126-381df8d52a3d\"',         ) uq_cells = sorted(adata.obs['cell_type'].unique().tolist()) uq_cells Out[2]: <pre>['B cell',\n 'CD4-positive, alpha-beta T cell',\n 'CD8-positive, alpha-beta T cell',\n 'IgA plasma cell',\n 'IgG plasma cell',\n 'M cell of gut',\n 'T follicular helper cell',\n 'activated CD4-positive, alpha-beta T cell, human',\n 'conventional dendritic cell',\n 'dendritic cell, human',\n 'endothelial cell of artery',\n 'endothelial cell of lymphatic vessel',\n 'enterocyte',\n 'enteroendocrine cell',\n 'fibroblast',\n 'gamma-delta T cell',\n 'glial cell',\n 'intestinal crypt stem cell',\n 'intestinal tuft cell',\n 'intestine goblet cell',\n 'mast cell',\n 'memory B cell',\n 'monocyte',\n 'myeloid cell',\n 'myofibroblast cell',\n 'pericyte',\n 'plasma cell',\n 'plasmacytoid dendritic cell',\n 'regulatory T cell',\n 'transit amplifying cell',\n 'vein endothelial cell']</pre> In\u00a0[3]: Copied! <pre>#NBVAL_CHECK_OUTPUT\nimport random\nfrom contextlib import contextmanager\n\n@contextmanager\ndef random_seed(seed:int):\n    state = random.getstate()\n    random.seed(seed)\n    try:\n        yield\n    finally:\n        # Go back to previous state\n        random.setstate(state)\n\nwith random_seed(32):\n    indices = list(range(len(adata)))\n    random.shuffle(indices)\n\nmicro_batch_size:int = 32\nnum_steps:int = 256\nselection = sorted(indices[:micro_batch_size*num_steps])\n# NOTE: there's a current constraint that predict_step needs to be a function of micro-batch-size.\n#  this is something we are working on fixing. A quick hack is to set micro-batch-size=1, but this is\n#  slow. In this notebook we are going to use mbs=32 and subsample the anndata.\nadata = adata[selection].copy() # so it's not a view\nadata.shape\n</pre> #NBVAL_CHECK_OUTPUT import random from contextlib import contextmanager  @contextmanager def random_seed(seed:int):     state = random.getstate()     random.seed(seed)     try:         yield     finally:         # Go back to previous state         random.setstate(state)  with random_seed(32):     indices = list(range(len(adata)))     random.shuffle(indices)  micro_batch_size:int = 32 num_steps:int = 256 selection = sorted(indices[:micro_batch_size*num_steps]) # NOTE: there's a current constraint that predict_step needs to be a function of micro-batch-size. #  this is something we are working on fixing. A quick hack is to set micro-batch-size=1, but this is #  slow. In this notebook we are going to use mbs=32 and subsample the anndata. adata = adata[selection].copy() # so it's not a view adata.shape Out[3]: <pre>(8192, 60664)</pre> In\u00a0[4]: Copied! <pre>import shutil\nfrom bionemo.core import BIONEMO_CACHE_DIR\ncleanup:bool=True\nnotebook_workdir = BIONEMO_CACHE_DIR / \"notebook_tutorials\" / \"geneformer_celltype_classification\"\nif cleanup and notebook_workdir.exists():\n    shutil.rmtree(notebook_workdir)\nnotebook_workdir.mkdir(parents=True, exist_ok=True)\ndata_dir = notebook_workdir / \"celltype-bench-dataset\"\ndata_dir.mkdir(parents=True, exist_ok=True)\nh5ad_outfile = data_dir / \"hs-celltype-bench.h5ad\"\nadata.write_h5ad(h5ad_outfile)\n</pre> import shutil from bionemo.core import BIONEMO_CACHE_DIR cleanup:bool=True notebook_workdir = BIONEMO_CACHE_DIR / \"notebook_tutorials\" / \"geneformer_celltype_classification\" if cleanup and notebook_workdir.exists():     shutil.rmtree(notebook_workdir) notebook_workdir.mkdir(parents=True, exist_ok=True) data_dir = notebook_workdir / \"celltype-bench-dataset\" data_dir.mkdir(parents=True, exist_ok=True) h5ad_outfile = data_dir / \"hs-celltype-bench.h5ad\" adata.write_h5ad(h5ad_outfile) In\u00a0[5]: Copied! <pre>!sc_memmap --data-path {data_dir} --save-path {data_dir} --obs-cols cell_type --strict-metadata\n</pre> !sc_memmap --data-path {data_dir} --save-path {data_dir} --obs-cols cell_type --strict-metadata <pre>Found 1 files\nStarting to create memmap files...\nCreating metadata...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  4.52it/s]\nDone creating `metadata.json`\nWriting data into memmaps to /home/bionemo/.cache/bionemo/notebook_tutorials/geneformer_celltype_classification/celltype-bench-dataset...\nMerging AnnData into numpy memaps...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.57it/s]\nSaving dataframe ...\nDone creating dataset ...\n</pre> <p>Importantly, the .npy files are used by BioNeMo dataset object. features.csv contains the metadata requested, in this case cell_type. It's important that the output of our model has the same order as features.csv, as this contains the labels used in the following benchmark.</p> In\u00a0[6]: Copied! <pre>#NBVAL_CHECK_OUTPUT\nfrom glob import glob\nfiles = sorted([f.split(\"/\")[-1] for f in glob(str(data_dir/\"*\"))]) # strip off the directory name and sort for the test\nfiles\n</pre> #NBVAL_CHECK_OUTPUT from glob import glob files = sorted([f.split(\"/\")[-1] for f in glob(str(data_dir/\"*\"))]) # strip off the directory name and sort for the test files Out[6]: <pre>['features.csv',\n 'gene_expression_data.npy',\n 'gene_expression_ind.npy',\n 'gene_expression_ptr.npy',\n 'hs-celltype-bench.h5ad',\n 'metadata.json']</pre> In\u00a0[7]: Copied! <pre>from bionemo.testing.data.load import load\n# 106m checkpoint\ngeneformer_106m = load(\"geneformer/106M_240530:2.0\")\n# 10m checkpoint\ngeneformer_10m = load(\"geneformer/10M_240530:2.0\")\n</pre> from bionemo.testing.data.load import load # 106m checkpoint geneformer_106m = load(\"geneformer/106M_240530:2.0\") # 10m checkpoint geneformer_10m = load(\"geneformer/10M_240530:2.0\") In\u00a0[8]: Copied! <pre>result_path_10m = notebook_workdir / \"results_10m.pt\"\nresults_path_10m_random = notebook_workdir / \"results_10m_randomweights.pt\"\nresult_path_106m = notebook_workdir / \"results_106m.pt\"\n</pre> result_path_10m = notebook_workdir / \"results_10m.pt\" results_path_10m_random = notebook_workdir / \"results_10m_randomweights.pt\" result_path_106m = notebook_workdir / \"results_106m.pt\" In\u00a0[9]: Copied! <pre># NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped\n#  so set micro-batch-size=1 to make sure we get all results.\n!infer_geneformer --data-dir {data_dir} --checkpoint-path {geneformer_10m} --result-path {result_path_10m} --micro-batch-size {micro_batch_size} --seq-len 2048  --num-dataset-workers 10\n</pre> # NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped #  so set micro-batch-size=1 to make sure we get all results. !infer_geneformer --data-dir {data_dir} --checkpoint-path {geneformer_10m} --result-path {result_path_10m} --micro-batch-size {micro_batch_size} --seq-len 2048  --num-dataset-workers 10 <pre>[NeMo W 2024-11-01 16:34:09 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n[NeMo W 2024-11-01 16:34:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[NeMo W 2024-11-01 16:34:12 preprocess:101] Tokenizer vocab file: /home/bionemo/.cache/bionemo/7bd3714b68c30c50f0a36a02fa9dd720-singlecell-testdata-20240506.tar.gz.untar/cellxgene_2023-12-15_small/processed_data/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2024-11-01 16:34:12 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:12 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:12 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:12 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:12 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:12 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:12 infer_geneformer:77] *************** Preprocessing Finished ************\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-11-01 16:34:12 dataset:165] Feature ids are the same across datasets. This is good, using the same feature_ids for all datasets.\n[NeMo I 2024-11-01 16:34:12 megatron_strategy:287] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2024-11-01 16:34:12 megatron_init:314] Rank 0 has data parallel group : [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:328] Ranks 0 has data parallel rank: 0\n[NeMo I 2024-11-01 16:34:12 megatron_init:336] Rank 0 has context parallel group: [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:339] All context parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:340] Ranks 0 has context parallel rank: 0\n[NeMo I 2024-11-01 16:34:12 megatron_init:347] Rank 0 has model parallel group: [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:348] All model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:361] All tensor model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2024-11-01 16:34:12 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:394] Rank 0 has embedding group: [0]\n[NeMo I 2024-11-01 16:34:12 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2024-11-01 16:34:12 megatron_init:402] All embedding group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:12 megatron_init:403] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\nWARNING: Logging before flag parsing goes to stderr.\nW1101 16:34:13.065422 138832905740736 config.py:86] Loading /home/bionemo/.cache/bionemo/190b3edf22b4fc87745cd9d2deed1e66-geneformer_10M_240530_nemo2.tar.gz.untar\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-11-01 16:34:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n    \n[NeMo I 2024-11-01 16:34:14 base:44] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\n[NeMo W 2024-11-01 16:34:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:25: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n      warnings.warn(\n    \nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2024-11-01 16:34:14 megatron_parallel:404]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 10300032\nWriting output ['embeddings'] into /home/bionemo/.cache/bionemo/notebook_tutorials/geneformer_celltype_classification/results_10m.pt\n</pre> In\u00a0[10]: Copied! <pre># NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped\n#  so set micro-batch-size=1 to make sure we get all results.\n!infer_geneformer --data-dir {data_dir} --result-path {results_path_10m_random} --micro-batch-size {micro_batch_size} --seq-len 2048  --num-dataset-workers 10\n</pre> # NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped #  so set micro-batch-size=1 to make sure we get all results. !infer_geneformer --data-dir {data_dir} --result-path {results_path_10m_random} --micro-batch-size {micro_batch_size} --seq-len 2048  --num-dataset-workers 10 <pre>[NeMo W 2024-11-01 16:34:29 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n[NeMo W 2024-11-01 16:34:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[NeMo W 2024-11-01 16:34:32 preprocess:101] Tokenizer vocab file: /home/bionemo/.cache/bionemo/7bd3714b68c30c50f0a36a02fa9dd720-singlecell-testdata-20240506.tar.gz.untar/cellxgene_2023-12-15_small/processed_data/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2024-11-01 16:34:32 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:32 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:32 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:32 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:32 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:32 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:32 infer_geneformer:77] *************** Preprocessing Finished ************\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-11-01 16:34:32 dataset:165] Feature ids are the same across datasets. This is good, using the same feature_ids for all datasets.\n[NeMo I 2024-11-01 16:34:32 megatron_strategy:287] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2024-11-01 16:34:32 megatron_init:314] Rank 0 has data parallel group : [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:328] Ranks 0 has data parallel rank: 0\n[NeMo I 2024-11-01 16:34:32 megatron_init:336] Rank 0 has context parallel group: [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:339] All context parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:340] Ranks 0 has context parallel rank: 0\n[NeMo I 2024-11-01 16:34:32 megatron_init:347] Rank 0 has model parallel group: [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:348] All model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:361] All tensor model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2024-11-01 16:34:32 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:394] Rank 0 has embedding group: [0]\n[NeMo I 2024-11-01 16:34:32 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2024-11-01 16:34:32 megatron_init:402] All embedding group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:32 megatron_init:403] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\n[NeMo I 2024-11-01 16:34:32 base:44] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2024-11-01 16:34:32 megatron_parallel:404]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 10300032\nWriting output ['embeddings'] into /home/bionemo/.cache/bionemo/notebook_tutorials/geneformer_celltype_classification/results_10m_randomweights.pt\n</pre> In\u00a0[11]: Copied! <pre># NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped\n#  so set micro-batch-size=1 to make sure we get all results.\n!infer_geneformer --data-dir {data_dir} --checkpoint-path {geneformer_106m} --result-path {result_path_106m} --micro-batch-size {micro_batch_size} --seq-len 2048 --num-dataset-workers 10\n</pre> # NOTE: due to a an issue we are working on fixing, predict results have the last batch dropped #  so set micro-batch-size=1 to make sure we get all results. !infer_geneformer --data-dir {data_dir} --checkpoint-path {geneformer_106m} --result-path {result_path_106m} --micro-batch-size {micro_batch_size} --seq-len 2048 --num-dataset-workers 10 <pre>[NeMo W 2024-11-01 16:34:47 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n[NeMo W 2024-11-01 16:34:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n    \n[NeMo W 2024-11-01 16:34:49 preprocess:101] Tokenizer vocab file: /home/bionemo/.cache/bionemo/7bd3714b68c30c50f0a36a02fa9dd720-singlecell-testdata-20240506.tar.gz.untar/cellxgene_2023-12-15_small/processed_data/train/geneformer.vocab already exists. Overwriting...\n[NeMo I 2024-11-01 16:34:49 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:49 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_name_id_dict_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:49 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:49 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:49 remote:124] Resource already exists, skipping download: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl?download=true\n[NeMo I 2024-11-01 16:34:49 remote:136] No checksum provided, filename exists. Assuming it is complete.\n[NeMo I 2024-11-01 16:34:49 infer_geneformer:77] *************** Preprocessing Finished ************\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-11-01 16:34:49 dataset:165] Feature ids are the same across datasets. This is good, using the same feature_ids for all datasets.\n[NeMo I 2024-11-01 16:34:49 megatron_strategy:287] Fixing mis-match between ddp-config &amp; mcore-optimizer config\n[NeMo I 2024-11-01 16:34:49 megatron_init:314] Rank 0 has data parallel group : [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:328] Ranks 0 has data parallel rank: 0\n[NeMo I 2024-11-01 16:34:49 megatron_init:336] Rank 0 has context parallel group: [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:339] All context parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:340] Ranks 0 has context parallel rank: 0\n[NeMo I 2024-11-01 16:34:49 megatron_init:347] Rank 0 has model parallel group: [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:348] All model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:361] All tensor model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n[NeMo I 2024-11-01 16:34:49 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:394] Rank 0 has embedding group: [0]\n[NeMo I 2024-11-01 16:34:49 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n[NeMo I 2024-11-01 16:34:49 megatron_init:402] All embedding group ranks: [[0]]\n[NeMo I 2024-11-01 16:34:49 megatron_init:403] Rank 0 has embedding rank: 0\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 1 processes\n----------------------------------------------------------------------------------------------------\n\nWARNING: Logging before flag parsing goes to stderr.\nW1101 16:34:50.284130 124981140636096 config.py:86] Loading /home/bionemo/.cache/bionemo/aa591443f6d1d92d4d5e6d691c41b81a-geneformer_106M_240530_nemo2.tar.gz.untar\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[NeMo W 2024-11-01 16:34:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n    \n[NeMo I 2024-11-01 16:34:51 base:44] Padded vocab_size: 25472, original vocab_size: 25429, dummy tokens: 43.\n[NeMo W 2024-11-01 16:34:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:25: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n      warnings.warn(\n    \nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[NeMo I 2024-11-01 16:34:51 megatron_parallel:404]  &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 106808960\nWriting output ['embeddings'] into /home/bionemo/.cache/bionemo/notebook_tutorials/geneformer_celltype_classification/results_106m.pt\n</pre> In\u00a0[12]: Copied! <pre>def run_benchmark(data, labels, use_pca=True):\n    ''' \n    data - contains the single cell expression (or whatever feature) in each row.\n    labels - contains the string label for each cell\n    \n    data_shape (R, C)\n    labels_shape (R,)\n    '''\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.pipeline import Pipeline\n    from sklearn.model_selection import StratifiedKFold, cross_validate\n    from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n    from sklearn.decomposition import PCA\n    from sklearn.model_selection import cross_val_predict\n\n    np.random.seed(1337)\n    # Define the target dimension 'n_components'\n    n_components = 10  # for example, adjust based on your specific needs\n\n    # Create a pipeline that includes Gaussian random projection and RandomForestClassifier\n    if use_pca:\n        pipeline = Pipeline([\n            ('projection', PCA(n_components=n_components)),\n            ('classifier', RandomForestClassifier(class_weight='balanced'))\n        ])\n    else:\n        pipeline = Pipeline([\n            ('classifier', RandomForestClassifier(class_weight='balanced'))\n        ])\n\n    # Set up StratifiedKFold to ensure each fold reflects the overall distribution of labels\n    cv = StratifiedKFold(n_splits=5)\n\n    # Define the scoring functions\n    scoring = {\n        'accuracy': make_scorer(accuracy_score),\n        'precision': make_scorer(precision_score, average='macro'),  # 'macro' averages over classes\n        'recall': make_scorer(recall_score, average='macro'),\n        'f1_score': make_scorer(f1_score, average='macro'),\n        # 'roc_auc' requires probability or decision function; hence use multi_class if applicable\n        'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', needs_proba=True),\n    }\n\n    # Perform stratified cross-validation with multiple metrics using the pipeline\n    results = cross_validate(pipeline, data, labels, cv=cv, scoring=scoring, return_train_score=False)\n\n    # Print the cross-validation results\n    print(\"Cross-validation metrics:\")\n    results_out = {}\n    for metric, scores in results.items():\n        if metric.startswith('test_'):\n            results_out[metric] = (scores.mean(), scores.std())\n            print(f\"{metric[5:]}: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n    \n    predictions = cross_val_predict(pipeline, data, labels, cv=cv)\n\n    # Return confusion matrix and metrics.\n    conf_matrix = confusion_matrix(labels, predictions)\n    \n    return results_out, conf_matrix\n</pre> def run_benchmark(data, labels, use_pca=True):     '''      data - contains the single cell expression (or whatever feature) in each row.     labels - contains the string label for each cell          data_shape (R, C)     labels_shape (R,)     '''     import numpy as np     from sklearn.ensemble import RandomForestClassifier     from sklearn.pipeline import Pipeline     from sklearn.model_selection import StratifiedKFold, cross_validate     from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix     from sklearn.decomposition import PCA     from sklearn.model_selection import cross_val_predict      np.random.seed(1337)     # Define the target dimension 'n_components'     n_components = 10  # for example, adjust based on your specific needs      # Create a pipeline that includes Gaussian random projection and RandomForestClassifier     if use_pca:         pipeline = Pipeline([             ('projection', PCA(n_components=n_components)),             ('classifier', RandomForestClassifier(class_weight='balanced'))         ])     else:         pipeline = Pipeline([             ('classifier', RandomForestClassifier(class_weight='balanced'))         ])      # Set up StratifiedKFold to ensure each fold reflects the overall distribution of labels     cv = StratifiedKFold(n_splits=5)      # Define the scoring functions     scoring = {         'accuracy': make_scorer(accuracy_score),         'precision': make_scorer(precision_score, average='macro'),  # 'macro' averages over classes         'recall': make_scorer(recall_score, average='macro'),         'f1_score': make_scorer(f1_score, average='macro'),         # 'roc_auc' requires probability or decision function; hence use multi_class if applicable         'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', needs_proba=True),     }      # Perform stratified cross-validation with multiple metrics using the pipeline     results = cross_validate(pipeline, data, labels, cv=cv, scoring=scoring, return_train_score=False)      # Print the cross-validation results     print(\"Cross-validation metrics:\")     results_out = {}     for metric, scores in results.items():         if metric.startswith('test_'):             results_out[metric] = (scores.mean(), scores.std())             print(f\"{metric[5:]}: {scores.mean():.3f} (+/- {scores.std():.3f})\")          predictions = cross_val_predict(pipeline, data, labels, cv=cv)      # Return confusion matrix and metrics.     conf_matrix = confusion_matrix(labels, predictions)          return results_out, conf_matrix In\u00a0[13]: Copied! <pre>import torch\ninfer_Xs_10m = torch.load(result_path_10m)['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_10m), (len(adata), len(infer_Xs_10m))\ninfer_Xs_10m.shape\n</pre> import torch infer_Xs_10m = torch.load(result_path_10m)['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_10m), (len(adata), len(infer_Xs_10m)) infer_Xs_10m.shape Out[13]: <pre>(8192, 256)</pre> In\u00a0[14]: Copied! <pre>infer_Xs_106m = torch.load(result_path_106m)['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_106m), (len(adata), len(infer_Xs_106m))\ninfer_Xs_106m.shape\n</pre> infer_Xs_106m = torch.load(result_path_106m)['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_106m), (len(adata), len(infer_Xs_106m)) infer_Xs_106m.shape Out[14]: <pre>(8192, 768)</pre> In\u00a0[15]: Copied! <pre>import torch\ninfer_Xs_10m_random = torch.load(results_path_10m_random)['embeddings'].float().cpu().numpy()\nassert len(adata) == len(infer_Xs_10m_random), (len(adata), len(infer_Xs_10m_random))\ninfer_Xs_10m_random.shape\n</pre> import torch infer_Xs_10m_random = torch.load(results_path_10m_random)['embeddings'].float().cpu().numpy() assert len(adata) == len(infer_Xs_10m_random), (len(adata), len(infer_Xs_10m_random)) infer_Xs_10m_random.shape Out[15]: <pre>(8192, 256)</pre> In\u00a0[16]: Copied! <pre>import pandas as pd\nimport scanpy\nimport numpy as np\n# Now fetch the class labels and raw expression for the same dataset. These are used as labels in classification and as one of our baselines.\n\ninfer_metadata = pd.read_csv(data_dir/'features.csv')\nraw_Xs = np.asarray(adata.X.todense())\n# Here we perform a norm over the total counts for each cell, adding a pseudocount to assist with the following logarithm.\nnormed_Xs = (raw_Xs + 1) / raw_Xs.sum(axis=1, keepdims=True)\nlogp1_Xs = np.log( normed_Xs )\n</pre> import pandas as pd import scanpy import numpy as np # Now fetch the class labels and raw expression for the same dataset. These are used as labels in classification and as one of our baselines.  infer_metadata = pd.read_csv(data_dir/'features.csv') raw_Xs = np.asarray(adata.X.todense()) # Here we perform a norm over the total counts for each cell, adding a pseudocount to assist with the following logarithm. normed_Xs = (raw_Xs + 1) / raw_Xs.sum(axis=1, keepdims=True) logp1_Xs = np.log( normed_Xs ) In\u00a0[17]: Copied! <pre># Now we look at our dataset, how is the distribution of cell counts? Its clear that certain celltypes dominate the dataset, this is good to keep in mind when investigating models. \n#  we expect the macro averages and F1-score to be the most reliable metrics for overall performance.\nfrom collections import Counter\nimport seaborn as sb\n\nlabels = infer_metadata['cell_type'].values\nlabel_counts = Counter(labels)\n\nax = sb.barplot(x=label_counts.keys(), y=label_counts.values())\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nax.set_title(\"Cell type counts for classification dataset\")\n</pre> # Now we look at our dataset, how is the distribution of cell counts? Its clear that certain celltypes dominate the dataset, this is good to keep in mind when investigating models.  #  we expect the macro averages and F1-score to be the most reliable metrics for overall performance. from collections import Counter import seaborn as sb  labels = infer_metadata['cell_type'].values label_counts = Counter(labels)  ax = sb.barplot(x=label_counts.keys(), y=label_counts.values()) ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right') ax.set_title(\"Cell type counts for classification dataset\") <pre>/tmp/ipykernel_4819/2938980837.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n</pre> Out[17]: <pre>Text(0.5, 1.0, 'Cell type counts for classification dataset')</pre> In\u00a0[18]: Copied! <pre># Now we assign integer labels to each of our strings. These do not need to be transformed into one-hot vectors as Random Forest is non-parametric.\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ninteger_labels = label_encoder.fit_transform(labels)\nprint(integer_labels)\n</pre> # Now we assign integer labels to each of our strings. These do not need to be transformed into one-hot vectors as Random Forest is non-parametric. from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() integer_labels = label_encoder.fit_transform(labels) print(integer_labels)   <pre>[ 1  1 19 ... 17 14 14]\n</pre> In\u00a0[19]: Copied! <pre># Distribution of log transforms, looks decent.\nfrom matplotlib import pyplot\npyplot.hist(logp1_Xs.flatten());\n</pre> # Distribution of log transforms, looks decent. from matplotlib import pyplot pyplot.hist(logp1_Xs.flatten()); In\u00a0[20]: Copied! <pre>def plot_cm(cm, labels=label_encoder.classes_):\n    '''\n    Helper function for visualizing accuracy across labels.\n    '''\n    from matplotlib.colors import BoundaryNorm, ListedColormap\n\n    # Example confusion matrix (replace with your actual data)\n    conf_matrix = np.random.rand(31, 31)\n\n    # Define the bins and the color map\n    #bounds = np.arange(0.0, 1.1, 0.1)\n    #cmap = ListedColormap(sb.color_palette(\"RdYlBu_r\", len(bounds) - 1))\n    #norm = BoundaryNorm(boundaries=bounds, ncolors=len(bounds) - 1, clip=True)\n\n    #_ = sb.heatmap(cm / cm.sum(axis=0),cmap=cmap, norm=norm, cbar_kws={\"ticks\": bounds}, linewidths=0.5, linecolor='black', xticklabels=labels, yticklabels=labels)\n    _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)\n    pyplot.xticks(rotation=45, ha='right')\n    _ = pyplot.yticks(rotation=0)\n</pre> def plot_cm(cm, labels=label_encoder.classes_):     '''     Helper function for visualizing accuracy across labels.     '''     from matplotlib.colors import BoundaryNorm, ListedColormap      # Example confusion matrix (replace with your actual data)     conf_matrix = np.random.rand(31, 31)      # Define the bins and the color map     #bounds = np.arange(0.0, 1.1, 0.1)     #cmap = ListedColormap(sb.color_palette(\"RdYlBu_r\", len(bounds) - 1))     #norm = BoundaryNorm(boundaries=bounds, ncolors=len(bounds) - 1, clip=True)      #_ = sb.heatmap(cm / cm.sum(axis=0),cmap=cmap, norm=norm, cbar_kws={\"ticks\": bounds}, linewidths=0.5, linecolor='black', xticklabels=labels, yticklabels=labels)     _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)     pyplot.xticks(rotation=45, ha='right')     _ = pyplot.yticks(rotation=0) In\u00a0[21]: Copied! <pre>logp1_results, logp1_cm = run_benchmark(logp1_Xs, integer_labels)\n</pre> logp1_results, logp1_cm = run_benchmark(logp1_Xs, integer_labels) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.777 (+/- 0.035)\nprecision: 0.627 (+/- 0.052)\nrecall: 0.549 (+/- 0.019)\nf1_score: 0.561 (+/- 0.028)\nroc_auc: 0.971 (+/- 0.007)\n</pre> In\u00a0[22]: Copied! <pre>plot_cm(logp1_cm)\n</pre> plot_cm(logp1_cm) <pre>/tmp/ipykernel_4819/3742577664.py:16: RuntimeWarning: invalid value encountered in divide\n  _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)\n</pre> In\u00a0[23]: Copied! <pre>results_10m_random, cm_10m_random = run_benchmark(infer_Xs_10m_random, integer_labels, use_pca=False)\n</pre> results_10m_random, cm_10m_random = run_benchmark(infer_Xs_10m_random, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.395 (+/- 0.012)\nprecision: 0.142 (+/- 0.024)\nrecall: 0.091 (+/- 0.004)\nf1_score: 0.079 (+/- 0.004)\nroc_auc: 0.732 (+/- 0.015)\n</pre> In\u00a0[24]: Copied! <pre>plot_cm(cm_10m_random)\n</pre> plot_cm(cm_10m_random) <pre>/tmp/ipykernel_4819/3742577664.py:16: RuntimeWarning: invalid value encountered in divide\n  _ = sb.heatmap(cm / cm.sum(axis=0), cmap=sb.color_palette(\"Blues\", as_cmap=True), vmin=0, vmax=1, linewidth=0.1, linecolor='lightgrey', xticklabels=labels, yticklabels=labels)\n</pre> In\u00a0[25]: Copied! <pre>results_10m, cm_10m = run_benchmark(infer_Xs_10m, integer_labels, use_pca=False)\n</pre> results_10m, cm_10m = run_benchmark(infer_Xs_10m, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.838 (+/- 0.023)\nprecision: 0.802 (+/- 0.041)\nrecall: 0.687 (+/- 0.021)\nf1_score: 0.712 (+/- 0.022)\nroc_auc: 0.987 (+/- 0.007)\n</pre> In\u00a0[26]: Copied! <pre>plot_cm(cm_10m)\n</pre> plot_cm(cm_10m) In\u00a0[27]: Copied! <pre>results_106M, cm_106M = run_benchmark(infer_Xs_106m, integer_labels, use_pca=False)\n</pre> results_106M, cm_106M = run_benchmark(infer_Xs_106m, integer_labels, use_pca=False) <pre>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <pre>Cross-validation metrics:\naccuracy: 0.904 (+/- 0.018)\nprecision: 0.911 (+/- 0.027)\nrecall: 0.823 (+/- 0.016)\nf1_score: 0.845 (+/- 0.021)\nroc_auc: 0.993 (+/- 0.004)\n</pre> In\u00a0[28]: Copied! <pre>plot_cm(cm_106M)\n</pre> plot_cm(cm_106M) In\u00a0[29]: Copied! <pre>data = {\n    'model': [\n        'Baseline Logp1 PCA+RF', \n        '10M RandomWeights',\n        '10M parameters', \n        '106M parameters'],\n    'f1_score_mean': [\n        logp1_results['test_f1_score'][0],\n        results_10m_random['test_f1_score'][0],\n        results_10m['test_f1_score'][0],\n        results_106M['test_f1_score'][0]\n    ],\n    'f1_score_std': [\n        logp1_results['test_f1_score'][1],\n        results_10m_random['test_f1_score'][1],\n        results_10m['test_f1_score'][1],\n        results_106M['test_f1_score'][1]\n    ],\n    'accuracy_mean': [\n        logp1_results['test_accuracy'][0],\n        results_10m_random['test_accuracy'][0],\n        results_10m['test_accuracy'][0],\n        results_106M['test_accuracy'][0]\n    ],\n    'accuracy_std': [\n        logp1_results['test_accuracy'][1],\n        results_10m_random['test_accuracy'][1],\n        results_10m['test_accuracy'][1],\n        results_106M['test_accuracy'][1]\n    ]\n}\n\ndf = pd.DataFrame(data)\n\nfig, ax = pyplot.subplots(figsize=(10, 10))\n# F1 Score plot\nsb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax)\nax.set_title('F1 Score Comparison')\nax.set_xlabel('Model')\nax.set_ylabel('F1 Score')\nax.set_yticks(np.arange(.0, 1.05, .05))\nax.set_ylim(.0, 1.0)\npyplot.savefig(\"F1-score-models.png\")\n\n# Accuracy plot\nfig, ax = pyplot.subplots(figsize=(10, 10))\nsb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis')\nax.set_title('Accuracy Comparison')\nax.set_xlabel('Model')\nax.set_ylabel('Accuracy')\nax.set_yticks(np.arange(.0, 1.05, .05))\nax.set_ylim(.0, 1.0)\npyplot.savefig(\"average-accuracy-models.png\")\n</pre> data = {     'model': [         'Baseline Logp1 PCA+RF',          '10M RandomWeights',         '10M parameters',          '106M parameters'],     'f1_score_mean': [         logp1_results['test_f1_score'][0],         results_10m_random['test_f1_score'][0],         results_10m['test_f1_score'][0],         results_106M['test_f1_score'][0]     ],     'f1_score_std': [         logp1_results['test_f1_score'][1],         results_10m_random['test_f1_score'][1],         results_10m['test_f1_score'][1],         results_106M['test_f1_score'][1]     ],     'accuracy_mean': [         logp1_results['test_accuracy'][0],         results_10m_random['test_accuracy'][0],         results_10m['test_accuracy'][0],         results_106M['test_accuracy'][0]     ],     'accuracy_std': [         logp1_results['test_accuracy'][1],         results_10m_random['test_accuracy'][1],         results_10m['test_accuracy'][1],         results_106M['test_accuracy'][1]     ] }  df = pd.DataFrame(data)  fig, ax = pyplot.subplots(figsize=(10, 10)) # F1 Score plot sb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax) ax.set_title('F1 Score Comparison') ax.set_xlabel('Model') ax.set_ylabel('F1 Score') ax.set_yticks(np.arange(.0, 1.05, .05)) ax.set_ylim(.0, 1.0) pyplot.savefig(\"F1-score-models.png\")  # Accuracy plot fig, ax = pyplot.subplots(figsize=(10, 10)) sb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis') ax.set_title('Accuracy Comparison') ax.set_xlabel('Model') ax.set_ylabel('Accuracy') ax.set_yticks(np.arange(.0, 1.05, .05)) ax.set_ylim(.0, 1.0) pyplot.savefig(\"average-accuracy-models.png\") <pre>/tmp/ipykernel_4819/4175651120.py:37: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sb.barplot(x='model', y='f1_score_mean', data=df, capsize=0.2, palette='viridis', ax=ax)\n/tmp/ipykernel_4819/4175651120.py:47: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sb.barplot(x='model', y='accuracy_mean', data=df, ax=ax, capsize=0.2, palette='viridis')\n</pre>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#geneformer-cell-type-classification-benchmark","title":"Geneformer Cell Type Classification Benchmark\u00b6","text":"<p>Here we benchmark four models, with two baselines. These models are tasked with cell type classification, using the Chron\u2019s disease small intestine dataset from Elmentaite et al. (2020), Developmental Cell. This dataset contains approximately 22,500 single cells from both healthy children aged 4-13 and chidlren with Chron\u2019s disease. This dataset contains 31 unique cell types which we assume to be annotated accurately. This dataset was held out of our pre-training dataset as all diseased samples were removed.</p> <ul> <li>Baseline (1) scRNA workflow: this model uses PCA with 10 components and random forest on normalized and log transformed expression counts to produce a result.</li> <li>Baseline (2) geneformer with random weight initialization. Some performance can come from large random projections, but we want to do better than that.</li> <li>geneformer-10M + geneformer106M as described in the model cards.</li> </ul> <p>First, we download the dataset from czi that we are interested in, and then create the requisite sc_memmap dataset object.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#create-the-scmemmap-object-check-outputs","title":"Create the scmemmap object, check outputs\u00b6","text":""},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#execute-inference","title":"Execute inference\u00b6","text":"<p>We run inference on all there of our models, which are downloaded by <code>load(...)</code> function in a previous cell. We have a one-off inference script for geneformer that is installed as part of the <code>bionemo-geneformer</code> package. See the <code>pyproject.toml</code> in the source directory if you are curious or want to use this as a template to make your own inference scripts. This script should work for any <code>sc_memmap</code> converted geneformer dataset, and geneformer bionemo2 model checkpoint though.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#benchmarking","title":"Benchmarking\u00b6","text":"<p>see below the benchmarking snippet. We take in a datavector, and a set of labels. We optionally fit PCA and then a RF model inside cross validation. Metrics are using the <code>macro</code> (average over each class) for handling multi-class labels. Additionally, we return the confusion matrix for further investigation.</p>"},{"location":"user-guide/examples/bionemo-geneformer/geneformer-celltype-classification/#execute-benchmarks","title":"Execute benchmarks\u00b6","text":"<p>Finally we execute our benchmarks, and collect results and confusion matrix. You can see in the figures below, we plot the performance by cell type for each model (confusion matrix heatmap). Perhaps unsurprisingly, we see that the most frequent cell type (enterocyte) has the highest accuracy across all models. This suggests bias in the model due to unbalanced data, however, further investigation is beyond the scope of this tutorial. Furthermore, we see continually improved performance as we move through the models, from baselines, to our provided pretrained model.</p> <p>Perhaps most interesting is the 106M parameter model, which clearly outperforms all other models by all metrics, but especially by F1-score. This suggests that training larger models based on geneformer perform well, and that more work may be done.</p>"},{"location":"user-guide/examples/bionemo-scdl/example_notebook/","title":"Example notebook","text":"In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport pooch\nfrom torch.utils.data import DataLoader\n\nfrom bionemo.core import BIONEMO_CACHE_DIR\nfrom bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset\nfrom bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch\n</pre> import os import tempfile  import pooch from torch.utils.data import DataLoader  from bionemo.core import BIONEMO_CACHE_DIR from bionemo.scdl.io.single_cell_memmap_dataset import SingleCellMemMapDataset from bionemo.scdl.util.torch_dataloader_utils import collate_sparse_matrix_batch <p>First, copy the input data. This can be done by copying https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad to a directory named <code>hdf5s</code>.</p> In\u00a0[2]: Copied! <pre>input_data = pooch.retrieve(\n    'https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad',\n    path=BIONEMO_CACHE_DIR / \"hdf5s\",\n    known_hash='a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc')\n</pre> input_data = pooch.retrieve(     'https://datasets.cellxgene.cziscience.com/97e96fb1-8caf-4f08-9174-27308eabd4ea.h5ad',     path=BIONEMO_CACHE_DIR / \"hdf5s\",     known_hash='a0728e13a421bbcd6b2718e1d32f88d0d5c7cb92289331e3f14a59b7c513b3bc') In\u00a0[3]: Copied! <pre>#Create a SingleCellMemMapDataset\ndataset_temp_dir = tempfile.TemporaryDirectory()\ndataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")\n\ndata = SingleCellMemMapDataset(dataset_dir, input_data)\n</pre> #Create a SingleCellMemMapDataset dataset_temp_dir = tempfile.TemporaryDirectory() dataset_dir = os.path.join(dataset_temp_dir.name, \"97e_scmm\")  data = SingleCellMemMapDataset(dataset_dir, input_data) In\u00a0[4]: Copied! <pre>#Save the dataset to the disk. \ndata.save()\n</pre> #Save the dataset to the disk.  data.save() Out[4]: <pre>True</pre> In\u00a0[5]: Copied! <pre>#Reload the data\nreloaded_data = SingleCellMemMapDataset(dataset_dir)\n</pre> #Reload the data reloaded_data = SingleCellMemMapDataset(dataset_dir) <p>There are various numbers of columns per observation. However, for a batch size of 1 the data does not need to be collated. It will then be outputted in a torch tensor of shape (1, 2, num_obs) The first row of lengh num_obs contains the column pointers, and the second row contains the corresponding values.</p> In\u00a0[6]: Copied! <pre>model = lambda x : x\n\ndataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x : x  dataloader = DataLoader(data, batch_size=1, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch)  <pre>/usr/local/lib/python3.10/dist-packages/bionemo/scdl/util/torch_dataloader_utils.py:39: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n  batch_sparse_tensor = torch.sparse_csr_tensor(batch_rows, batch_cols, batch_values, size=(len(batch), max_pointer))\n</pre> <p>The data can be collated with a batch size of 1 and must be collated with larger batch sizes. This will collate several sparse matrices into the CSR (Compressed Sparse Row) torch tensor format.</p> In\u00a0[7]: Copied! <pre>model = lambda x : x\n\ndataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch)\nn_epochs = 1\nfor e in range(n_epochs):\n    for batch in dataloader:\n        model(batch)\n</pre> model = lambda x : x  dataloader = DataLoader(data, batch_size=8, shuffle=True, collate_fn=collate_sparse_matrix_batch) n_epochs = 1 for e in range(n_epochs):     for batch in dataloader:         model(batch)  <p>Alternatively, if there are multiple AnnData files, they can be converted into a single SingleCellMemMapDataset. If the hdf5 directory has one or more AnnData files, the SingleCellCollection class crawls the filesystem to recursively find AnnData files (with the h5ad extension). The code below is in scripts/convert_h5ad_to_scdl.py. It will create a new dataset at example_dataset. This can also be called with the convert_h5ad_to_scdl command.</p> In\u00a0[8]: Copied! <pre># path to dir holding hdf5s data\nhdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"\n\n# path to output dir where SCDataset will be stored\noutput_temp_dir = tempfile.TemporaryDirectory()\noutput_dir = os.path.join(output_temp_dir.name, 'scdataset_output')\n</pre> # path to dir holding hdf5s data hdf5s = BIONEMO_CACHE_DIR / \"hdf5s\"  # path to output dir where SCDataset will be stored output_temp_dir = tempfile.TemporaryDirectory() output_dir = os.path.join(output_temp_dir.name, 'scdataset_output') In\u00a0[9]: Copied! <pre>from bionemo.scdl.io.single_cell_collection import SingleCellCollection\nwith tempfile.TemporaryDirectory() as temp_dir:\n    coll = SingleCellCollection(temp_dir)\n    coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)\n    coll.flatten(output_dir, destroy_on_copy=True)\n</pre> from bionemo.scdl.io.single_cell_collection import SingleCellCollection with tempfile.TemporaryDirectory() as temp_dir:     coll = SingleCellCollection(temp_dir)     coll.load_h5ad_multi(hdf5s, max_workers=4, use_processes=True)     coll.flatten(output_dir, destroy_on_copy=True) In\u00a0[10]: Copied! <pre>dataset_temp_dir.cleanup()\noutput_temp_dir.cleanup()\n</pre> dataset_temp_dir.cleanup() output_temp_dir.cleanup()"},{"location":"user-guide/getting-started/","title":"Getting Started","text":""},{"location":"user-guide/getting-started/#repository-structure","title":"Repository structure","text":""},{"location":"user-guide/getting-started/#high-level-overview","title":"High level overview","text":"<p>This repository is structured as a meta-package that collects together many python packages. We designed in this way because this is how we expect our users to use bionemo, as a package that they themselves import and use in their own projects. By structuring code like this ourselves we ensure that bionemo developers follow similar patterns to our end users.</p> <p>Each model is stored in its own <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>sub-packages/bionemo-esm2</code>: ESM2 model</li> <li><code>sub-packages/bionemo-geneformer</code>: Geneformer</li> <li><code>sub-packages/bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight     megatron model that doesn't actually support any megatron parallelism, but should run fine as long as you only use     data parallelism to train.</li> </ul> <p>There are also useful utility packages, for example:</p> <ul> <li><code>sub-packages/bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by downstream     single-cell models in the bionemo package.</li> <li><code>sub-packages/bionemo-testing</code>: a suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>sub-packages/bionemo-core</code>: mostly just high level APIs</li> <li><code>sub-packages/bionemo-llm</code>: ABCs for code that multiple large language models (eg BERT variants) share.</li> </ul> <p>Documentation source is stored in <code>docs/</code></p> <p>The script for building a local docker container is <code>./launch.sh</code> which has some useful commands including:</p> <ul> <li><code>./launch.sh build</code> to build the container</li> <li><code>./launch.sh run</code> to get into a running container with reasonable settings for data/code mounts etc.</li> </ul>"},{"location":"user-guide/getting-started/#more-detailed-structure-notes","title":"More detailed structure notes","text":"<pre><code>$ tree -C -I \"*.pyc\" -I \"test_data\" -I \"test_experiment\" -I \"test_finettune_experiment\" -I __pycache__ -I \"*.egg-info\" -I lightning_logs -I results -I data -I MNIST* -I 3rdparty\n.\n\u251c\u2500\u2500 CODE-REVIEW.md -&gt; docs/CODE-REVIEW.md\n\u251c\u2500\u2500 CODEOWNERS\n\u251c\u2500\u2500 CONTRIBUTING.md -&gt; docs/CONTRIBUTING.md\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 LICENSE\n\u2502   \u251c\u2500\u2500 license.txt\n\u2502   \u2514\u2500\u2500 third_party.txt\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 VERSION\n\u251c\u2500\u2500 ci\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u251c\u2500\u2500 nightly_test.sh\n\u2502       \u251c\u2500\u2500 pr_test.sh\n\u2502       \u2514\u2500\u2500 static_checks.sh\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 docs\n\u2502   \u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 color-schemes.css\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 custom-material.css\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 fonts.css\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 images\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 favicon.png\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 logo-icon-black.svg\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 logo-white.svg\n\u2502   \u2502   \u251c\u2500\u2500 developer-guide\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CODE-REVIEW.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 CONTRIBUTING.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 jupyter-notebooks.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2502   \u2514\u2500\u2500 user-guide\n\u2502   \u2502       \u2514\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 mkdocs.yml\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 scripts\n\u2502       \u2514\u2500\u2500 gen_ref_pages.py\n\u251c\u2500\u2500 launch.sh\n\u251c\u2500\u2500 license_header\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 requirements-cve.txt\n\u251c\u2500\u2500 requirements-dev.txt\n\u251c\u2500\u2500 requirements-test.txt\n\u251c\u2500\u2500 scripts   # \ud83d\udfe2 Temporary scripts that demonstrate how to run some of these programs. These will be replaced.\n\u2502   \u251c\u2500\u2500 artifact_paths.yaml\n\u2502   \u251c\u2500\u2500 download_artifacts.py\n\u2502   \u251c\u2500\u2500 gpt-pretrain.py\n\u2502   \u251c\u2500\u2500 protein\n\u2502   \u2502   \u2514\u2500\u2500 esm2\n\u2502   \u2502       \u251c\u2500\u2500 esm2_pretrain.py\n\u2502   \u2502       \u2514\u2500\u2500 test_esm2_pretrain.py\n\u2502   \u2514\u2500\u2500 singlecell\n\u2502       \u2514\u2500\u2500 geneformer\n\u2502           \u251c\u2500\u2500 test_train.py\n\u2502           \u2514\u2500\u2500 train.py\n# \ud83d\udfe2 All work goes into `sub-packages`\n#  Sub-packages represent individually installable subsets of the bionemo codebase. We recommend that you\n#  create new sub-packages to track your experiments and save any updated models or utilities that you need.\n\u251c\u2500\u2500 sub-packages\n\u2502   \u251c\u2500\u2500 bionemo-core  # \ud83d\udfe2 bionemo-core, and bionemo-llm represent top level sub-packages that do not depend on others\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src  # \ud83d\udfe2 All sub-packages have a `src` and a `test` sub-directory.\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 core\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 batching_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 dtypes.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 random_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests  # \ud83d\udfe2 Test files should be mirrored with `src` files, and have the same name other than `test_[file_name].py`\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u251c\u2500\u2500 core\n\u2502   \u2502           \u2514\u2500\u2500 pytorch\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u2514\u2500\u2500 test_dtypes.py\n\u2502   \u251c\u2500\u2500 bionemo-esm2  # \ud83d\udfe2 The ESM2 model sub-package. This stores models and dataloaders necessary for pretraining and some example fine-tuning.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 esm2\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 model\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 attention.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 embedding.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 lr_scheduler.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 model.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 esm2\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 conftest.py\n\u2502   \u2502               \u2514\u2500\u2500 model\n\u2502   \u2502                   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_attention.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_embedding.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_lr_scheduler.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_model.py\n\u2502   \u251c\u2500\u2500 bionemo-example_model  # \ud83d\udfe2 a small example model that demonstrates how to write a megatron model from scratch and train on MNIST\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 example_model\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 lightning_basic.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 example_model\n\u2502   \u2502               \u2514\u2500\u2500 test_lightning_basic.py\n\u2502   \u251c\u2500\u2500 bionemo-fw  # \ud83d\udfe2 a meta-package that pulls together all other packages. A user can install this and get all of bionemo.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 fw\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 fw\n\u2502   \u2502               \u2514\u2500\u2500 test_sub_package_imports.py\n\u2502   \u251c\u2500\u2500 bionemo-geneformer  # \ud83d\udfe2 geneformer sub-module\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 geneformer\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 finetune_token_regressor.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 tokenizer\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 gene_tokenizer.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 label2id_tokenizer.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 geneformer\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 test_model.py\n\u2502   \u2502               \u251c\u2500\u2500 test_stop_and_go.py\n\u2502   \u2502               \u2514\u2500\u2500 test_transformer_specs.py\n\u2502   \u251c\u2500\u2500 bionemo-llm  # \ud83d\udfe2 shared model code for LLM style models, eg BERT variants, transformer variants, etc.\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements-test.txt\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 llm\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 model\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 lightning.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 model.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u251c\u2500\u2500 testing_utils.py\n\u2502   \u2502   \u2502           \u2502   \u2502   \u2514\u2500\u2500 transformer_specs.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 layers.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 loss.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 datamodule_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 iomixin_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 logger_utils.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 remote.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 weight_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 llm\n\u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502               \u251c\u2500\u2500 model\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 biobert\n\u2502   \u2502               \u2502   \u2502   \u2514\u2500\u2500 test_transformer_specs.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_loss.py\n\u2502   \u2502               \u251c\u2500\u2500 test_lightning.py\n\u2502   \u2502               \u2514\u2500\u2500 utils\n\u2502   \u2502                   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_datamodule_utils.py\n\u2502   \u2502                   \u251c\u2500\u2500 test_iomixin_utils.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_logger_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-scdl  # \ud83d\udfe2\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 examples\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 example_notebook.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 scdl\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 api\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_row_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 index\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 row_feature_index.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 io\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 single_cell_collection.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 single_cell_memmap_dataset.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 scripts\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 convert_h5ad_to_scdl.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 util\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502               \u251c\u2500\u2500 async_worker_queue.py\n\u2502   \u2502   \u2502               \u2514\u2500\u2500 torch_dataloader_utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 scdl\n\u2502   \u2502               \u251c\u2500\u2500 conftest.py\n\u2502   \u2502               \u251c\u2500\u2500 index\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_row_feature_index.py\n\u2502   \u2502               \u251c\u2500\u2500 io\n\u2502   \u2502               \u2502   \u251c\u2500\u2500 test_single_cell_collection.py\n\u2502   \u2502               \u2502   \u2514\u2500\u2500 test_single_cell_memmap_dataset.py\n\u2502   \u2502               \u2514\u2500\u2500 util\n\u2502   \u2502                   \u251c\u2500\u2500 test_async_worker_queue.py\n\u2502   \u2502                   \u2514\u2500\u2500 test_torch_dataloader_utils.py\n\u2502   \u251c\u2500\u2500 bionemo-testing\n\u2502   \u2502   \u251c\u2500\u2500 LICENSE\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 _requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 setup.py\n\u2502   \u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 bionemo\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 testing\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 callbacks.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 harnesses\n\u2502   \u2502   \u2502           \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502           \u2502   \u2514\u2500\u2500 stop_and_go.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 megatron_parallel_state_utils.py\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 testing_callbacks.py\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 utils.py\n\u2502   \u2502   \u2514\u2500\u2500 tests\n\u2502   \u2502       \u2514\u2500\u2500 bionemo\n\u2502   \u2502           \u2514\u2500\u2500 testing\n\u2502   \u2502               \u2514\u2500\u2500 test_megatron_parallel_state_utils.py\n\u2502   \u2514\u2500\u2500 bionemo-webdatamodule\n\u2502       \u251c\u2500\u2500 LICENSE\n\u2502       \u251c\u2500\u2500 README.md\n\u2502       \u251c\u2500\u2500 pyproject.toml\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u251c\u2500\u2500 setup.py\n\u2502       \u251c\u2500\u2500 src\n\u2502       \u2502   \u2514\u2500\u2500 bionemo\n\u2502       \u2502       \u2514\u2500\u2500 webdatamodule\n\u2502       \u2502           \u251c\u2500\u2500 __init__.py\n\u2502       \u2502           \u251c\u2500\u2500 datamodule.py\n\u2502       \u2502           \u2514\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 tests\n\u2502           \u2514\u2500\u2500 bionemo\n\u2502               \u2514\u2500\u2500 webdatamodule\n\u2502                   \u251c\u2500\u2500 __init__.py\n\u2502                   \u251c\u2500\u2500 conftest.py\n\u2502                   \u2514\u2500\u2500 test_datamodule.py\n</code></pre>"},{"location":"user-guide/getting-started/#installation","title":"Installation","text":""},{"location":"user-guide/getting-started/#initializing-3rd-party-dependencies-as-git-submodules","title":"Initializing 3rd-party dependencies as git submodules","text":"<p>For development, the NeMo and Megatron-LM dependencies are vendored in the bionemo-2 repository workspace as git submodules. The pinned commits for these submodules represent the \"last-known-good\" versions of these packages that are confirmed to be working with bionemo2 (and those that are tested in CI).</p> <p>To initialize these sub-modules when cloning the repo, add the <code>--recursive</code> flag to the git clone command:</p> <pre><code>git clone --recursive git@github.com:NVIDIA/bionemo-fw-ea.git\n</code></pre> <p>To download the pinned versions of these submodules within an existing git repository, run</p> <pre><code>git submodule update --init --recursive\n</code></pre>"},{"location":"user-guide/getting-started/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Hardware and Software Prerequisites</li> <li>Access and Startup</li> <li>Initialization Guide</li> <li>Development</li> </ul>"},{"location":"user-guide/getting-started/access-startup/","title":"Access and Startup","text":"<p>The BioNeMo Framework is free to use and easily accessible. The preferred method of accessing the software is through the BioNeMo Docker container, which provides a seamless and hassle-free way to develop and execute code. By using the Docker container, you can bypass the complexity of handling dependencies, ensuring that you have a consistent and reproducible environment for your projects.</p> <p>In this section of the documentation, we will guide you through the process of pulling the BioNeMo Docker container and setting up a local development environment. By following these steps, you will be able to quickly get started with the BioNeMo Framework and begin exploring its features and capabilities.</p>"},{"location":"user-guide/getting-started/access-startup/#access-the-bionemo-framework","title":"Access the BioNeMo Framework","text":"<p>To access the BioNeMo Framework container, you will need a free NVIDIA GPU Cloud (NGC) account and an API key linked to that account.</p>"},{"location":"user-guide/getting-started/access-startup/#ngc-account-and-api-key-configuration","title":"NGC Account and API Key Configuration","text":"<p>NGC is a portal of enterprise services, software, and support for artificial intelligence and high-performance computing (HPC) workloads. The BioNeMo Docker container is hosted on the NGC Container Registry. To pull and run a container from this registry, you will need to create a free NGC account and an API Key using the following steps:</p> <ol> <li>Create a free account on NGC and log in.</li> <li>At the top right, click on the User &gt; Setup &gt; Generate API Key, then click + Generate API Key and Confirm. Copy and store your API Key in a secure location.</li> </ol> <p>You can now view the BioNeMo Framework container at this direct link in the NGC Catalog or by searching the NGC Catalog for \u201cBioNeMo Framework\u201d. Feel free to explore the other resources available to you in the catalog.</p>"},{"location":"user-guide/getting-started/access-startup/#ngc-cli-configuration","title":"NGC CLI Configuration","text":"<p>The NGC Command Line Interface (CLI) is a command-line tool for managing resources in NGC, including datasets and model checkpoints. You can download the CLI on your local machine using the instructions on the NGC CLI website.</p> <p>Once you have installed the NGC CLI, run <code>ngc config set</code> at the command line to setup your NGC credentials:</p> <ul> <li>API key: Enter your API Key</li> <li>CLI output: Accept the default (ASCII format) by pressing <code>Enter</code></li> <li>org: Choose your preferred organization from the supplied list</li> <li>team: Choose the team to which you have been assigned from the supplied list</li> <li>ace : Choose an ACE, if applicable, otherwise press <code>Enter</code> to continue</li> </ul> <p>Note that the org and team are only relevant when pulling private containers/datasets from NGC created by you or your team. To access BioNeMo Framework, you can use the default value.</p>"},{"location":"user-guide/getting-started/access-startup/#startup-instructions","title":"Startup Instructions","text":"<p>BioNeMo is compatible with a wide variety of computing environments, including both local workstations, data centers, and Cloud Service Providers (CSPs) such as Amazon Web Services, Microsoft Azure, Google Cloud Platform, and Oracle Cloud Infrastructure, and NVIDIA\u2019s own DGX Cloud.</p>"},{"location":"user-guide/getting-started/access-startup/#running-the-container-on-a-local-machine","title":"Running the Container on a Local Machine","text":"<p>This section will provide instructions for running the BioNeMo Framework container on a local workstation. This process will involve the following steps:</p> <ol> <li>Logging into the NGC Container Registry (<code>nvcr.io</code>)</li> <li>Pulling the container from the registry</li> <li>Running a Jupyter Lab instance inside the container for local development</li> </ol>"},{"location":"user-guide/getting-started/access-startup/#pull-docker-container-from-ngc","title":"Pull Docker Container from NGC","text":"<p>Open a command prompt on your machine and enter the following:</p> <pre><code>docker login nvcr.io\n</code></pre> <p>This command will prompt you to enter your API key. Fill in the details as shown below. Note that you should enter the string <code>$oauthtoken</code> as your username. Replace the password (<code>&lt;YOUR_API_KEY&gt;</code>) with the API key that you generated in the NGC Account and API Key Configuration section above:</p> <pre><code>Username: $oauthtoken\nPassword: &lt;YOUR_API_KEY&gt;\n</code></pre> <p>You can now pull the BioNeMo Framework container using the following command:</p> <pre><code>docker pull nvcr.io/nvidia/clara/bionemo-framework:main--nightly\n</code></pre>"},{"location":"user-guide/getting-started/access-startup/#run-the-bionemo-framework-container","title":"Run the BioNeMo Framework Container","text":"<p>Now that you have pulled the BioNeMo Framework container, you can run it as you would a normal Docker container. For instance, to get basic shell access you can run the following command:</p> <pre><code>docker run --rm -it --gpus all \\\n  nvcr.io/nvidia/clara/bionemo-framework:main--nightly \\\n  /bin/bash\n</code></pre> <p>Because BioNeMo is distributed as a Docker container, standard arguments can be passed to the <code>docker run</code> command to alter the behavior of the container and its interactions with the host system. For more information on these arguments, refer to the Docker documentation.</p> <p>In the next section, Initialization Guide, we will present some useful <code>docker run</code> command variants for common workflows.</p>"},{"location":"user-guide/getting-started/access-startup/#running-on-any-major-csp-with-the-nvidia-gpu-optimized-vmi","title":"Running on Any Major CSP with the NVIDIA GPU-Optimized VMI","text":"<p>The BioNeMo Framework container is supported on cloud-based GPU instances through the NVIDIA GPU-Optimized Virtual Machine Image (VMI), available for AWS, GCP, Azure, and OCI. NVIDIA VMIs are built on Ubuntu and provide a standardized operating system environment across cloud infrastructure for running NVIDIA GPU-accelerated software. These images are pre-configured with software dependencies such as NVIDIA GPU drivers, Docker, and the NVIDIA Container Toolkit. More details about NVIDIA VMIs can be found in the NGC Catalog.</p> <p>The general steps for launching the BioNeMo Framework container using a CSP are:</p> <ol> <li>Launch a GPU-equipped instance running the NVIDIA GPU-Optimized VMI on your preferred CSP. Follow the instructions for     launching a GPU-equipped instance provided by your CSP.</li> <li>Connect to the running instance using SSH and run the BioNeMo Framework container exactly as outlined in the     Running the Container on a Local Machine section on     the Access and Startup page.</li> </ol>"},{"location":"user-guide/getting-started/development/","title":"Development with BioNeMo","text":"<p>On this page, we will cover the organization of the codebase and the setup necessary for the two primary development workflows for users of the BioNeMo Framework: training and fine-tuning models. For both of these workflows, we recommend setting the <code>NGC_CLI_API_KEY</code> environment variable as discussed in the Initialization Guide. This environment variable is required by the script that will be used to download both model checkpoints and data from NGC to be used in these workflows.</p>"},{"location":"user-guide/getting-started/development/#bionemo-code-overview","title":"BioNeMo Code Overview","text":"<p>The BioNeMo codebase is structured as a meta-package that collects together many Python packages. We designed BioNeMo this way with the expectation that users will import and use BioNeMo in their own projects. By structuring code this way, we ensure that BioNeMo developers follow similar patterns to those we expect of our end users.</p> <p>Each model is stored in its own subdirectory of <code>sub-packages</code>. Some examples of models include:</p> <ul> <li><code>bionemo-esm2</code>: The ESM-2 model</li> <li><code>bionemo-geneformer</code>: The Geneformer model</li> <li><code>bionemo-example_model</code>: A minimal example MNIST model that demonstrates how you can write a lightweight     Megatron model that does not actually support any megatron parallelism, but should run fine as long as you only use     data parallelism to train.</li> </ul> <p>We also include useful utility packages, for example:</p> <ul> <li><code>bionemo-scdl</code>: Single Cell Dataloader (SCDL) provides a dataset implementation that can be used by     downstream single-cell models in the bionemo package.</li> <li><code>bionemo-testing</code>: A suite of utilities that are useful in testing, think <code>torch.testing</code> or <code>np.testing</code>.</li> </ul> <p>Finally some of the packages represent common functions and abstract base classes that expose APIs that are useful for interacting with <code>NeMo2</code>. Some examples of these include:</p> <ul> <li><code>bionemo-core</code>: High-level APIs</li> <li><code>bionemo-llm</code>: Abstract base classes for code that multiple large language models (eg BERT variants) share.</li> </ul>"},{"location":"user-guide/getting-started/development/#package-structure","title":"Package Structure","text":"<p>Within each of the Bionemo packages, a consistent structure is employed to facilitate organization and maintainability. The following components are present in each package:</p> <ul> <li><code>pyproject.toml</code>: Defines package metadata, including version, package name, and executable scripts to be installed.</li> <li><code>src</code>: Contains all source code for the package. Each package features a top-level <code>bionemo</code> folder, which serves     as the primary namespace for imports. During the build process, all <code>bionemo/*</code> source files are combined into a     single package, with unique subdirectory names appended to the <code>bionemo</code> directory.</li> <li><code>tests</code>: Houses all package tests. The convention for test files is to locate them in the same path as the     corresponding <code>src</code> file, but within the <code>tests</code> directory, with a <code>test_</code> prefix added to the test file name. For     example, to test a module-level file <code>src/bionemo/my_module</code>, a test file <code>tests/bionemo/test_my_module.py</code> should     be created. Similarly, to test a specific file <code>src/bionemo/my_module/my_file.py</code>, the test file should be named     <code>tests/bionemo/my_module/test_my_file.py</code>. Running <code>py.test sub-packages/my_package</code> will execute all tests within     the <code>tests</code> directory.</li> <li><code>examples</code>: Some packages include an <code>examples</code> directory containing Jupyter Notebook (<code>.ipynb</code>) files, which are     aggregated into the main documentation.</li> <li><code>README.md</code>: The core package README file serves as the primary documentation for each sub-package when uploaded     to PyPI.</li> <li><code>LICENSE</code>: For consistency, all Bionemo packages should utilize the Apache-2.0 license. By contributing code to     BioNeMo, you acknowledge permission for the code to be re-released under an Apache v2 license.</li> </ul>"},{"location":"user-guide/getting-started/development/#model-training-process","title":"Model Training Process","text":"<p>The process for pretraining models from BioNeMo involves running scripts located in the <code>scripts</code> directory. Each script exposes a Command-Line Interface (CLI) that contains and documents the options available for that model.</p> <p>To pretrain a model, you need to run the corresponding script with the required parameters. For example, to pretrain the ESM-2 model, you would run the <code>esm2_pretrain.py</code> script located in <code>scripts/protein/esm2</code>. Similarly, to pretrain the Geneformer model, you would run the <code>train.py</code> script located in <code>scripts/singlecell/geneformer</code>.</p> <p>The scripts provide various options that can be customized for pretraining, such as:</p> <ul> <li>Data directories and paths</li> <li>Model checkpoint paths</li> <li>Experiment names for tracking</li> <li>Number of GPUs and nodes</li> <li>Validation check intervals</li> <li>Number of dataset workers</li> <li>Number of steps</li> <li>Sequence lengths</li> <li>Micro-batch sizes</li> <li>Limit on validation batches</li> </ul> <p>You can specify these options when running the script using command-line arguments. For each of the available scripts, you can use the <code>--help</code> option for an explanation of the available options for that model.</p> <p>For more information on pretraining a model, refer to the ESM-2 Pretraining Tutorial.</p>"},{"location":"user-guide/getting-started/development/#fine-tuning","title":"Fine-Tuning","text":"<p>The model fine-tuning process involves downloading the required model checkpoints using the <code>download_bionemo_data</code> script. This script takes in the model name and version as arguments, along with the data source, which can be either <code>ngc</code> (default) or <code>pbss</code> for NVIDIA employees.</p> <p>To view a list of available resources (both model checkpoints and datasets), you can use the following command:</p> <pre><code>download_bionemo_data --list-resources\n</code></pre>"},{"location":"user-guide/getting-started/development/#step-1-download-data-and-checkpoints","title":"Step 1: Download Data and Checkpoints","text":"<p>To download the data and checkpoints, use the following command:</p> <pre><code>export DATA_SOURCE=\"ngc\"\nMODEL_CKPT=$(download_bionemo_data &lt;model_name&gt;/&lt;checkpoint_name&gt;:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Replace <code>&lt;model_name&gt;</code> with the desired model (for example, <code>esm2</code> or <code>geneformer</code>), <code>&lt;version&gt;</code> with the desired version, and <code>&lt;checkpoint_name&gt;</code> with the desired checkpoint name.</p> <p>Additionally, you can download available datasets from NGC using the following command, making similar substitutions as with the model checkpoint download command above:</p> <pre><code>TEST_DATA_DIR=$(download_bionemo_data &lt;model_name&gt;/testdata:&lt;version&gt; --source $DATA_SOURCE);\n</code></pre> <p>Alternatively, you can use your own data by configuring your container run with volume mounts as discussed in the Initialization Guide.</p>"},{"location":"user-guide/getting-started/development/#step-2-adapt-the-training-process","title":"Step 2: Adapt the Training Process","text":"<p>Fine-tuning may involve specifying a different combination of model and loss than was used to train the initial version of the model. The fine-tuning steps will be application-specific, but a general set of steps include:</p> <ol> <li>Prepare your dataset: Collect and prepare your dataset, including the sequence data and target values. This step is     crucial to ensure that your dataset is in a format that can be used for training.</li> <li>Create a custom dataset class: Define a custom dataset class that can handle your specific data format. This class should     be able to initialize the dataset and retrieve individual data points.</li> <li>Create a datamodule: Define a datamodule that prepares the data for training. This includes tasks such as data loading,     tokenization, and batching.</li> <li>Fine-tune the model: Use a pre-trained model as a starting point and fine-tune it on your dataset. This involves     adjusting the model's parameters to fit your specific task and dataset.</li> <li>Configure the fine-tuning: Set various hyperparameters for the fine-tuning process, such as the batch size, number of     training steps, and learning rate. These hyperparameters can significantly affect the performance of the fine-tuned     model.</li> <li>Run inference: Once the model is fine-tuned, use it to make predictions on new, unseen data.</li> </ol> <p>For more information on fine-tuning a model, refer to the ESM-2 Fine-tuning Tutorial.</p>"},{"location":"user-guide/getting-started/development/#advanced-developer-documentation","title":"Advanced Developer Documentation","text":"<p>For advanced development information (for example, developing the source code of BioNeMo), refer to the README found on the main page of the BioNeMo GitHub Repository.</p>"},{"location":"user-guide/getting-started/initialization-guide/","title":"Initialization Guide","text":"<p>Note</p> <p>Prior to beginning this section, you must confirm that your computing platform meets or exceeds the prerequisites outlined in the Hardware and Software Prerequisites page and that you have already pulled and verified that you can run the BioNeMo container as outlined in the Access and Startup page.</p> <p>At this point, you have successfully launched and run the Docker container. This section will guide you through setting up your host machine environment, suggest Docker commands for various common workflows, and explain helpful <code>docker run</code> options.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-up-your-host-machine-environment","title":"Setting Up Your Host Machine Environment","text":"<p>To effectively use the BioNeMo Framework, we recommend an organized environment configuration and directory structure. Specifically, we recommend having several cache directories per project. These directories will contain project files such as data, model checkpoints, training scripts, and outputs such as logs and predictions. To facilitate container set up, we recommend storing the paths to these directories in a <code>.env</code> file that can be referenced at container runtime. Below, we suggest useful environment variables to define in this file.</p>"},{"location":"user-guide/getting-started/initialization-guide/#creating-a-env-file-for-first-time-setup","title":"Creating a .env File For First Time Setup","text":"<p>We recommend using a <code>.env</code> file in your local workspace to define environment variables. Specifically, the following variables are useful to include in your <code>.env</code> file:</p> <pre><code># Local Cache Directories\nLOCAL_RESULTS_PATH\nDOCKER_RESULTS_PATH\nLOCAL_DATA_PATH\nDOCKER_DATA_PATH\nLOCAL_MODELS_PATH\nDOCKER_MODELS_PATH\n\n# Desired Jupyter Port\nJUPYTER_PORT\n\n# NGC Configuration Settings\nNGC_CLI_API_KEY\nNGC_CLI_ORG\nNGC_CLI_TEAM\nNGC_CLI_FORMAT_TYPE\n\n# Weights and Biases API Key\nWANDB_API_KEY\n</code></pre> <p>For each of these variables, you can define them inside the <code>.env</code> file using <code>=</code>. For example, you can set the NGC API key using <code>NGC_CLI_API_KEY=&lt;your API key here&gt;</code>. You can then define these variables in your current shell using:</p> <pre><code>source .env\n</code></pre> <p>Running this command will make these variables available for use in the <code>docker run</code> command examples shown below.</p> <p>NGC Credentials Required for Data Download</p> <p>Some of the credentials in the above <code>.env</code> file are optional for specific workflows. However, if you intend to use data hosted on the NGC platform (for example, model checkpoints and example training data), you must define both NGC_CLI_API_KEY and NGC_CLI_ORG at container run time. The easiest way to ensure these variables are set is to use the <code>.env</code> file as shown here with your specific variable definitions.</p> <p>Refer to the list below for an explanation of each of these variables:</p> <ul> <li><code>LOCAL_RESULTS_PATH</code> and <code>DOCKER_RESULTS_PATH</code>: Paths for storing results, with <code>LOCAL</code> referring to the path on the     local machine and <code>DOCKER</code> referring to the path inside the Docker container.</li> <li><code>LOCAL_DATA_PATH</code> and <code>DOCKER_DATA_PATH</code>: Paths for storing data, again with <code>LOCAL</code> and <code>DOCKER</code> distinctions.</li> <li><code>LOCAL_MODELS_PATH</code> and <code>DOCKER_MODELS_PATH</code>: Paths for storing machine learning models, with the same local and     Docker differences.</li> <li><code>JUPYTER_PORT</code>: The port number for a Jupyter Lab server, default port is 8888.</li> <li><code>NGC_CLI_API_KEY</code>, <code>NGC_CLI_ORG</code>, <code>NGC_CLI_TEAM</code>, and <code>NGC_CLI_FORMAT_TYPE</code>: API key, organization, team, and format     type for the NVIDIA GPU Cloud (NGC) command-line interface (CLI).</li> <li><code>WANDB_API_KEY</code>: An API key for Weights and Biases (W&amp;B), a platform for machine learning experiment tracking and     visualization.</li> </ul> Weights and Biases Setup (WANDB_API_KEY, Optional) <p>Weights and Biases (W&amp;B) is a machine learning operations platform that provides tools and services to help machine learning practitioners build, train, and deploy models more efficiently. BioNeMo is built to work with W&amp;B and requires only simple setup steps to start tracking your experiments. To set up W&amp;B inside your container, follow the steps below:</p> <ol> <li>Sign up for an account at Weights and Biases.</li> <li>Setup your API Key with W&amp;B.</li> <li>Set the <code>WANDB_API_KEY</code> variable in your <code>.env</code> in the same way as you set the previous environment variable     above.</li> <li>Set the environment variable inside your container using the <code>-e</code> option, as shown in the next section.</li> </ol>"},{"location":"user-guide/getting-started/initialization-guide/#starting-the-bionemo-container-for-common-workflows","title":"Starting the BioNeMo Container for Common Workflows","text":"<p>Below we describe some common BioNeMo workflows, including how to setup and run the container in each case. Each of the following examples will assume that you have local workspace directories as defined in your <code>.env</code> file shown above that you will attach to the container via volume mounts.</p>"},{"location":"user-guide/getting-started/initialization-guide/#starting-a-shell-inside-the-container","title":"Starting a Shell Inside the Container","text":"<p>With a shell inside the BioNeMo Docker container, you can execute commands, edit files, and run applications as if you were working directly on the host machine. This self-contained environment allows you to work with your project's dependencies and configurations in isolation, ensuring consistent results and reproducibility. You can install packages, test and debug applications, and customize the environment to suit your needs.</p> <p>You can launch a Bash shell inside the BioNeMo container using the command below. Note that any files modified in the mounted directories while inside the container will persist on the host machine, but other modifications (such as installed software) will not.</p> <pre><code>docker run \\\n  --rm -it \\\n  --gpus all \\\n  --network host \\\n  --shm-size=4g \\\n  -e WANDB_API_KEY \\\n  -e NGC_CLI_API_KEY \\\n  -e NGC_CLI_ORG \\\n  -e NGC_CLI_TEAM \\\n  -e NGC_CLI_FORMAT_TYPE \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:main--nightly \\\n  /bin/bash\n</code></pre> <ul> <li><code>--rm</code>: Removes the container when it exits.</li> <li><code>-it</code>: Allocates a pseudo-TTY and keeps the container running in the foreground.</li> <li><code>--gpus all</code>: Allocates all available GPUs on the host machine.</li> <li><code>--network host</code>: Allows the container to use the host's network stack, effectively sharing the host's network     namespace and allowing the container to access the host's network interfaces directly.</li> <li><code>--shm-size=4g</code>: Sets the size of the shared memory (/dev/shm) in the container to 4 gigabytes, which can be useful for applications that rely heavily on shared memory.</li> <li><code>-e &lt;VARIABLE&gt;</code>: Sets the environment variable inside the container, taking the value set on the host machine.</li> <li><code>-v &lt;LOCAL DIRECTORY&gt;:&lt;DOCKER DIRECTORY&gt;</code>: Mounts a volume from the host machine to the container.</li> <li><code>nvcr.io/nvidia/clara/bionemo-framework:main--nightly</code>: The path to the Docker image to use.</li> <li><code>/bin/bash</code>: The command to run inside the container, which starts a Bash shell.</li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#running-a-model-training-script-inside-the-container","title":"Running a Model Training Script Inside the Container","text":"<p>Running a model training script inside the BioNeMo Docker container is the preferred workflow for model training. The container provides an encapsulated and reproducible training environment. By mounting a volume from the host machine, the output directory containing results such as logs and checkpoints can be persisted even after the container is removed. A training script can be run as in the example below. Replace <code>training.py</code> and option (for example, <code>--option1</code>) with the file name and relevant command line options, respectively.</p> <pre><code>docker run --rm -it --gpus all \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:main--nightly \\\n  python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH\n</code></pre> <p>Many of the Docker run options are identical to the shell example above, with the exception of the command being run:</p> <ul> <li><code>python $DOCKER_RESULTS_PATH/training.py --option1 --option2 --output=$DOCKER_RESULTS_PATH</code>: The command to run inside the container, which runs the <code>training.py</code> Python script with the specified command-line arguments.</li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#running-jupyter-lab-inside-the-container","title":"Running Jupyter Lab Inside the Container","text":"<p>By starting a Jupyter Lab instance inside the BioNeMo Framework container, users can leverage the container's optimized environment for machine learning workloads to accelerate their data science workflows, while also benefiting from the interactive and collaborative features of Jupyter Lab. This allows users to seamlessly transition between data preparation, model development, and visualization, all within a single, streamlined environment. You can then launch the container. We recommend running the container in a Jupyter Lab environment using the command below:</p> <pre><code>docker run --rm -d --gpus all \\\n  -p $JUPYTER_PORT:$JUPYTER_PORT \\\n  -e NGC_CLI_API_KEY \\\n  -e WANDB_API_KEY \\\n  -v $LOCAL_DATA_PATH:$DOCKER_DATA_PATH \\\n  -v $LOCAL_MODELS_PATH:$DOCKER_MODELS_PATH \\\n  -v $LOCAL_RESULTS_PATH:$DOCKER_RESULTS_PATH \\\n  nvcr.io/nvidia/clara/bionemo-framework:main--nightly \\\n  \"jupyter lab \\\n    --allow-root \\\n    --ip=* \\\n    --port=$JUPYTER_PORT \\\n    --no-browser \\\n    --NotebookApp.token='' \\\n    --NotebookApp.allow_origin='*' \\\n    --ContentsManager.allow_hidden=True \\\n    --notebook-dir=$DOCKER_RESULTS_PATH\"\n</code></pre> <p>Refer to the guide below for an explanation of the recommended Jupyter Lab options:</p> <ul> <li><code>\"jupyter lab ...\"</code>: The command to run inside the container, which starts a Jupyter Lab server. The options are:<ul> <li><code>--allow-root</code>: Allow the Jupyter Lab server to run as the root user.</li> <li><code>--ip=*</code>: Listen on all available network interfaces, which allows access from outside the container.</li> <li><code>--port=$JUPYTER_PORT</code>: Listen on port 8888.</li> <li><code>--no-browser</code>: Do not open a browser window automatically.</li> <li><code>--NotebookApp.token=''</code>: Set an empty token for the Jupyter Lab server (no authentication is required).</li> <li><code>--NotebookApp.allow_origin='*'</code>: Allow requests from any origin.</li> <li><code>--ContentsManager.allow_hidden=True</code>: Allow the contents manager to access hidden files and directories.</li> <li><code>--notebook-dir=$DOCKER_RESULTS_PATH</code>: Set the notebook directory to     <code>$DOCKER_RESULTS_PATH</code> inside the container.</li> </ul> </li> </ul>"},{"location":"user-guide/getting-started/initialization-guide/#common-docker-run-options","title":"Common <code>docker run</code> Options","text":"<p>Below we explain some common <code>docker run</code> options and how to use them as part of your BioNeMo development workflows.</p>"},{"location":"user-guide/getting-started/initialization-guide/#mounting-volumes-with-the-v-option","title":"Mounting Volumes with the <code>-v</code> Option","text":"<p>The <code>-v</code>  allows you to mount a host machine's directory as a volume inside the container. This enables data persistence even after the container is deleted or restarted. In the context of machine learning workflows, leveraging the <code>-v</code> option is essential for maintaining a local cache of datasets, model weights, and results on the host machine such that they can persist after the container terminates and be reused across container runs.</p> <p>Syntax:</p> <p><pre><code>docker run -v &lt;host_directory&gt;:&lt;container_directory&gt; &lt;image_name&gt;\n</code></pre> Example:</p> <pre><code>docker run -v /path/to/local/cache:/workspace/bionemo2/cache \\\n    nvcr.io/nvidia/clara/bionemo-framework:main--nightly\n</code></pre> <p>In this example, the <code>/path/to/local/cache</code> directory on the host machine is mounted as a volume at <code>/workspace/bionemo2/cache</code> inside the container.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-environment-variables-with-the-e-option","title":"Setting Environment Variables with the <code>-e</code> Option","text":"<p>The <code>-e</code> option allows you to set environment variables inside the container. You can use this option to define variables that will be available to the application running inside the container.</p> <p>Example:</p> <pre><code>docker run -e MY_VAR=value -e ANOTHER_VAR=another_value \\\n    nvcr.io/nvidia/clara/bionemo-framework:main--nightly\n</code></pre> <ul> <li><code>-e MY_VAR=value</code> sets the <code>MY_VAR</code> environment variable to <code>value</code> inside the container.</li> <li><code>-e ANOTHER_VAR=another_value</code> sets the <code>ANOTHER_VAR</code> environment variable to <code>another_value</code> inside the container.</li> </ul> <p>You can set multiple environment variables by repeating the <code>-e</code> option. The values of these variables will be available to the application running inside the container, allowing you to customize its behavior.</p> <p>Note that you can also use shell variables and command substitutions to set environment variables dynamically. For example:</p> <pre><code>MY_EXTERNAL_VAR=external_value\ndocker run -e MY_INTERNAL_VAR=$MY_EXTERNAL_VAR \\\n    nvcr.io/nvidia/clara/bionemo-framework:main--nightly\n</code></pre> <p>In this example, the <code>MY_INTERNAL_VAR</code> environment variable inside the container will be set to the value of the <code>MY_EXTERNAL_VAR</code> shell variable on the host machine.</p>"},{"location":"user-guide/getting-started/initialization-guide/#setting-user-and-group-ids-with-the-u-option","title":"Setting User and Group IDs with the <code>-u</code> Option","text":"<p>The <code>-u</code> option sets the user and group IDs to use for the container process. By matching the IDs of the user on the host machine, the user inside the container will have identical permissions for reading and writing files in the mounted volumes as the user that ran the command. You can use command substitutions to automatically retrieve your user and group IDs.</p> <p>Example:</p> <pre><code>docker run -u $(id -u):$(id -g) \\\n    nvcr.io/nvidia/clara/bionemo-framework:main--nightly\n</code></pre> <ul> <li><code>$(id -u)</code> is a command substitution that executes the id -u command and captures its output. <code>id -u</code> prints the     effective user ID of the current user.</li> <li><code>$(id -g)</code> is another command substitution that executes the <code>id -g</code> command and captures its output. <code>id -g</code> prints     the effective group ID of the current user.</li> </ul>"},{"location":"user-guide/getting-started/pre-reqs/","title":"Hardware and Software Prerequisites","text":"<p>Before you begin using the BioNeMo framework, please ensure the hardware and software prerequisites outlined below are met.</p>"},{"location":"user-guide/getting-started/pre-reqs/#hardware","title":"Hardware","text":"<p>The BioNeMo Framework is compatible with environments that have access to NVIDIA GPUs. <code>bfloat16</code> precision requires an Ampere generation GPU or higher (Compute Capability \u22658.0). You may be able to run BioNeMo on GPUs without <code>bfloat16</code>, but this use-case is not supported by the development team.</p>"},{"location":"user-guide/getting-started/pre-reqs/#gpu-support-matrix","title":"GPU Support Matrix","text":"<p>The following datacenter and desktop GPUs have Compute Capability \u22658.0 and are supported hardware for BioNeMo:</p> GPU Compute Capability Support H100 9.0 Full L4 8.9 Full L40 8.9 Full A100 8.0 Full A40 8.6 Full A30 8.0 Full A10 8.6 Full A16 8.6 Full A2 8.6 Full RTX 6000 8.9 Full RTX A6000 8.6 Full RTX A5000 8.6 Full RTX A4000 8.6 Full"},{"location":"user-guide/getting-started/pre-reqs/#software","title":"Software","text":"<p>The BioNeMo Framework is supported on x86 Linux systems.</p> <p>Please ensure that the following are installed in your desired execution environment:</p> <ul> <li>Appropriate GPU drivers (minimum version: 535)</li> <li>Docker (with GPU support, Docker Engine 19.03 or above)</li> <li>NVIDIA Container Toolkit to allow Docker to access the GPUs</li> </ul>"}]}