# Build image, note the -devel tag which gives us nvcc and other build-time tools. In this image we'll install uv and
# create a python virtual environment containing all our dependencies and an installed version of our bionemo library.
# (Later, we can split that final step out into a separate image for an easier devcontainer setup.)
FROM nvcr.io/nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04 AS build

# for pushd/popd, etc.
SHELL ["/bin/bash", "-exc"]

# Build-time dependencies. We install and link against the ubuntu-provided version of python rather than uv, since it's
# easier to get a consistent version of python in the runtime image without needing to install UV there as well.
RUN <<EOT
apt-get update -qy
apt-get install -qyy \
    -o APT::Install-Recommends=false \
    -o APT::Install-Suggests=false \
    build-essential \
    ca-certificates \
    curl \
    software-properties-common \
    git \
    ninja-build \
    cmake \
    openmpi-bin \
    libopenmpi-dev \

add-apt-repository ppa:deadsnakes/ppa
apt-get update -qy
apt-get install -qyy \
    python3.10 \
    python3.10-dev
EOT

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# - Silence uv complaining about not being able to use hard links,
# - tell uv to byte-compile packages for faster application startups,
# - prevent uv from accidentally downloading isolated Python builds,
# - pick a Python,
# - and finally declare `/app` as the target for `uv sync`.
ENV UV_LINK_MODE=copy \
    UV_COMPILE_BYTECODE=1 \
    UV_PYTHON=python3.10 \
    UV_PYTHON_DOWNLOADS=never \
    UV_PROJECT_ENVIRONMENT=/bionemo

COPY uv.lock /_lock/
COPY pyproject.toml /_lock/

# If we don't use a `package=` option, we only install top-level dependencies, i.e., we don't install everything we'll
# need recursively. Hopefully https://github.com/astral-sh/uv/issues/6935 will lead to a cleaner way of installing all
# non-package dependencies before having to copy in all our source code. Once we copy in our source code, we're
# essentially giving up on these dependency installs being cached between builds.
#
# We have to do this in two steps in order to ensure that `flash-attn` (and later, any other packages with build-time
# requirements) are installed after we have created a base environment with torch & numpy.
RUN --mount=type=cache,target=/root/.cache <<EOT
cd /_lock
uv sync \
    --frozen \
    --no-install-workspace \
    --no-dev -v \
    --package bionemo-meta

uv sync \
    --frozen \
    --no-install-workspace \
    --no-dev -v \
    --package bionemo-meta \
    --extra build
EOT

# TODO: figure out if & how we can include apex and transformer-engine in the uv build system. `uv pip install` works,
# but including them in the uv.lock file (with these build-time arguments) doesn't seem to work.
RUN --mount=type=cache,target=/root/.cache <<EOT
uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps \
    git+https://github.com/NVIDIA/apex.git@810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c \
    -v --no-build-isolation \
    --config-settings "\
    --build-option=--cpp_ext \
    --cuda_ext \
    --fast_layer_norm \
    --distributed_adam \
    --deprecated_fused_adam \
    --group_norm \
    "
EOT

# Bumping transformer-engine to v1.9, I got compile-time errors with v1.7 that seemed to be resolved in more recent
# versions.
RUN --mount=type=cache,target=/root/.cache <<EOT
NVTE_FRAMEWORK=pytorch NVTE_WITH_USERBUFFERS=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi/ \
uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps \
    git+https://github.com/NVIDIA/TransformerEngine.git@v1.9 \
    -v --no-build-isolation
EOT

# Just copy over the 3rdparty/ and sub-packages/ directories to limit the radius of file changes that trigger a cache
# miss. Then we install these (in a non-editable way) into the virtual environment without dependencies (since these
# were installed above).
#
# TODO: to set up the devcontainer, we'll need to break this into a separate image
# (pre-build/build) so that we can install the packages as editable from the workspace.
COPY 3rdparty /src/3rdparty/
COPY sub-packages /src/sub-packages/
RUN --mount=type=cache,target=/root/.cache <<EOT
for sub in /src/3rdparty/* /src/sub-packages/bionemo-*;
do pushd ${sub} &&
uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps . &&
popd;
done
EOT


FROM nvcr.io/nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04 AS release
# Note the -release tag, which gives us a smaller image without the build-time tools.

SHELL ["/bin/bash", "-exc"]

# Make sure the eventual virtualenv is in the runtim path.
ENV PATH=/bionemo/bin:$PATH

# Runtime dependencies only. We need python-dev and build-essential for torch.compile.
RUN <<EOT
apt-get update -qy
apt-get install -qyy \
    -o APT::Install-Recommends=false \
    -o APT::Install-Suggests=false \
    gosu \
    build-essential \
    software-properties-common \
    openmpi-bin

add-apt-repository ppa:deadsnakes/ppa
apt-get update -qy
apt-get install -qyy \
    python3.10 \
    python3.10-dev

apt-get clean
rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
EOT

# Create a non-root user. We'll also want to do this in the devcontainer setup.
ARG USERNAME=bionemo
ARG USER_UID=1000
ARG USER_GID=$USER_UID
RUN <<EOT
groupadd --gid $USER_GID $USERNAME
useradd --uid $USER_UID --gid $USER_GID -m $USERNAME -d /home/bionemo
EOT

# Copy the virtual environment from the build image; this is how we get the build libraries above into the runtime
# container.
COPY --from=build --chown=$USERNAME:$USERNAME /bionemo /bionemo
WORKDIR /home/bionemo

# Note that we don't use a USER command to switch to the non-root user, and instead use this entrypoint script that
# mocks the user's UID and GID and runs the provided command as `bionemo`.
COPY ci/docker/entrypoint.sh /usr/local/bin/
ENTRYPOINT ["entrypoint.sh"]
